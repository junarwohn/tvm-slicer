{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tvm import relay\n",
    "from tvm.relay import testing\n",
    "import tvm\n",
    "from tvm import te\n",
    "from tvm.contrib import graph_executor\n",
    "import tvm.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "num_class = 1000\n",
    "\n",
    "img_shape = (3, 224, 224)\n",
    "\n",
    "data_shape = (bs, ) + img_shape\n",
    "output_shape = (bs, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, params = relay.testing.resnet.get_workload(\n",
    "    num_layers=18, batch_size=bs, image_shape=img_shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#[version = \"0.0.5\"]\n",
      "def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {\n",
      "  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;\n",
      "  %1 = %0.0;\n",
      "  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
      "  %3 = nn.batch_norm(%2, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %4 = %3.0;\n",
      "  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
      "  %6 = nn.max_pool2d(%5, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %7 = nn.batch_norm(%6, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %8 = %7.0;\n",
      "  %9 = nn.relu(%8) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %10 = nn.conv2d(%9, %stage1_unit1_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %11 = nn.batch_norm(%10, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %12 = %11.0;\n",
      "  %13 = nn.relu(%12) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %14 = nn.conv2d(%13, %stage1_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %15 = nn.conv2d(%9, %stage1_unit1_sc_weight, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %16 = add(%14, %15) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %17 = nn.batch_norm(%16, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %18 = %17.0;\n",
      "  %19 = nn.relu(%18) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %20 = nn.conv2d(%19, %stage1_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %21 = nn.batch_norm(%20, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %22 = %21.0;\n",
      "  %23 = nn.relu(%22) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %24 = nn.conv2d(%23, %stage1_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %25 = add(%24, %16) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %26 = nn.batch_norm(%25, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %27 = %26.0;\n",
      "  %28 = nn.relu(%27) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %29 = nn.conv2d(%28, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %30 = nn.batch_norm(%29, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %31 = %30.0;\n",
      "  %32 = nn.relu(%31) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %33 = nn.conv2d(%32, %stage2_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %34 = nn.conv2d(%28, %stage2_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %35 = add(%33, %34) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %36 = nn.batch_norm(%35, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %37 = %36.0;\n",
      "  %38 = nn.relu(%37) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %39 = nn.conv2d(%38, %stage2_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %40 = nn.batch_norm(%39, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %41 = %40.0;\n",
      "  %42 = nn.relu(%41) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %43 = nn.conv2d(%42, %stage2_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %44 = add(%43, %35) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %45 = nn.batch_norm(%44, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %46 = %45.0;\n",
      "  %47 = nn.relu(%46) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %48 = nn.conv2d(%47, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %49 = nn.batch_norm(%48, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %50 = %49.0;\n",
      "  %51 = nn.relu(%50) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %52 = nn.conv2d(%51, %stage3_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %53 = nn.conv2d(%47, %stage3_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %54 = add(%52, %53) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %55 = nn.batch_norm(%54, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %56 = %55.0;\n",
      "  %57 = nn.relu(%56) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %58 = nn.conv2d(%57, %stage3_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %59 = nn.batch_norm(%58, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %60 = %59.0;\n",
      "  %61 = nn.relu(%60) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %62 = nn.conv2d(%61, %stage3_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %63 = add(%62, %54) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %64 = nn.batch_norm(%63, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %65 = %64.0;\n",
      "  %66 = nn.relu(%65) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %67 = nn.conv2d(%66, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %68 = nn.batch_norm(%67, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %69 = %68.0;\n",
      "  %70 = nn.relu(%69) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %71 = nn.conv2d(%70, %stage4_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %72 = nn.conv2d(%66, %stage4_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %73 = add(%71, %72) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %74 = nn.batch_norm(%73, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %75 = %74.0;\n",
      "  %76 = nn.relu(%75) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %77 = nn.conv2d(%76, %stage4_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %78 = nn.batch_norm(%77, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %79 = %78.0;\n",
      "  %80 = nn.relu(%79) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %81 = nn.conv2d(%80, %stage4_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %82 = add(%81, %73) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %83 = nn.batch_norm(%82, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %84 = %83.0;\n",
      "  %85 = nn.relu(%84) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;\n",
      "  %88 = nn.dense(%87, %fc1_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;\n",
      "  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;\n",
      "  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set show_meta_data=True if you want to show meta data\n",
    "print(mod.astext(show_meta_data=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76629/2282994270.py:5: DeprecationWarning: legacy graph executor behavior of producing json / lib / params will be removed in the next release. Please see documents of tvm.contrib.graph_executor.GraphModule for the  new recommended usage.\n",
      "  _, mylib, _ = relay.build(mod, target, params=params)\n"
     ]
    }
   ],
   "source": [
    "opt_level = 3\n",
    "target = tvm.target.cuda()\n",
    "with tvm.transform.PassContext(opt_level=opt_level):\n",
    "    lib = relay.build(mod, target, params=params)\n",
    "    # _, mylib, _ = relay.build(mod, target, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = tvm.cuda()\n",
    "data = np.random.uniform(-1, 1, size=data_shape).astype(\"float32\")\n",
    "module = graph_executor.GraphModule(lib[\"default\"](dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.set_input(\"data\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = module.get_output(0, tvm.nd.empty(output_shape)).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00089283 0.00103331 0.0009094  0.00102275 0.00108751 0.00106737\n",
      " 0.00106262 0.00095838 0.00110792 0.00113151]\n"
     ]
    }
   ],
   "source": [
    "print(out.flatten()[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 15:09:58.999490: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tvm.relay as relay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 15:10:09.481709: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-05 15:10:09.481773: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-05 15:10:09.481863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 15:10:09.482254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.665GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-04-05 15:10:09.482270: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-05 15:10:09.483948: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-05 15:10:09.483992: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-04-05 15:10:09.484686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-05 15:10:09.484850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-05 15:10:09.486584: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-05 15:10:09.486981: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-04-05 15:10:09.487010: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-05 15:10:09.487074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 15:10:09.487458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 15:10:09.487776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-04-05 15:10:09.488084: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-05 15:10:09.488236: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-05 15:10:09.488288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 15:10:09.488618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.665GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-04-05 15:10:09.488635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-05 15:10:09.488652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-05 15:10:09.488662: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-04-05 15:10:09.488671: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-05 15:10:09.488681: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-05 15:10:09.488690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-05 15:10:09.488700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-04-05 15:10:09.488707: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-05 15:10:09.488738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 15:10:09.489073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 15:10:09.489382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-04-05 15:10:09.838151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-05 15:10:09.838171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-04-05 15:10:09.838176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-04-05 15:10:09.838298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 15:10:09.838647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 15:10:09.838966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 15:10:09.839275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9618 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "model_keras = tf.keras.models.load_model('./very_simple_model.h5')\n",
    "\n",
    "input_data = np.random.normal(0,1,(1,256,256,3)).astype(np.float32)\n",
    "input_data = input_data.transpose([0, 3, 1, 2])\n",
    "shape_dict = {\"input_1\": input_data.shape}\n",
    "mod, params = relay.frontend.from_keras(model_keras, shape_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import te\n",
    "import tvm.relay as relay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example():\n",
    "    shape = (1, 64, 54, 54)\n",
    "    c_data = np.empty(shape).astype(\"float32\")\n",
    "    c = relay.const(c_data)\n",
    "    weight = relay.var(\"weight\", shape=(64, 64, 3, 3))\n",
    "    x = relay.var(\"x\", relay.TensorType((1, 64, 56, 56), \"float32\"))\n",
    "    conv = relay.nn.conv2d(x, weight)\n",
    "    y = relay.add(c, c)\n",
    "    y = relay.multiply(y, relay.const(2, \"float32\"))\n",
    "    y = relay.add(conv, y)\n",
    "    z = relay.add(y, c)\n",
    "    z1 = relay.add(y, c)\n",
    "    z2 = relay.add(z, z1)\n",
    "    return relay.Function([x, weight], z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first create a relay Module which contains one or multiple Relay\n",
    "# functions for optimization.\n",
    "f = example()\n",
    "mod = tvm.IRModule.from_expr(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#[version = \"0.0.5\"]\n",
      "def @main(%x: Tensor[(1, 64, 56, 56), float32], %weight: Tensor[(64, 64, 3, 3), float32]) {\n",
      "  %0 = add(meta[relay.Constant][0], meta[relay.Constant][0]);\n",
      "  %1 = nn.conv2d(%x, %weight, padding=[0, 0, 0, 0]);\n",
      "  %2 = multiply(%0, 2f);\n",
      "  %3 = add(%1, %2);\n",
      "  %4 = add(%3, meta[relay.Constant][0]);\n",
      "  %5 = add(%3, meta[relay.Constant][0]);\n",
      "  add(%4, %5)\n",
      "}\n",
      "\n",
      "/* For debugging purposes the metadata section has been omitted.\n",
      " * If you would like to see the full metadata section you can set the \n",
      " * option to `True` when invoking `astext`. \n",
      " */\n"
     ]
    }
   ],
   "source": [
    "print(mod.astext(show_meta_data=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%x: Tensor[(1, 64, 56, 56), float32], %weight: Tensor[(64, 64, 3, 3), float32]) -> Tensor[(1, 64, 54, 54), float32] {\n",
      "  %0 = nn.conv2d(%x, %weight, padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %1 = add(%0, meta[relay.Constant][0] /* ty=Tensor[(1, 64, 54, 54), float32] */) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %2 = add(%1, meta[relay.Constant][1] /* ty=Tensor[(1, 64, 54, 54), float32] */) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %3 = add(%1, meta[relay.Constant][1] /* ty=Tensor[(1, 64, 54, 54), float32] */) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  add(%2, %3) /* ty=Tensor[(1, 64, 54, 54), float32] */\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_const = relay.transform.FoldConstant()\n",
    "# Then, we can invoke the pass on the given module. Note that the constant\n",
    "# folding pass works at the function-level. That being said, each function in\n",
    "# the module will be applied with the optimization. Users don't need to iterate\n",
    "# through individual functions manually to apply this pass.\n",
    "mod1 = fold_const(mod)\n",
    "# We can see from the updated program that the constants are folded.\n",
    "print(mod1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%x: Tensor[(1, 64, 56, 56), float32], %weight: Tensor[(64, 64, 3, 3), float32]) -> Tensor[(1, 64, 54, 54), float32] {\n",
      "  %0 = nn.conv2d(%x, %weight, padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %1 = add(%0, meta[relay.Constant][0] /* ty=Tensor[(1, 64, 54, 54), float32] */) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %2 = add(%1, meta[relay.Constant][1] /* ty=Tensor[(1, 64, 54, 54), float32] */) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  add(%2, %2) /* ty=Tensor[(1, 64, 54, 54), float32] */\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod2 = relay.transform.EliminateCommonSubexpr()(mod1)\n",
    "print(mod2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%x: Tensor[(1, 64, 56, 56), float32], %weight: Tensor[(64, 64, 3, 3), float32]) -> Tensor[(1, 64, 54, 54), float32] {\n",
      "  %0 = fn (%p03: Tensor[(1, 64, 56, 56), float32], %p12: Tensor[(64, 64, 3, 3), float32], Primitive=1) -> Tensor[(1, 64, 54, 54), float32] {\n",
      "    nn.conv2d(%p03, %p12, padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 54, 54), float32] */\n",
      "  };\n",
      "  %1 = %0(%x, %weight) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %2 = fn (%p02: Tensor[(1, 64, 54, 54), float32], %p11: Tensor[(1, 64, 54, 54), float32], Primitive=1) -> Tensor[(1, 64, 54, 54), float32] {\n",
      "    add(%p02, %p11) /* ty=Tensor[(1, 64, 54, 54), float32] */\n",
      "  };\n",
      "  %3 = %2(%1, meta[relay.Constant][0] /* ty=Tensor[(1, 64, 54, 54), float32] */) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %4 = fn (%p01: Tensor[(1, 64, 54, 54), float32], %p1: Tensor[(1, 64, 54, 54), float32], Primitive=1) -> Tensor[(1, 64, 54, 54), float32] {\n",
      "    add(%p01, %p1) /* ty=Tensor[(1, 64, 54, 54), float32] */\n",
      "  };\n",
      "  %5 = %4(%3, meta[relay.Constant][1] /* ty=Tensor[(1, 64, 54, 54), float32] */) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %6 = fn (%p0: Tensor[(1, 64, 54, 54), float32], Primitive=1) -> Tensor[(1, 64, 54, 54), float32] {\n",
      "    add(%p0, %p0) /* ty=Tensor[(1, 64, 54, 54), float32] */\n",
      "  };\n",
      "  %6(%5) /* ty=Tensor[(1, 64, 54, 54), float32] */\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod = relay.transform.FuseOps(fuse_opt_level=0)(mod)\n",
    "\n",
    "# We can observe that the optimized module contains functions that only have\n",
    "# a signle primitive op.\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tvm.relay' has no attribute 'fromtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000022vscode-remote?line=0'>1</a>\u001b[0m relay\u001b[39m.\u001b[39;49mfromtext()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tvm.relay' has no attribute 'fromtext'"
     ]
    }
   ],
   "source": [
    "relay.fromtext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.import_from_std(mod1.astext())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.relay.build_module import bind_params_by_name\n",
    "from tvm.relay.dataflow_pattern import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input;\n",
      "free_var %weight;\n",
      "%0 = nn.conv2d(%input, %weight, padding=[0, 0, 0, 0]);\n",
      "nn.relu(%0)\n"
     ]
    }
   ],
   "source": [
    "pattern = is_op(\"nn.relu\")(is_op(\"nn.conv2d\")(wildcard(), wildcard()))\n",
    "\n",
    "# A graph.\n",
    "x = relay.var('input')\n",
    "w = relay.var('weight')\n",
    "conv2d = relay.op.nn.conv2d(x, w)\n",
    "relu = relay.op.nn.relu(conv2d)\n",
    "print(relu)\n",
    "# free_var %x: Tensor[(1, 3, 224, 224), float32]\n",
    "# free_var %w: Tensor[(3, 3, 3, 3), float32]\n",
    "# %0 = nn.conv2d(%x, %w, padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 3, 222, 222), float32] */;\n",
    "# free_var %b: Tensor[(3), float32]\n",
    "# nn.bias_add(%0, %b) /* ty=Tensor[(1, 3, 222, 222), float32] */\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input;\n",
      "free_var %weight;\n",
      "%1 = fn (%FunctionVar_0_0, %FunctionVar_0_1, PartitionedFromPattern=\"nn.conv2d_nn.relu_\") {\n",
      "  %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[0, 0, 0, 0]);\n",
      "  nn.relu(%0)\n",
      "};\n",
      "%1(%input, %weight)\n"
     ]
    }
   ],
   "source": [
    "# After partition.\n",
    "print(pattern.partition(relu))\n",
    "# free_var %x: Tensor[(1, 3, 224, 224), float32]\n",
    "# free_var %w: Tensor[(3, 3, 3, 3), float32]\n",
    "# free_var %b: Tensor[(3), float32]\n",
    "# %1 = fn (%FunctionVar_0_0, %FunctionVar_0_1,\n",
    "#          %FunctionVar_0_2, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_\") {\n",
    "#   %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[0, 0, 0, 0]);\n",
    "#   nn.bias_add(%0, %FunctionVar_0_2)\n",
    "# };\n",
    "# %1(%x, %w, %b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input;\n",
      "free_var %weight;\n",
      "%1 = fn (%FunctionVar_0_0, %FunctionVar_0_1, PartitionedFromPattern=\"nn.conv2d_nn.relu_\", Composite=\"one_layer\") {\n",
      "  %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[0, 0, 0, 0]);\n",
      "  nn.relu(%0)\n",
      "};\n",
      "%1(%input, %weight)\n"
     ]
    }
   ],
   "source": [
    "print(pattern.partition(relu, {'Composite': 'one_layer'}))\n",
    "# free_var %x: Tensor[(1, 3, 224, 224), float32]\n",
    "# free_var %w: Tensor[(3, 3, 3, 3), float32]\n",
    "# free_var %b: Tensor[(3), float32]\n",
    "# %1 = fn (%FunctionVar_0_0, %FunctionVar_0_1,\n",
    "#          %FunctionVar_0_2, Composite=\"one_layer\",\n",
    "#                            PartitionedFromPattern=\"nn.conv2d_nn.bias_add_\") {\n",
    "#   %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[0, 0, 0, 0]);\n",
    "#   nn.bias_add(%0, %FunctionVar_0_2)\n",
    "# };\n",
    "# %1(%x, %w, %b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  7: TVMFuncCall\n  6: tvm::runtime::TypedPackedFunc<tvm::RelayExpr (tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)>::AssignTypedLambda<tvm::relay::{lambda(tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)#3}>(tvm::relay::{lambda(tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)#3}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const\n  5: tvm::relay::PartitionPattern(tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)\n  4: tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)\n  3: void tvm::relay::ExpandDataflow<tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}) [clone .isra.0]\n  2: tvm::relay::MixedModeMutator::VisitLeaf(tvm::RelayExpr const&)\n  1: tvm::relay::PatternPartitioner::DispatchVisitExpr(tvm::RelayExpr const&)\n  0: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&) [clone .cold]\n  File \"/home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py\", line 81, in cfun\n    rv = local_pyfunc(*pyargs)\n  File \"/tmp/ipykernel_130642/923390876.py\", line 3, in check\n    return (conv.attrs.data_layout == \"NCHW\") and bool(conv.checked_type.shape[0] == 1)\n  File \"/home/j/tvm-slicer/include/tvm/python/tvm/ir/expr.py\", line 50, in checked_type\n    raise ValueError(\"The type checker has not populated\" \" the checked_type for this node\")\nValueError: The type checker has not populated the checked_type for this node",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000026vscode-remote?line=1'>2</a>\u001b[0m     conv \u001b[39m=\u001b[39m pre\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000026vscode-remote?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (conv\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mdata_layout \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNCHW\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(conv\u001b[39m.\u001b[39mchecked_type\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000026vscode-remote?line=4'>5</a>\u001b[0m pattern\u001b[39m.\u001b[39;49mpartition(relu, check\u001b[39m=\u001b[39;49mcheck)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py:174\u001b[0m, in \u001b[0;36mDFPattern.partition\u001b[0;34m(self, expr, attrs, check)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=149'>150</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpartition\u001b[39m(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=150'>151</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=151'>152</a>\u001b[0m     expr: Expr,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=152'>153</a>\u001b[0m     attrs: Optional[Dict[\u001b[39mstr\u001b[39m, Object]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=153'>154</a>\u001b[0m     check: Callable[[Expr], \u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=154'>155</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Expr:\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=155'>156</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=156'>157</a>\u001b[0m \u001b[39m    Partition the expression into functions defined by this pattern\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=157'>158</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m        The Expression with matched subgraphs replaced by function calls to that subgraph\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=172'>173</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=173'>174</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m partition(\u001b[39mself\u001b[39;49m, expr, attrs, check)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py:913\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(pattern, expr, attrs, check)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=886'>887</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpartition\u001b[39m(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=887'>888</a>\u001b[0m     pattern: \u001b[39m\"\u001b[39m\u001b[39mDFPattern\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=888'>889</a>\u001b[0m     expr: Expr,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=889'>890</a>\u001b[0m     attrs: Optional[Dict[\u001b[39mstr\u001b[39m, Object]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=890'>891</a>\u001b[0m     check: Callable[[Expr], \u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=891'>892</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Expr:\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=892'>893</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=893'>894</a>\u001b[0m \u001b[39m    Parition the expression into a series of functions that match the pattern\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=894'>895</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=910'>911</a>\u001b[0m \u001b[39m        The Expression with matched subgraphs replaced by function calls to that subgraph\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=911'>912</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=912'>913</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ffi\u001b[39m.\u001b[39;49mpartition(pattern, expr, attrs, check)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=224'>225</a>\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=225'>226</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=226'>227</a>\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=227'>228</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=234'>235</a>\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=235'>236</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=236'>237</a>\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=237'>238</a>\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=238'>239</a>\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  7: TVMFuncCall\n  6: tvm::runtime::TypedPackedFunc<tvm::RelayExpr (tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)>::AssignTypedLambda<tvm::relay::{lambda(tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)#3}>(tvm::relay::{lambda(tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)#3}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const\n  5: tvm::relay::PartitionPattern(tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)\n  4: tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)\n  3: void tvm::relay::ExpandDataflow<tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}) [clone .isra.0]\n  2: tvm::relay::MixedModeMutator::VisitLeaf(tvm::RelayExpr const&)\n  1: tvm::relay::PatternPartitioner::DispatchVisitExpr(tvm::RelayExpr const&)\n  0: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&) [clone .cold]\n  File \"/home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py\", line 81, in cfun\n    rv = local_pyfunc(*pyargs)\n  File \"/tmp/ipykernel_130642/923390876.py\", line 3, in check\n    return (conv.attrs.data_layout == \"NCHW\") and bool(conv.checked_type.shape[0] == 1)\n  File \"/home/j/tvm-slicer/include/tvm/python/tvm/ir/expr.py\", line 50, in checked_type\n    raise ValueError(\"The type checker has not populated\" \" the checked_type for this node\")\nValueError: The type checker has not populated the checked_type for this node"
     ]
    }
   ],
   "source": [
    "def check(pre):\n",
    "    conv = pre.args[0]\n",
    "    return (conv.attrs.data_layout == \"NCHW\") and bool(conv.checked_type.shape[0] == 1)\n",
    "\n",
    "pattern.partition(relu, check=check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %x;\n",
      "free_var %mean;\n",
      "free_var %gamma;\n",
      "%0 = subtract(%x, %mean);\n",
      "free_var %var;\n",
      "%1 = add(%var, %var);\n",
      "%2 = add(%1, 1e-05f);\n",
      "%3 = multiply(%gamma, %0);\n",
      "%4 = sqrt(%2);\n",
      "%5 = divide(%3, %4);\n",
      "free_var %beta;\n",
      "add(%5, %beta)\n",
      "1\n"
     ]
    },
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  10: TVMFuncCall\n  9: tvm::runtime::TypedPackedFunc<tvm::RelayExpr (tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule)>::AssignTypedLambda<tvm::RelayExpr (*)(tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule)>(tvm::RelayExpr (*)(tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const\n  8: tvm::relay::RewritePatterns(tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule)\n  7: tvm::relay::PatternRewriter::Rewrite(tvm::runtime::Array<tvm::relay::DFPatternCallback, void> const&, tvm::RelayExpr const&)\n  6: tvm::relay::PatternGrouper::GroupMatches(tvm::relay::DFPattern const&, tvm::RelayExpr const&)\n  5: tvm::relay::CreateIndexedGraph(tvm::RelayExpr const&)\n  4: tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)\n  3: void tvm::relay::ExpandDataflow<tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}) [clone .isra.0]\n  2: tvm::relay::CreateIndexedGraph(tvm::RelayExpr const&)::Creator::VisitLeaf(tvm::RelayExpr const&)\n  1: tvm::relay::MixedModeVisitor::VisitLeaf(tvm::RelayExpr const&)\n  0: tvm::relay::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>::VisitExpr(tvm::RelayExpr const&) [clone .part.0]\n  File \"/home/j/tvm-slicer/include/tvm/include/tvm/relay/expr_functor.h\", line 92\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (n.defined()) is false: Found null pointer node while traversing AST. The previous pass may have generated invalid data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000028vscode-remote?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(BN)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000028vscode-remote?line=40'>41</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtvm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrelay\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataflow_pattern\u001b[39;00m \u001b[39mimport\u001b[39;00m rewrite\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000028vscode-remote?line=41'>42</a>\u001b[0m out \u001b[39m=\u001b[39m rewrite(BatchnormCallback(), BN)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py:884\u001b[0m, in \u001b[0;36mrewrite\u001b[0;34m(callbacks, expr, mod)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=876'>877</a>\u001b[0m     \u001b[39massert\u001b[39;00m callback\u001b[39m.\u001b[39mpattern \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=877'>878</a>\u001b[0m     tmp\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=878'>879</a>\u001b[0m         _DFPatternCallback(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=879'>880</a>\u001b[0m             callback\u001b[39m.\u001b[39mpattern, callback\u001b[39m.\u001b[39mcallback, callback\u001b[39m.\u001b[39mrequire_type, callback\u001b[39m.\u001b[39mrewrite_once\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=880'>881</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=881'>882</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=883'>884</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ffi\u001b[39m.\u001b[39;49mrewrite(tmp, expr, mod)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=224'>225</a>\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=225'>226</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=226'>227</a>\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=227'>228</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=234'>235</a>\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=235'>236</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=236'>237</a>\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=237'>238</a>\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=238'>239</a>\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  10: TVMFuncCall\n  9: tvm::runtime::TypedPackedFunc<tvm::RelayExpr (tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule)>::AssignTypedLambda<tvm::RelayExpr (*)(tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule)>(tvm::RelayExpr (*)(tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const\n  8: tvm::relay::RewritePatterns(tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule)\n  7: tvm::relay::PatternRewriter::Rewrite(tvm::runtime::Array<tvm::relay::DFPatternCallback, void> const&, tvm::RelayExpr const&)\n  6: tvm::relay::PatternGrouper::GroupMatches(tvm::relay::DFPattern const&, tvm::RelayExpr const&)\n  5: tvm::relay::CreateIndexedGraph(tvm::RelayExpr const&)\n  4: tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)\n  3: void tvm::relay::ExpandDataflow<tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}) [clone .isra.0]\n  2: tvm::relay::CreateIndexedGraph(tvm::RelayExpr const&)::Creator::VisitLeaf(tvm::RelayExpr const&)\n  1: tvm::relay::MixedModeVisitor::VisitLeaf(tvm::RelayExpr const&)\n  0: tvm::relay::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>::VisitExpr(tvm::RelayExpr const&) [clone .part.0]\n  File \"/home/j/tvm-slicer/include/tvm/include/tvm/relay/expr_functor.h\", line 92\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (n.defined()) is false: Found null pointer node while traversing AST. The previous pass may have generated invalid data."
     ]
    }
   ],
   "source": [
    "class BatchnormCallback(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        self.x = wildcard()\n",
    "        self.var = wildcard()\n",
    "        self.mean = wildcard()\n",
    "        self.beta = wildcard()\n",
    "        self.gamma = wildcard()\n",
    "        self.eps = wildcard()\n",
    "\n",
    "        self.pattern = self.gamma * (self.x - self.mean)/is_op(\"sqrt\")(self.var + self.eps) + self.beta\n",
    "        self.cnt = 0\n",
    "        self.target = 2\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        if self.cnt != self.target:\n",
    "            print(1)\n",
    "            self.cnt += 1\n",
    "        else :    \n",
    "            # print(node_map)\n",
    "            x = node_map[self.x][0]\n",
    "            var = node_map[self.var][0]\n",
    "            mean = node_map[self.mean][0]\n",
    "            beta = node_map[self.beta][0]\n",
    "            gamma = node_map[self.gamma][0]\n",
    "            eps = node_map[self.eps][0]\n",
    "            return relay.op.nn.batch_norm(x, gamma, beta, mean, var, epsilon = eps.data.numpy().item())[0]\n",
    "\n",
    "# A graph of arithmetic operators that are functional equivalent to batch_norm.\n",
    "x = relay.var('x')\n",
    "var = relay.var('var')\n",
    "mean = relay.var('mean')\n",
    "beta = relay.var('beta')\n",
    "gamma = relay.var('gamma')\n",
    "\n",
    "JUNO = var + var + var + var + var + var + var \n",
    "\n",
    "BN = gamma * (x - mean)/relay.op.sqrt(var + var + relay.const(1e-5)) + beta\n",
    "print(BN)\n",
    "from tvm.relay.dataflow_pattern import rewrite\n",
    "out = rewrite(BatchnormCallback(), BN)\n",
    "# assert tvm.ir.structural_equal(out, relay.op.nn.batch_norm(x, gamma, beta, mean, var, epsilon = 1e-5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===target relay===\n",
      "free_var %x1;\n",
      "free_var %x2;\n",
      "%0 = add(%x1, %x2);\n",
      "free_var %x3;\n",
      "%1 = add(%0, %x3);\n",
      "free_var %x4;\n",
      "%2 = add(%1, %x4);\n",
      "free_var %x5;\n",
      "add(%2, %x5)\n",
      "==================\n",
      "======OUT=========\n",
      "free_var %x1;\n",
      "free_var %x2;\n",
      "%0 = add(%x1, %x2);\n",
      "free_var %x3;\n",
      "%1 = add(%0, %x3);\n",
      "free_var %x4;\n",
      "%2 = add(%1, %x4);\n",
      "free_var %x5;\n",
      "add(%2, %x5)\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "class JUNOCallback(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        super().__init__(rewrite_once=True)\n",
    "        # self.x = wildcard()\n",
    "        self.var1 = wildcard()\n",
    "        self.var2 = wildcard()\n",
    "        # self.var = wildcard()\n",
    "        # self.meta = wildcard()\n",
    "        self.var = relay.var(\"var\")\n",
    "        self.meta = relay.var(\"meta\")\n",
    "        self.padding = [1, 1, 1, 1]\n",
    "        self.channels = 32 \n",
    "        self.kernel_size = [3, 3]\n",
    "        self.out_dtype = \"int32\"\n",
    "        # self.var = wildcard()\n",
    "        \n",
    "        # self.mean = wildcard()\n",
    "        # self.beta = wildcard()\n",
    "        # self.gamma = wildcard()\n",
    "        # self.eps = wildcard()\n",
    "\n",
    "        # self.pattern = self.gamma * (self.x - self.mean)/is_op(\"sqrt\")(self.var + self.eps) + self.beta\n",
    "        # self.pattern = is_op(relay.op.nn.max_pool2d)\n",
    "        # self.pattern = relay.op.nn.nn.max_pool2d(self.var1)\n",
    "        # self.pattern = relay.op.nn.conv2d(\n",
    "        #     self.var, \n",
    "        #     self.meta, \n",
    "        #     padding=self.padding, \n",
    "        #     channels=self.channels,\n",
    "        #     kernel_size=self.kernel_size,\n",
    "        #     out_dtype=self.out_dtype\n",
    "        # )\n",
    "        # self.pattern = is_op('nn.conv2d')(wildcard(), wildcard(), is_constant(), is_constant(), is_constant())\n",
    "        self.pattern = is_op('nn.bias_add')(self.var1, self.var2)\n",
    "        # self.pattern = is_op('max_pool2d')(self.var1, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0] )\n",
    "        self.cnt = 0\n",
    "        self.target = 0\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        var1 = node_map[self.var1][0]\n",
    "        var2 = node_map[self.var2][0]\n",
    "        original = relay.nn.bias_add(var1, var2)\n",
    "\n",
    "        if self.cnt != self.target:\n",
    "            print(self.cnt, self.target)\n",
    "            print(\"fuck\")\n",
    "            self.cnt += 1\n",
    "            return original\n",
    "        else:\n",
    "            self.cnt += 1\n",
    "            # return var1 * var2\n",
    "            original = relay.nn.bias_add(var1, var2)\n",
    "            cast_to_int8 = relay.cast(\n",
    "                relay.clip(\n",
    "                    relay.round(\n",
    "                        relay.multiply(original, relay.const(16.0))\n",
    "                    ), \n",
    "                    a_min=-127.0, a_max=127.0\n",
    "                ),\n",
    "                dtype=\"int8\"\n",
    "            )\n",
    "            print(cast_to_int8)\n",
    "            cast_to_float32 = relay.cast(\n",
    "                relay.clip(\n",
    "                    relay.right_shift(\n",
    "                        relay.add(relay.cast(cast_to_int8, dtype='int32'), relay.const(512)),\n",
    "                        relay.const(10)),\n",
    "                    a_min=-127.0, a_max=127.0), \n",
    "                dtype=\"float32\"\n",
    "            )\n",
    "            return cast_to_float32\n",
    "            # return relay.cliprelay.right_shift(relay.add(relay.cast(\n",
    "            #     relay.clip(\n",
    "            #         relay.round(\n",
    "            #             var1 * relay.const(16.0)\n",
    "            #         ), \n",
    "            #         a_min=relay.const(-127.0), \n",
    "            #         a_max=relay.const(127.0)\n",
    "            #     ), dtype=\"int8\"), relay.const(512)), 10)\n",
    "\n",
    "x1 = relay.var('x1')\n",
    "x2 = relay.var('x2')\n",
    "x3 = relay.var('x3')\n",
    "x4 = relay.var('x4')\n",
    "x5 = relay.var('x5')\n",
    "\n",
    "JUNO = x1 + x2 + x3 + x4 + x5\n",
    "\n",
    "print(\"===target relay===\")\n",
    "print(JUNO)\n",
    "print(\"==================\")\n",
    "bc = JUNOCallback()\n",
    "# print(\"target pattern\")\n",
    "# print(bc.pattern)\n",
    "out = rewrite(JUNOCallback(), JUNO)\n",
    "print(\"======OUT=========\")\n",
    "print(out)\n",
    "print(\"==================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "fuck\n",
      "2 0\n",
      "fuck\n",
      "1 0\n",
      "fuck\n",
      "2 0\n",
      "fuck\n"
     ]
    }
   ],
   "source": [
    "out = rewrite(JUNOCallback(), mod['main'])\n",
    "out_body = rewrite(JUNOCallback(), mod['main'].body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fn (%input_1: Tensor[(1, 3, 256, 256), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(32, 16, 3, 3), float32], %v_param_4: Tensor[(32), float32], %v_param_5: Tensor[(64, 32, 3, 3), float32], %v_param_6: Tensor[(64), float32]) {\n",
       "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %1 = nn.bias_add(%0, %v_param_2);\n",
       "  %2 = multiply(%1, 16f);\n",
       "  %3 = round(%2);\n",
       "  %4 = clip(%3, a_min=-127f, a_max=127f);\n",
       "  %5 = cast(%4, dtype=\"int8\");\n",
       "  %6 = cast(%5, dtype=\"int32\");\n",
       "  %7 = add(%6, 512);\n",
       "  %8 = right_shift(%7, 10);\n",
       "  %9 = clip(%8, a_min=-127f, a_max=127f);\n",
       "  %10 = cast(%9, dtype=\"float32\");\n",
       "  %11 = nn.max_pool2d(%10, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %12 = nn.conv2d(%11, %v_param_3, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %13 = nn.bias_add(%12, %v_param_4);\n",
       "  %14 = nn.max_pool2d(%13, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %15 = nn.conv2d(%14, %v_param_5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %16 = nn.bias_add(%15, %v_param_6);\n",
       "  nn.max_pool2d(%16, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0])\n",
       "}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod['main'] = out\n",
    "# mod['main'].body = out_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %x1;\n",
      "free_var %x2;\n",
      "%0 = add(%x1, %x2);\n",
      "free_var %x3;\n",
      "%1 = add(%0, %x3);\n",
      "free_var %x4;\n",
      "%2 = add(%1, %x4);\n",
      "free_var %x5;\n",
      "add(%2, %x5)\n"
     ]
    }
   ],
   "source": [
    "mod, params = relay.frontend.from_keras(model_keras, shape_dict)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  4: TVMFuncCall\n  3: _ZNSt17_Function_handlerIFvN3\n  2: tvm::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const [clone .isra.0]\n  1: tvm::IRModuleNode::Add(tvm::GlobalVar const&, tvm::BaseFunc const&, bool)\n  0: tvm::WarnIfMalformed(tvm::IRModule const&, tvm::relay::Function)\n  File \"/home/j/tvm-slicer/include/tvm/src/ir/module.cc\", line 190\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n\n  Check failed: fv.size() == 0 (7 vs. 0) : Function:\nfn () {\n  free_var %input_1: Tensor[(1, 3, 256, 256), float32];\n  free_var %v_param_1: Tensor[(16, 3, 3, 3), float32];\n  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n  free_var %v_param_2: Tensor[(16), float32];\n  %1 = nn.bias_add(%0, %v_param_2);\n  %2 = multiply(%1, 16f);\n  %3 = round(%2);\n  %4 = clip(%3, a_min=-127f, a_max=127f);\n  %5 = cast(%4, dtype=\"int8\");\n  %6 = cast(%5, dtype=\"int32\");\n  %7 = add(%6, 512);\n  %8 = right_shift(%7, 10);\n  %9 = clip(%8, a_min=-127f, a_max=127f);\n  %10 = cast(%9, dtype=\"float32\");\n  %11 = nn.max_pool2d(%10, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n  free_var %v_param_3: Tensor[(32, 16, 3, 3), float32];\n  %12 = nn.conv2d(%11, %v_param_3, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n  free_var %v_param_4: Tensor[(32), float32];\n  %13 = add(%12, %v_param_4);\n  %14 = nn.max_pool2d(%13, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n  free_var %v_param_5: Tensor[(64, 32, 3, 3), float32];\n  %15 = nn.conv2d(%14, %v_param_5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n  free_var %v_param_6: Tensor[(64), float32];\n  %16 = add(%15, %v_param_6);\n  nn.max_pool2d(%16, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0])\n}\ncontains free variables: [Var(input_1, ty=TensorType([1, 3, 256, 256], float32)), Var(_param_1, ty=TensorType([16, 3, 3, 3], float32)), Var(_param_2, ty=TensorType([16], float32)), Var(_param_3, ty=TensorType([32, 16, 3, 3], float32)), Var(_param_4, ty=TensorType([32], float32)), Var(_param_5, ty=TensorType([64, 32, 3, 3], float32)), Var(_param_6, ty=TensorType([64], float32))]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb Cell 34'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000046vscode-remote?line=0'>1</a>\u001b[0m mod[\u001b[39m'\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m out\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/ir/module.py:75\u001b[0m, in \u001b[0;36mIRModule.__setitem__\u001b[0;34m(self, var, val)\u001b[0m\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=63'>64</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__setitem__\u001b[39m(\u001b[39mself\u001b[39m, var, val):\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=64'>65</a>\u001b[0m     \u001b[39m\"\"\"Add a mapping to the module.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=65'>66</a>\u001b[0m \n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=66'>67</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=72'>73</a>\u001b[0m \u001b[39m        The value.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=73'>74</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=74'>75</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add(var, val, \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/ir/module.py:84\u001b[0m, in \u001b[0;36mIRModule._add\u001b[0;34m(self, var, val, update)\u001b[0m\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=81'>82</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=82'>83</a>\u001b[0m             var \u001b[39m=\u001b[39m _expr\u001b[39m.\u001b[39mGlobalVar(var)\n\u001b[0;32m---> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=83'>84</a>\u001b[0m     _ffi_api\u001b[39m.\u001b[39;49mModule_Add(\u001b[39mself\u001b[39;49m, var, val, update)\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=84'>85</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=85'>86</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(val, _ty\u001b[39m.\u001b[39mType)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=224'>225</a>\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=225'>226</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=226'>227</a>\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=227'>228</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=234'>235</a>\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=235'>236</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=236'>237</a>\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=237'>238</a>\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=238'>239</a>\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  4: TVMFuncCall\n  3: _ZNSt17_Function_handlerIFvN3\n  2: tvm::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const [clone .isra.0]\n  1: tvm::IRModuleNode::Add(tvm::GlobalVar const&, tvm::BaseFunc const&, bool)\n  0: tvm::WarnIfMalformed(tvm::IRModule const&, tvm::relay::Function)\n  File \"/home/j/tvm-slicer/include/tvm/src/ir/module.cc\", line 190\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n\n  Check failed: fv.size() == 0 (7 vs. 0) : Function:\nfn () {\n  free_var %input_1: Tensor[(1, 3, 256, 256), float32];\n  free_var %v_param_1: Tensor[(16, 3, 3, 3), float32];\n  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n  free_var %v_param_2: Tensor[(16), float32];\n  %1 = nn.bias_add(%0, %v_param_2);\n  %2 = multiply(%1, 16f);\n  %3 = round(%2);\n  %4 = clip(%3, a_min=-127f, a_max=127f);\n  %5 = cast(%4, dtype=\"int8\");\n  %6 = cast(%5, dtype=\"int32\");\n  %7 = add(%6, 512);\n  %8 = right_shift(%7, 10);\n  %9 = clip(%8, a_min=-127f, a_max=127f);\n  %10 = cast(%9, dtype=\"float32\");\n  %11 = nn.max_pool2d(%10, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n  free_var %v_param_3: Tensor[(32, 16, 3, 3), float32];\n  %12 = nn.conv2d(%11, %v_param_3, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n  free_var %v_param_4: Tensor[(32), float32];\n  %13 = add(%12, %v_param_4);\n  %14 = nn.max_pool2d(%13, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n  free_var %v_param_5: Tensor[(64, 32, 3, 3), float32];\n  %15 = nn.conv2d(%14, %v_param_5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n  free_var %v_param_6: Tensor[(64), float32];\n  %16 = add(%15, %v_param_6);\n  nn.max_pool2d(%16, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0])\n}\ncontains free variables: [Var(input_1, ty=TensorType([1, 3, 256, 256], float32)), Var(_param_1, ty=TensorType([16, 3, 3, 3], float32)), Var(_param_2, ty=TensorType([16], float32)), Var(_param_3, ty=TensorType([32, 16, 3, 3], float32)), Var(_param_4, ty=TensorType([32], float32)), Var(_param_5, ty=TensorType([64, 32, 3, 3], float32)), Var(_param_6, ty=TensorType([64], float32))]"
     ]
    }
   ],
   "source": [
    "mod['main'] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_fun = tvm.IRModule.from_expr(out)\n",
    "# mod['main'].body = out\n",
    "\n",
    "target = 'cuda'\n",
    "dev = tvm.cuda()\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=2):\n",
    "    lib = relay.build(mod, target, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fn (%input_1: Tensor[(1, 3, 256, 256), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(32, 16, 3, 3), float32], %v_param_4: Tensor[(32), float32], %v_param_5: Tensor[(64, 32, 3, 3), float32], %v_param_6: Tensor[(64), float32]) {\n",
       "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %1 = nn.bias_add(%0, %v_param_2);\n",
       "  %2 = nn.max_pool2d(%1, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %3 = nn.conv2d(%2, %v_param_3, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %4 = nn.bias_add(%3, %v_param_4);\n",
       "  %5 = nn.max_pool2d(%4, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %6 = nn.conv2d(%5, %v_param_5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %7 = nn.bias_add(%6, %v_param_6);\n",
       "  nn.max_pool2d(%7, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0])\n",
       "}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pygraphviz as pgv\n",
    "\n",
    "def show_graph(json_data, file_name=None):\n",
    "    if type(json_data) == str:\n",
    "        json_data = json.loads(json_data)\n",
    "    A = pgv.AGraph(directed=True)\n",
    "    for node_idx, node in enumerate(json_data['nodes']):\n",
    "        for src in node['inputs']:\n",
    "            A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]), node['name'] + '[{}]'.format(node_idx) + '{}'.format(json_data['attrs']['dltype'][1][node_idx]))\n",
    "            #A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(shape_size(json_data['attrs']['shape'][1][src[0]])) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]), node['name'] + '[{}]'.format(node_idx) + '{}'.format(shape_size(json_data['attrs']['shape'][1][node_idx])) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]))\n",
    "    if file_name:\n",
    "        A.draw(file_name + '.png', format='png', prog='dot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(lib['get_graph_json'](), \"quanadd_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model_keras = tf.keras.models.load_model('./very_simple_model.h5')\n",
    "\n",
    "input_data = np.random.normal(0,1,(1,256,256,3)).astype(np.float32)\n",
    "input_data = input_data.transpose([0, 3, 1, 2])\n",
    "shape_dict = {\"input_1\": input_data.shape}\n",
    "mod, params = relay.frontend.from_keras(model_keras, shape_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  2: TVMFuncCall\n  1: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::relay::Call (tvm::RelayExpr, tvm::RelayExpr)>::AssignTypedLambda<tvm::relay::{lambda(tvm::RelayExpr, tvm::RelayExpr)#17}>(tvm::relay::{lambda(tvm::RelayExpr, tvm::RelayExpr)#17}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  0: tvm::runtime::TVMMovableArgValueWithContext_::operator tvm::RelayExpr<tvm::RelayExpr>() const\n  3: TVMFuncCall\n  2: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::relay::Call (tvm::RelayExpr, tvm::RelayExpr)>::AssignTypedLambda<tvm::relay::{lambda(tvm::RelayExpr, tvm::RelayExpr)#17}>(tvm::relay::{lambda(tvm::RelayExpr, tvm::RelayExpr)#17}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  1: tvm::runtime::TVMMovableArgValueWithContext_::operator tvm::RelayExpr<tvm::RelayExpr>() const\n  0: tvm::RelayExpr tvm::runtime::TVMPODValue_::AsObjectRef<tvm::RelayExpr>() const\n  File \"/home/j/tvm-slicer/include/tvm/include/tvm/runtime/packed_func.h\", line 714\nTVMError: In function relay.op._make.multiply: error while converting argument 1: [15:29:22] /home/j/tvm-slicer/include/tvm/include/tvm/runtime/packed_func.h:1611: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n\n  Check failed: type_code_ == kTVMObjectHandle (2 vs. 8) : expected Object but got float\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb Cell 30'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000031vscode-remote?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m relay\u001b[39m.\u001b[39mvar(\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000031vscode-remote?line=1'>2</a>\u001b[0m rmultiply \u001b[39m=\u001b[39m relay\u001b[39m.\u001b[39;49mop\u001b[39m.\u001b[39;49mmultiply(x, \u001b[39m1.0\u001b[39;49m)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/relay/op/tensor.py:566\u001b[0m, in \u001b[0;36mmultiply\u001b[0;34m(lhs, rhs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/op/tensor.py?line=550'>551</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmultiply\u001b[39m(lhs, rhs):\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/op/tensor.py?line=551'>552</a>\u001b[0m     \u001b[39m\"\"\"Multiplication with numpy-style broadcasting.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/op/tensor.py?line=552'>553</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/op/tensor.py?line=553'>554</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/op/tensor.py?line=563'>564</a>\u001b[0m \u001b[39m        The computed result.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/op/tensor.py?line=564'>565</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/op/tensor.py?line=565'>566</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _make\u001b[39m.\u001b[39;49mmultiply(lhs, rhs)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=224'>225</a>\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=225'>226</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=226'>227</a>\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=227'>228</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=234'>235</a>\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=235'>236</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=236'>237</a>\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=237'>238</a>\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=238'>239</a>\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  2: TVMFuncCall\n  1: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::relay::Call (tvm::RelayExpr, tvm::RelayExpr)>::AssignTypedLambda<tvm::relay::{lambda(tvm::RelayExpr, tvm::RelayExpr)#17}>(tvm::relay::{lambda(tvm::RelayExpr, tvm::RelayExpr)#17}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  0: tvm::runtime::TVMMovableArgValueWithContext_::operator tvm::RelayExpr<tvm::RelayExpr>() const\n  3: TVMFuncCall\n  2: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::relay::Call (tvm::RelayExpr, tvm::RelayExpr)>::AssignTypedLambda<tvm::relay::{lambda(tvm::RelayExpr, tvm::RelayExpr)#17}>(tvm::relay::{lambda(tvm::RelayExpr, tvm::RelayExpr)#17}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  1: tvm::runtime::TVMMovableArgValueWithContext_::operator tvm::RelayExpr<tvm::RelayExpr>() const\n  0: tvm::RelayExpr tvm::runtime::TVMPODValue_::AsObjectRef<tvm::RelayExpr>() const\n  File \"/home/j/tvm-slicer/include/tvm/include/tvm/runtime/packed_func.h\", line 714\nTVMError: In function relay.op._make.multiply: error while converting argument 1: [15:29:22] /home/j/tvm-slicer/include/tvm/include/tvm/runtime/packed_func.h:1611: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n\n  Check failed: type_code_ == kTVMObjectHandle (2 vs. 8) : expected Object but got float\n"
     ]
    }
   ],
   "source": [
    "x = relay.var('x')\n",
    "rmultiply = relay.op.multiply(x, 1.0)\n",
    "# r\n",
    "# QN = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %x;\n",
      "free_var %mean;\n",
      "free_var %gamma;\n",
      "%0 = subtract(%x, %mean);\n",
      "free_var %var;\n",
      "%1 = add(%var, 1e-05f);\n",
      "%2 = multiply(%gamma, %0);\n",
      "%3 = sqrt(%1);\n",
      "%4 = divide(%2, %3);\n",
      "free_var %beta;\n",
      "add(%4, %beta)\n"
     ]
    }
   ],
   "source": [
    "print(BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %x;\n",
      "free_var %gamma;\n",
      "free_var %beta;\n",
      "free_var %mean;\n",
      "free_var %var;\n",
      "%0 = nn.batch_norm(%x, %gamma, %beta, %mean, %var);\n",
      "%0.0\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
