{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tvm import relay\n",
    "from tvm.relay import testing\n",
    "import tvm\n",
    "from tvm import te\n",
    "from tvm.contrib import graph_executor\n",
    "import tvm.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "num_class = 1000\n",
    "\n",
    "img_shape = (3, 224, 224)\n",
    "\n",
    "data_shape = (bs, ) + img_shape\n",
    "output_shape = (bs, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, params = relay.testing.resnet.get_workload(\n",
    "    num_layers=18, batch_size=bs, image_shape=img_shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#[version = \"0.0.5\"]\n",
      "def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {\n",
      "  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;\n",
      "  %1 = %0.0;\n",
      "  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
      "  %3 = nn.batch_norm(%2, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %4 = %3.0;\n",
      "  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
      "  %6 = nn.max_pool2d(%5, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %7 = nn.batch_norm(%6, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %8 = %7.0;\n",
      "  %9 = nn.relu(%8) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %10 = nn.conv2d(%9, %stage1_unit1_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %11 = nn.batch_norm(%10, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %12 = %11.0;\n",
      "  %13 = nn.relu(%12) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %14 = nn.conv2d(%13, %stage1_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %15 = nn.conv2d(%9, %stage1_unit1_sc_weight, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %16 = add(%14, %15) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %17 = nn.batch_norm(%16, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %18 = %17.0;\n",
      "  %19 = nn.relu(%18) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %20 = nn.conv2d(%19, %stage1_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %21 = nn.batch_norm(%20, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %22 = %21.0;\n",
      "  %23 = nn.relu(%22) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %24 = nn.conv2d(%23, %stage1_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %25 = add(%24, %16) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %26 = nn.batch_norm(%25, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
      "  %27 = %26.0;\n",
      "  %28 = nn.relu(%27) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
      "  %29 = nn.conv2d(%28, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %30 = nn.batch_norm(%29, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %31 = %30.0;\n",
      "  %32 = nn.relu(%31) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %33 = nn.conv2d(%32, %stage2_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %34 = nn.conv2d(%28, %stage2_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %35 = add(%33, %34) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %36 = nn.batch_norm(%35, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %37 = %36.0;\n",
      "  %38 = nn.relu(%37) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %39 = nn.conv2d(%38, %stage2_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %40 = nn.batch_norm(%39, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %41 = %40.0;\n",
      "  %42 = nn.relu(%41) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %43 = nn.conv2d(%42, %stage2_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %44 = add(%43, %35) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %45 = nn.batch_norm(%44, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
      "  %46 = %45.0;\n",
      "  %47 = nn.relu(%46) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
      "  %48 = nn.conv2d(%47, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %49 = nn.batch_norm(%48, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %50 = %49.0;\n",
      "  %51 = nn.relu(%50) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %52 = nn.conv2d(%51, %stage3_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %53 = nn.conv2d(%47, %stage3_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %54 = add(%52, %53) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %55 = nn.batch_norm(%54, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %56 = %55.0;\n",
      "  %57 = nn.relu(%56) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %58 = nn.conv2d(%57, %stage3_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %59 = nn.batch_norm(%58, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %60 = %59.0;\n",
      "  %61 = nn.relu(%60) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %62 = nn.conv2d(%61, %stage3_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %63 = add(%62, %54) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %64 = nn.batch_norm(%63, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
      "  %65 = %64.0;\n",
      "  %66 = nn.relu(%65) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
      "  %67 = nn.conv2d(%66, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %68 = nn.batch_norm(%67, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %69 = %68.0;\n",
      "  %70 = nn.relu(%69) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %71 = nn.conv2d(%70, %stage4_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %72 = nn.conv2d(%66, %stage4_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %73 = add(%71, %72) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %74 = nn.batch_norm(%73, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %75 = %74.0;\n",
      "  %76 = nn.relu(%75) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %77 = nn.conv2d(%76, %stage4_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %78 = nn.batch_norm(%77, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %79 = %78.0;\n",
      "  %80 = nn.relu(%79) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %81 = nn.conv2d(%80, %stage4_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %82 = add(%81, %73) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %83 = nn.batch_norm(%82, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
      "  %84 = %83.0;\n",
      "  %85 = nn.relu(%84) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
      "  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
      "  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;\n",
      "  %88 = nn.dense(%87, %fc1_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;\n",
      "  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;\n",
      "  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set show_meta_data=True if you want to show meta data\n",
    "print(mod.astext(show_meta_data=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_level = 3\n",
    "target = tvm.target.cuda()\n",
    "with tvm.transform.PassContext(opt_level=opt_level):\n",
    "    lib = relay.build(mod, target, params=params)\n",
    "    # _, mylib, _ = relay.build(mod, target, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = tvm.cuda()\n",
    "data = np.random.uniform(-1, 1, size=data_shape).astype(\"float32\")\n",
    "module = graph_executor.GraphModule(lib[\"default\"](dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.set_input(\"data\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = module.get_output(0, tvm.nd.empty(output_shape)).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00089283 0.00103331 0.0009094  0.00102275 0.00108751 0.00106737\n",
      " 0.00106262 0.00095838 0.00110792 0.00113151]\n"
     ]
    }
   ],
   "source": [
    "print(out.flatten()[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 12:56:45.037977: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tvm.relay as relay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 12:56:45.825453: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-07 12:56:45.825575: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-07 12:56:45.861276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 12:56:45.861865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.665GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-04-07 12:56:45.861896: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-07 12:56:45.864408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-07 12:56:45.864454: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-04-07 12:56:45.865462: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-07 12:56:45.865687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-07 12:56:45.868125: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-07 12:56:45.868638: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-04-07 12:56:45.868672: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-07 12:56:45.868736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 12:56:45.869185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 12:56:45.869576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-04-07 12:56:45.869942: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-07 12:56:45.870459: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-07 12:56:45.870641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 12:56:45.871046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.665GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-04-07 12:56:45.871064: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-07 12:56:45.871081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-07 12:56:45.871093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-04-07 12:56:45.871105: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-07 12:56:45.871116: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-07 12:56:45.871127: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-07 12:56:45.871140: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-04-07 12:56:45.871149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-07 12:56:45.871189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 12:56:45.871601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 12:56:45.871981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-04-07 12:56:46.235374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-07 12:56:46.235401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-04-07 12:56:46.235405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-04-07 12:56:46.235507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 12:56:46.235809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 12:56:46.236067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-07 12:56:46.236304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9698 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "model_keras = tf.keras.models.load_model('./very_simple_model.h5')\n",
    "\n",
    "input_data = np.random.normal(0,1,(1,256,256,3)).astype(np.float32)\n",
    "input_data = input_data.transpose([0, 3, 1, 2])\n",
    "shape_dict = {\"input_1\": input_data.shape}\n",
    "mod, params = relay.frontend.from_keras(model_keras, shape_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import te\n",
    "import tvm.relay as relay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example():\n",
    "    shape = (1, 64, 54, 54)\n",
    "    c_data = np.empty(shape).astype(\"float32\")\n",
    "    c = relay.const(c_data)\n",
    "    weight = relay.var(\"weight\", shape=(64, 64, 3, 3))\n",
    "    x = relay.var(\"x\", relay.TensorType((1, 64, 56, 56), \"float32\"))\n",
    "    conv = relay.nn.conv2d(x, weight)\n",
    "    y = relay.add(c, c)\n",
    "    y = relay.multiply(y, relay.const(2, \"float32\"))\n",
    "    y = relay.add(conv, y)\n",
    "    z = relay.add(y, c)\n",
    "    z1 = relay.add(y, c)\n",
    "    z2 = relay.add(z, z1)\n",
    "    return relay.Function([x, weight], z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first create a relay Module which contains one or multiple Relay\n",
    "# functions for optimization.\n",
    "f = example()\n",
    "mod = tvm.IRModule.from_expr(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#[version = \"0.0.5\"]\n",
      "def @main(%x: Tensor[(1, 64, 56, 56), float32], %weight: Tensor[(64, 64, 3, 3), float32]) {\n",
      "  %0 = add(meta[relay.Constant][0], meta[relay.Constant][0]);\n",
      "  %1 = nn.conv2d(%x, %weight, padding=[0, 0, 0, 0]);\n",
      "  %2 = multiply(%0, 2f);\n",
      "  %3 = add(%1, %2);\n",
      "  %4 = add(%3, meta[relay.Constant][0]);\n",
      "  %5 = add(%3, meta[relay.Constant][0]);\n",
      "  add(%4, %5)\n",
      "}\n",
      "\n",
      "/* For debugging purposes the metadata section has been omitted.\n",
      " * If you would like to see the full metadata section you can set the \n",
      " * option to `True` when invoking `astext`. \n",
      " */\n"
     ]
    }
   ],
   "source": [
    "print(mod.astext(show_meta_data=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%x: Tensor[(1, 64, 56, 56), float32], %weight: Tensor[(64, 64, 3, 3), float32]) -> Tensor[(1, 64, 54, 54), float32] {\n",
      "  %0 = nn.conv2d(%x, %weight, padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %1 = add(%0, meta[relay.Constant][0] /* ty=Tensor[(1, 64, 54, 54), float32] */) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %2 = add(%1, meta[relay.Constant][1] /* ty=Tensor[(1, 64, 54, 54), float32] */) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %3 = add(%1, meta[relay.Constant][1] /* ty=Tensor[(1, 64, 54, 54), float32] */) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  add(%2, %3) /* ty=Tensor[(1, 64, 54, 54), float32] */\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_const = relay.transform.FoldConstant()\n",
    "# Then, we can invoke the pass on the given module. Note that the constant\n",
    "# folding pass works at the function-level. That being said, each function in\n",
    "# the module will be applied with the optimization. Users don't need to iterate\n",
    "# through individual functions manually to apply this pass.\n",
    "mod1 = fold_const(mod)\n",
    "# We can see from the updated program that the constants are folded.\n",
    "print(mod1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%x: Tensor[(1, 64, 56, 56), float32], %weight: Tensor[(64, 64, 3, 3), float32]) -> Tensor[(1, 64, 54, 54), float32] {\n",
      "  %0 = nn.conv2d(%x, %weight, padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %1 = add(%0, meta[relay.Constant][0] /* ty=Tensor[(1, 64, 54, 54), float32] */) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  %2 = add(%1, meta[relay.Constant][1] /* ty=Tensor[(1, 64, 54, 54), float32] */) /* ty=Tensor[(1, 64, 54, 54), float32] */;\n",
      "  add(%2, %2) /* ty=Tensor[(1, 64, 54, 54), float32] */\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod2 = relay.transform.EliminateCommonSubexpr()(mod1)\n",
    "print(mod2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  14: TVMFuncCall\n  13: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::transform::Pass, tvm::IRModule)>::AssignTypedLambda<tvm::transform::{lambda(tvm::transform::Pass, tvm::IRModule)#7}>(tvm::transform::{lambda(tvm::transform::Pass, tvm::IRModule)#7}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  12: tvm::transform::Pass::operator()(tvm::IRModule) const\n  11: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  10: tvm::relay::transform::FunctionPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  9: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::relay::Function (tvm::relay::Function, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::relay::transform::FuseOps(int)::{lambda(tvm::relay::Function, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::relay::transform::FuseOps(int)::{lambda(tvm::relay::Function, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  8: tvm::relay::FuseOps(tvm::RelayExpr const&, int, unsigned long, tvm::IRModule const&)\n  7: tvm::relay::FuseMutator::Transform(tvm::RelayExpr const&, int, unsigned long)\n  6: tvm::relay::IndexedForwardGraph::Create(tvm::support::GenericArena<tvm::support::SimplePageAllocator>*, tvm::RelayExpr const&)\n  5: tvm::relay::ExprVisitor::VisitExpr(tvm::RelayExpr const&)\n  4: tvm::relay::IndexedForwardGraph::Creator::VisitExpr_(tvm::relay::FunctionNode const*)\n  3: tvm::relay::ExprVisitor::VisitExpr_(tvm::relay::FunctionNode const*)\n  2: tvm::relay::ExprVisitor::VisitExpr(tvm::RelayExpr const&)\n  1: tvm::relay::IndexedForwardGraph::Creator::VisitExpr_(tvm::relay::CallNode const*)\n  0: tvm::RelayExprNode::checked_type() const\n  File \"/home/j/tvm-slicer/include/tvm/include/tvm/ir/expr.h\", line 475\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (checked_type_.defined()) is false: internal error: the type checker has not populated the checked_type field for CallNode(Op(add), [CallNode(Op(add), [CallNode(Op(add), [CallNode(Op(nn.conv2d), [Var(x, ty=TensorType([1, 64, 56, 56], float32)), Var(weight, ty=TensorType([64, 64, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x1a1956e8), []), CallNode(Op(multiply), [CallNode(Op(add), [Constant([[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.66741014e-02\n    -1.36016917e+00 -2.64421725e+00]\n   [-4.11465645e-01 -1.32302451e+00 -1.27327085e+00 ... -1.14453161e+00\n    -5.43792248e-01  3.86265904e-01]\n   [ 4.55055267e-01 -2.26475727e-02  1.06080008e+00 ...  5.39571881e-01\n     6.99768841e-01  1.98648795e-01]\n   ...\n   [-9.26206529e-01 -1.84237456e+00  5.28953969e-01 ...  1.25595391e+00\n     9.79239464e-01  2.27478798e-03]\n   [-1.18007019e-01  1.36406839e+00 -3.67243737e-01 ...  5.43178013e-03\n     2.06253916e-01 -1.38207686e+00]\n   [ 4.52283978e-01  1.83468640e+00  1.61386681e+00 ...  1.05118561e+00\n    -2.40624007e-02 -1.70598209e-01]]\n\n  [[ 8.12199950e-01  8.40371668e-01 -1.98992276e+00 ...  1.36784673e+00\n    -1.53337136e-01  1.30889320e+00]\n   [-8.60946596e-01 -1.48725092e+00  9.49378490e-01 ... -2.11173203e-02\n     3.79850082e-02 -2.23771378e-01]\n   [-4.73900378e-01 -7.52061427e-01  4.46268201e-01 ...  6.27109945e-01\n    -9.03462887e-01 -5.92898726e-01]\n   ...\n   [-3.31205511e+00 -1.13749492e+00 -6.45402730e-01 ...  2.15534282e+00\n    -1.37266638e-02 -1.38680589e+00]\n   [-7.37232566e-02  1.43600106e-02  1.35434127e+00 ...  7.44642839e-02\n    -3.06443185e-01 -6.96511507e-01]\n   [ 4.58851963e-01 -2.86112934e-01  5.40829003e-02 ...  7.77311549e-02\n     2.63538122e-01  7.47988760e-01]]\n\n  [[-5.83339453e-01  6.27477705e-01  1.26614416e+00 ... -5.21698236e-01\n    -2.20017242e+00  8.03836528e-03]\n   [-1.46706298e-01  7.34090447e-01 -1.15049696e+00 ...  6.03682637e-01\n     1.33468047e-01  6.11706600e-02]\n   [-5.25465965e-01 -1.35533881e+00 -2.71581084e-01 ...  7.63601601e-01\n    -4.24022749e-02 -3.53040576e-01]\n   ...\n   [-6.14963114e-01 -4.30240542e-01 -7.32103229e-01 ...  1.94113895e-01\n     7.97250271e-01  1.27782667e+00]\n   [-1.30445808e-01  2.30272591e-01  8.17684457e-02 ... -9.25382912e-01\n     6.87287569e-01 -2.44663930e+00]\n   [ 2.55437422e+00  6.56390071e-01 -5.17573893e-01 ...  5.17595828e-01\n     1.89075708e+00  6.28534257e-01]]\n\n  ...\n\n  [[ 1.60160387e+00  1.48001301e+00  1.00542390e+00 ...  4.13084388e-01\n     7.41505206e-01  1.28634834e+00]\n   [ 1.75255075e-01 -4.19689298e-01  4.79065850e-02 ... -3.07148665e-01\n     1.97522259e+00  5.13377070e-01]\n   [ 6.13370895e-01 -1.14634424e-01 -6.22611582e-01 ...  4.53738391e-01\n     4.50554192e-01 -2.93912649e+00]\n   ...\n   [ 8.09042156e-02 -1.08365357e+00  1.29005861e+00 ...  1.41440660e-01\n    -1.13513753e-01  6.24023736e-01]\n   [-1.59668589e+00  4.94843215e-01  8.33825707e-01 ...  7.33713031e-01\n    -2.99754292e-01 -5.36540091e-01]\n   [ 1.34503484e-01 -5.59379458e-01 -9.36695635e-01 ... -2.43555278e-01\n     5.67731023e-01 -1.76169109e+00]]\n\n  [[-1.49234831e-01 -1.14689171e-01 -1.36011660e+00 ... -1.99518478e+00\n    -3.77886921e-01  1.48089498e-01]\n   [ 7.36416698e-01 -6.57431841e-01 -3.52052212e-01 ...  8.32062304e-01\n     1.29972053e+00  9.28789556e-01]\n   [-1.41303909e+00  1.18253279e+00  3.44581068e-01 ... -1.96349728e+00\n     9.73015249e-01  9.27874371e-02]\n   ...\n   [-2.02912569e-01  8.56642842e-01 -2.66288847e-01 ...  6.99093819e-01\n     9.19181108e-01  1.01225996e+00]\n   [-4.15730953e-01  8.51417840e-01 -2.94560678e-02 ... -1.59633803e+00\n    -3.28884393e-01 -1.25070572e+00]\n   [-3.74630690e-01 -1.48265600e-01 -6.77288413e-01 ... -1.47698379e+00\n     1.93633169e-01 -1.59089267e-01]]\n\n  [[ 6.01487637e-01 -2.37229705e-01  8.18106890e-01 ... -6.15804434e-01\n     8.79203498e-01  1.20912826e+00]\n   [-3.57045144e-01 -5.08658707e-01 -4.68377262e-01 ...  1.09971750e+00\n     1.75888345e-01  8.27478111e-01]\n   [-8.96502256e-01 -2.42691845e-01 -6.84654117e-01 ...  6.78359747e-01\n    -1.08821034e+00  1.00649631e+00]\n   ...\n   [-2.36663699e-01 -2.02354118e-02 -1.27195537e+00 ...  1.19090831e+00\n    -4.70652461e-01 -1.77716649e+00]\n   [-9.15824771e-01 -1.64313570e-01  1.02521372e+00 ... -4.68726128e-01\n    -1.58422709e+00 -1.78228244e-01]\n   [-2.91392970e+00 -4.32814658e-01  4.81758326e-01 ... -2.75747776e-01\n    -8.69198799e-01  2.72537053e-01]]]]), Constant([[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.66741014e-02\n    -1.36016917e+00 -2.64421725e+00]\n   [-4.11465645e-01 -1.32302451e+00 -1.27327085e+00 ... -1.14453161e+00\n    -5.43792248e-01  3.86265904e-01]\n   [ 4.55055267e-01 -2.26475727e-02  1.06080008e+00 ...  5.39571881e-01\n     6.99768841e-01  1.98648795e-01]\n   ...\n   [-9.26206529e-01 -1.84237456e+00  5.28953969e-01 ...  1.25595391e+00\n     9.79239464e-01  2.27478798e-03]\n   [-1.18007019e-01  1.36406839e+00 -3.67243737e-01 ...  5.43178013e-03\n     2.06253916e-01 -1.38207686e+00]\n   [ 4.52283978e-01  1.83468640e+00  1.61386681e+00 ...  1.05118561e+00\n    -2.40624007e-02 -1.70598209e-01]]\n\n  [[ 8.12199950e-01  8.40371668e-01 -1.98992276e+00 ...  1.36784673e+00\n    -1.53337136e-01  1.30889320e+00]\n   [-8.60946596e-01 -1.48725092e+00  9.49378490e-01 ... -2.11173203e-02\n     3.79850082e-02 -2.23771378e-01]\n   [-4.73900378e-01 -7.52061427e-01  4.46268201e-01 ...  6.27109945e-01\n    -9.03462887e-01 -5.92898726e-01]\n   ...\n   [-3.31205511e+00 -1.13749492e+00 -6.45402730e-01 ...  2.15534282e+00\n    -1.37266638e-02 -1.38680589e+00]\n   [-7.37232566e-02  1.43600106e-02  1.35434127e+00 ...  7.44642839e-02\n    -3.06443185e-01 -6.96511507e-01]\n   [ 4.58851963e-01 -2.86112934e-01  5.40829003e-02 ...  7.77311549e-02\n     2.63538122e-01  7.47988760e-01]]\n\n  [[-5.83339453e-01  6.27477705e-01  1.26614416e+00 ... -5.21698236e-01\n    -2.20017242e+00  8.03836528e-03]\n   [-1.46706298e-01  7.34090447e-01 -1.15049696e+00 ...  6.03682637e-01\n     1.33468047e-01  6.11706600e-02]\n   [-5.25465965e-01 -1.35533881e+00 -2.71581084e-01 ...  7.63601601e-01\n    -4.24022749e-02 -3.53040576e-01]\n   ...\n   [-6.14963114e-01 -4.30240542e-01 -7.32103229e-01 ...  1.94113895e-01\n     7.97250271e-01  1.27782667e+00]\n   [-1.30445808e-01  2.30272591e-01  8.17684457e-02 ... -9.25382912e-01\n     6.87287569e-01 -2.44663930e+00]\n   [ 2.55437422e+00  6.56390071e-01 -5.17573893e-01 ...  5.17595828e-01\n     1.89075708e+00  6.28534257e-01]]\n\n  ...\n\n  [[ 1.60160387e+00  1.48001301e+00  1.00542390e+00 ...  4.13084388e-01\n     7.41505206e-01  1.28634834e+00]\n   [ 1.75255075e-01 -4.19689298e-01  4.79065850e-02 ... -3.07148665e-01\n     1.97522259e+00  5.13377070e-01]\n   [ 6.13370895e-01 -1.14634424e-01 -6.22611582e-01 ...  4.53738391e-01\n     4.50554192e-01 -2.93912649e+00]\n   ...\n   [ 8.09042156e-02 -1.08365357e+00  1.29005861e+00 ...  1.41440660e-01\n    -1.13513753e-01  6.24023736e-01]\n   [-1.59668589e+00  4.94843215e-01  8.33825707e-01 ...  7.33713031e-01\n    -2.99754292e-01 -5.36540091e-01]\n   [ 1.34503484e-01 -5.59379458e-01 -9.36695635e-01 ... -2.43555278e-01\n     5.67731023e-01 -1.76169109e+00]]\n\n  [[-1.49234831e-01 -1.14689171e-01 -1.36011660e+00 ... -1.99518478e+00\n    -3.77886921e-01  1.48089498e-01]\n   [ 7.36416698e-01 -6.57431841e-01 -3.52052212e-01 ...  8.32062304e-01\n     1.29972053e+00  9.28789556e-01]\n   [-1.41303909e+00  1.18253279e+00  3.44581068e-01 ... -1.96349728e+00\n     9.73015249e-01  9.27874371e-02]\n   ...\n   [-2.02912569e-01  8.56642842e-01 -2.66288847e-01 ...  6.99093819e-01\n     9.19181108e-01  1.01225996e+00]\n   [-4.15730953e-01  8.51417840e-01 -2.94560678e-02 ... -1.59633803e+00\n    -3.28884393e-01 -1.25070572e+00]\n   [-3.74630690e-01 -1.48265600e-01 -6.77288413e-01 ... -1.47698379e+00\n     1.93633169e-01 -1.59089267e-01]]\n\n  [[ 6.01487637e-01 -2.37229705e-01  8.18106890e-01 ... -6.15804434e-01\n     8.79203498e-01  1.20912826e+00]\n   [-3.57045144e-01 -5.08658707e-01 -4.68377262e-01 ...  1.09971750e+00\n     1.75888345e-01  8.27478111e-01]\n   [-8.96502256e-01 -2.42691845e-01 -6.84654117e-01 ...  6.78359747e-01\n    -1.08821034e+00  1.00649631e+00]\n   ...\n   [-2.36663699e-01 -2.02354118e-02 -1.27195537e+00 ...  1.19090831e+00\n    -4.70652461e-01 -1.77716649e+00]\n   [-9.15824771e-01 -1.64313570e-01  1.02521372e+00 ... -4.68726128e-01\n    -1.58422709e+00 -1.78228244e-01]\n   [-2.91392970e+00 -4.32814658e-01  4.81758326e-01 ... -2.75747776e-01\n    -8.69198799e-01  2.72537053e-01]]]])], (nullptr), []), Constant(2.0)], (nullptr), [])], (nullptr), []), Constant([[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.66741014e-02\n    -1.36016917e+00 -2.64421725e+00]\n   [-4.11465645e-01 -1.32302451e+00 -1.27327085e+00 ... -1.14453161e+00\n    -5.43792248e-01  3.86265904e-01]\n   [ 4.55055267e-01 -2.26475727e-02  1.06080008e+00 ...  5.39571881e-01\n     6.99768841e-01  1.98648795e-01]\n   ...\n   [-9.26206529e-01 -1.84237456e+00  5.28953969e-01 ...  1.25595391e+00\n     9.79239464e-01  2.27478798e-03]\n   [-1.18007019e-01  1.36406839e+00 -3.67243737e-01 ...  5.43178013e-03\n     2.06253916e-01 -1.38207686e+00]\n   [ 4.52283978e-01  1.83468640e+00  1.61386681e+00 ...  1.05118561e+00\n    -2.40624007e-02 -1.70598209e-01]]\n\n  [[ 8.12199950e-01  8.40371668e-01 -1.98992276e+00 ...  1.36784673e+00\n    -1.53337136e-01  1.30889320e+00]\n   [-8.60946596e-01 -1.48725092e+00  9.49378490e-01 ... -2.11173203e-02\n     3.79850082e-02 -2.23771378e-01]\n   [-4.73900378e-01 -7.52061427e-01  4.46268201e-01 ...  6.27109945e-01\n    -9.03462887e-01 -5.92898726e-01]\n   ...\n   [-3.31205511e+00 -1.13749492e+00 -6.45402730e-01 ...  2.15534282e+00\n    -1.37266638e-02 -1.38680589e+00]\n   [-7.37232566e-02  1.43600106e-02  1.35434127e+00 ...  7.44642839e-02\n    -3.06443185e-01 -6.96511507e-01]\n   [ 4.58851963e-01 -2.86112934e-01  5.40829003e-02 ...  7.77311549e-02\n     2.63538122e-01  7.47988760e-01]]\n\n  [[-5.83339453e-01  6.27477705e-01  1.26614416e+00 ... -5.21698236e-01\n    -2.20017242e+00  8.03836528e-03]\n   [-1.46706298e-01  7.34090447e-01 -1.15049696e+00 ...  6.03682637e-01\n     1.33468047e-01  6.11706600e-02]\n   [-5.25465965e-01 -1.35533881e+00 -2.71581084e-01 ...  7.63601601e-01\n    -4.24022749e-02 -3.53040576e-01]\n   ...\n   [-6.14963114e-01 -4.30240542e-01 -7.32103229e-01 ...  1.94113895e-01\n     7.97250271e-01  1.27782667e+00]\n   [-1.30445808e-01  2.30272591e-01  8.17684457e-02 ... -9.25382912e-01\n     6.87287569e-01 -2.44663930e+00]\n   [ 2.55437422e+00  6.56390071e-01 -5.17573893e-01 ...  5.17595828e-01\n     1.89075708e+00  6.28534257e-01]]\n\n  ...\n\n  [[ 1.60160387e+00  1.48001301e+00  1.00542390e+00 ...  4.13084388e-01\n     7.41505206e-01  1.28634834e+00]\n   [ 1.75255075e-01 -4.19689298e-01  4.79065850e-02 ... -3.07148665e-01\n     1.97522259e+00  5.13377070e-01]\n   [ 6.13370895e-01 -1.14634424e-01 -6.22611582e-01 ...  4.53738391e-01\n     4.50554192e-01 -2.93912649e+00]\n   ...\n   [ 8.09042156e-02 -1.08365357e+00  1.29005861e+00 ...  1.41440660e-01\n    -1.13513753e-01  6.24023736e-01]\n   [-1.59668589e+00  4.94843215e-01  8.33825707e-01 ...  7.33713031e-01\n    -2.99754292e-01 -5.36540091e-01]\n   [ 1.34503484e-01 -5.59379458e-01 -9.36695635e-01 ... -2.43555278e-01\n     5.67731023e-01 -1.76169109e+00]]\n\n  [[-1.49234831e-01 -1.14689171e-01 -1.36011660e+00 ... -1.99518478e+00\n    -3.77886921e-01  1.48089498e-01]\n   [ 7.36416698e-01 -6.57431841e-01 -3.52052212e-01 ...  8.32062304e-01\n     1.29972053e+00  9.28789556e-01]\n   [-1.41303909e+00  1.18253279e+00  3.44581068e-01 ... -1.96349728e+00\n     9.73015249e-01  9.27874371e-02]\n   ...\n   [-2.02912569e-01  8.56642842e-01 -2.66288847e-01 ...  6.99093819e-01\n     9.19181108e-01  1.01225996e+00]\n   [-4.15730953e-01  8.51417840e-01 -2.94560678e-02 ... -1.59633803e+00\n    -3.28884393e-01 -1.25070572e+00]\n   [-3.74630690e-01 -1.48265600e-01 -6.77288413e-01 ... -1.47698379e+00\n     1.93633169e-01 -1.59089267e-01]]\n\n  [[ 6.01487637e-01 -2.37229705e-01  8.18106890e-01 ... -6.15804434e-01\n     8.79203498e-01  1.20912826e+00]\n   [-3.57045144e-01 -5.08658707e-01 -4.68377262e-01 ...  1.09971750e+00\n     1.75888345e-01  8.27478111e-01]\n   [-8.96502256e-01 -2.42691845e-01 -6.84654117e-01 ...  6.78359747e-01\n    -1.08821034e+00  1.00649631e+00]\n   ...\n   [-2.36663699e-01 -2.02354118e-02 -1.27195537e+00 ...  1.19090831e+00\n    -4.70652461e-01 -1.77716649e+00]\n   [-9.15824771e-01 -1.64313570e-01  1.02521372e+00 ... -4.68726128e-01\n    -1.58422709e+00 -1.78228244e-01]\n   [-2.91392970e+00 -4.32814658e-01  4.81758326e-01 ... -2.75747776e-01\n    -8.69198799e-01  2.72537053e-01]]]])], (nullptr), []), CallNode(Op(add), [CallNode(Op(add), [CallNode(Op(nn.conv2d), [Var(x, ty=TensorType([1, 64, 56, 56], float32)), Var(weight, ty=TensorType([64, 64, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x1a1956e8), []), CallNode(Op(multiply), [CallNode(Op(add), [Constant([[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.66741014e-02\n    -1.36016917e+00 -2.64421725e+00]\n   [-4.11465645e-01 -1.32302451e+00 -1.27327085e+00 ... -1.14453161e+00\n    -5.43792248e-01  3.86265904e-01]\n   [ 4.55055267e-01 -2.26475727e-02  1.06080008e+00 ...  5.39571881e-01\n     6.99768841e-01  1.98648795e-01]\n   ...\n   [-9.26206529e-01 -1.84237456e+00  5.28953969e-01 ...  1.25595391e+00\n     9.79239464e-01  2.27478798e-03]\n   [-1.18007019e-01  1.36406839e+00 -3.67243737e-01 ...  5.43178013e-03\n     2.06253916e-01 -1.38207686e+00]\n   [ 4.52283978e-01  1.83468640e+00  1.61386681e+00 ...  1.05118561e+00\n    -2.40624007e-02 -1.70598209e-01]]\n\n  [[ 8.12199950e-01  8.40371668e-01 -1.98992276e+00 ...  1.36784673e+00\n    -1.53337136e-01  1.30889320e+00]\n   [-8.60946596e-01 -1.48725092e+00  9.49378490e-01 ... -2.11173203e-02\n     3.79850082e-02 -2.23771378e-01]\n   [-4.73900378e-01 -7.52061427e-01  4.46268201e-01 ...  6.27109945e-01\n    -9.03462887e-01 -5.92898726e-01]\n   ...\n   [-3.31205511e+00 -1.13749492e+00 -6.45402730e-01 ...  2.15534282e+00\n    -1.37266638e-02 -1.38680589e+00]\n   [-7.37232566e-02  1.43600106e-02  1.35434127e+00 ...  7.44642839e-02\n    -3.06443185e-01 -6.96511507e-01]\n   [ 4.58851963e-01 -2.86112934e-01  5.40829003e-02 ...  7.77311549e-02\n     2.63538122e-01  7.47988760e-01]]\n\n  [[-5.83339453e-01  6.27477705e-01  1.26614416e+00 ... -5.21698236e-01\n    -2.20017242e+00  8.03836528e-03]\n   [-1.46706298e-01  7.34090447e-01 -1.15049696e+00 ...  6.03682637e-01\n     1.33468047e-01  6.11706600e-02]\n   [-5.25465965e-01 -1.35533881e+00 -2.71581084e-01 ...  7.63601601e-01\n    -4.24022749e-02 -3.53040576e-01]\n   ...\n   [-6.14963114e-01 -4.30240542e-01 -7.32103229e-01 ...  1.94113895e-01\n     7.97250271e-01  1.27782667e+00]\n   [-1.30445808e-01  2.30272591e-01  8.17684457e-02 ... -9.25382912e-01\n     6.87287569e-01 -2.44663930e+00]\n   [ 2.55437422e+00  6.56390071e-01 -5.17573893e-01 ...  5.17595828e-01\n     1.89075708e+00  6.28534257e-01]]\n\n  ...\n\n  [[ 1.60160387e+00  1.48001301e+00  1.00542390e+00 ...  4.13084388e-01\n     7.41505206e-01  1.28634834e+00]\n   [ 1.75255075e-01 -4.19689298e-01  4.79065850e-02 ... -3.07148665e-01\n     1.97522259e+00  5.13377070e-01]\n   [ 6.13370895e-01 -1.14634424e-01 -6.22611582e-01 ...  4.53738391e-01\n     4.50554192e-01 -2.93912649e+00]\n   ...\n   [ 8.09042156e-02 -1.08365357e+00  1.29005861e+00 ...  1.41440660e-01\n    -1.13513753e-01  6.24023736e-01]\n   [-1.59668589e+00  4.94843215e-01  8.33825707e-01 ...  7.33713031e-01\n    -2.99754292e-01 -5.36540091e-01]\n   [ 1.34503484e-01 -5.59379458e-01 -9.36695635e-01 ... -2.43555278e-01\n     5.67731023e-01 -1.76169109e+00]]\n\n  [[-1.49234831e-01 -1.14689171e-01 -1.36011660e+00 ... -1.99518478e+00\n    -3.77886921e-01  1.48089498e-01]\n   [ 7.36416698e-01 -6.57431841e-01 -3.52052212e-01 ...  8.32062304e-01\n     1.29972053e+00  9.28789556e-01]\n   [-1.41303909e+00  1.18253279e+00  3.44581068e-01 ... -1.96349728e+00\n     9.73015249e-01  9.27874371e-02]\n   ...\n   [-2.02912569e-01  8.56642842e-01 -2.66288847e-01 ...  6.99093819e-01\n     9.19181108e-01  1.01225996e+00]\n   [-4.15730953e-01  8.51417840e-01 -2.94560678e-02 ... -1.59633803e+00\n    -3.28884393e-01 -1.25070572e+00]\n   [-3.74630690e-01 -1.48265600e-01 -6.77288413e-01 ... -1.47698379e+00\n     1.93633169e-01 -1.59089267e-01]]\n\n  [[ 6.01487637e-01 -2.37229705e-01  8.18106890e-01 ... -6.15804434e-01\n     8.79203498e-01  1.20912826e+00]\n   [-3.57045144e-01 -5.08658707e-01 -4.68377262e-01 ...  1.09971750e+00\n     1.75888345e-01  8.27478111e-01]\n   [-8.96502256e-01 -2.42691845e-01 -6.84654117e-01 ...  6.78359747e-01\n    -1.08821034e+00  1.00649631e+00]\n   ...\n   [-2.36663699e-01 -2.02354118e-02 -1.27195537e+00 ...  1.19090831e+00\n    -4.70652461e-01 -1.77716649e+00]\n   [-9.15824771e-01 -1.64313570e-01  1.02521372e+00 ... -4.68726128e-01\n    -1.58422709e+00 -1.78228244e-01]\n   [-2.91392970e+00 -4.32814658e-01  4.81758326e-01 ... -2.75747776e-01\n    -8.69198799e-01  2.72537053e-01]]]]), Constant([[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.66741014e-02\n    -1.36016917e+00 -2.64421725e+00]\n   [-4.11465645e-01 -1.32302451e+00 -1.27327085e+00 ... -1.14453161e+00\n    -5.43792248e-01  3.86265904e-01]\n   [ 4.55055267e-01 -2.26475727e-02  1.06080008e+00 ...  5.39571881e-01\n     6.99768841e-01  1.98648795e-01]\n   ...\n   [-9.26206529e-01 -1.84237456e+00  5.28953969e-01 ...  1.25595391e+00\n     9.79239464e-01  2.27478798e-03]\n   [-1.18007019e-01  1.36406839e+00 -3.67243737e-01 ...  5.43178013e-03\n     2.06253916e-01 -1.38207686e+00]\n   [ 4.52283978e-01  1.83468640e+00  1.61386681e+00 ...  1.05118561e+00\n    -2.40624007e-02 -1.70598209e-01]]\n\n  [[ 8.12199950e-01  8.40371668e-01 -1.98992276e+00 ...  1.36784673e+00\n    -1.53337136e-01  1.30889320e+00]\n   [-8.60946596e-01 -1.48725092e+00  9.49378490e-01 ... -2.11173203e-02\n     3.79850082e-02 -2.23771378e-01]\n   [-4.73900378e-01 -7.52061427e-01  4.46268201e-01 ...  6.27109945e-01\n    -9.03462887e-01 -5.92898726e-01]\n   ...\n   [-3.31205511e+00 -1.13749492e+00 -6.45402730e-01 ...  2.15534282e+00\n    -1.37266638e-02 -1.38680589e+00]\n   [-7.37232566e-02  1.43600106e-02  1.35434127e+00 ...  7.44642839e-02\n    -3.06443185e-01 -6.96511507e-01]\n   [ 4.58851963e-01 -2.86112934e-01  5.40829003e-02 ...  7.77311549e-02\n     2.63538122e-01  7.47988760e-01]]\n\n  [[-5.83339453e-01  6.27477705e-01  1.26614416e+00 ... -5.21698236e-01\n    -2.20017242e+00  8.03836528e-03]\n   [-1.46706298e-01  7.34090447e-01 -1.15049696e+00 ...  6.03682637e-01\n     1.33468047e-01  6.11706600e-02]\n   [-5.25465965e-01 -1.35533881e+00 -2.71581084e-01 ...  7.63601601e-01\n    -4.24022749e-02 -3.53040576e-01]\n   ...\n   [-6.14963114e-01 -4.30240542e-01 -7.32103229e-01 ...  1.94113895e-01\n     7.97250271e-01  1.27782667e+00]\n   [-1.30445808e-01  2.30272591e-01  8.17684457e-02 ... -9.25382912e-01\n     6.87287569e-01 -2.44663930e+00]\n   [ 2.55437422e+00  6.56390071e-01 -5.17573893e-01 ...  5.17595828e-01\n     1.89075708e+00  6.28534257e-01]]\n\n  ...\n\n  [[ 1.60160387e+00  1.48001301e+00  1.00542390e+00 ...  4.13084388e-01\n     7.41505206e-01  1.28634834e+00]\n   [ 1.75255075e-01 -4.19689298e-01  4.79065850e-02 ... -3.07148665e-01\n     1.97522259e+00  5.13377070e-01]\n   [ 6.13370895e-01 -1.14634424e-01 -6.22611582e-01 ...  4.53738391e-01\n     4.50554192e-01 -2.93912649e+00]\n   ...\n   [ 8.09042156e-02 -1.08365357e+00  1.29005861e+00 ...  1.41440660e-01\n    -1.13513753e-01  6.24023736e-01]\n   [-1.59668589e+00  4.94843215e-01  8.33825707e-01 ...  7.33713031e-01\n    -2.99754292e-01 -5.36540091e-01]\n   [ 1.34503484e-01 -5.59379458e-01 -9.36695635e-01 ... -2.43555278e-01\n     5.67731023e-01 -1.76169109e+00]]\n\n  [[-1.49234831e-01 -1.14689171e-01 -1.36011660e+00 ... -1.99518478e+00\n    -3.77886921e-01  1.48089498e-01]\n   [ 7.36416698e-01 -6.57431841e-01 -3.52052212e-01 ...  8.32062304e-01\n     1.29972053e+00  9.28789556e-01]\n   [-1.41303909e+00  1.18253279e+00  3.44581068e-01 ... -1.96349728e+00\n     9.73015249e-01  9.27874371e-02]\n   ...\n   [-2.02912569e-01  8.56642842e-01 -2.66288847e-01 ...  6.99093819e-01\n     9.19181108e-01  1.01225996e+00]\n   [-4.15730953e-01  8.51417840e-01 -2.94560678e-02 ... -1.59633803e+00\n    -3.28884393e-01 -1.25070572e+00]\n   [-3.74630690e-01 -1.48265600e-01 -6.77288413e-01 ... -1.47698379e+00\n     1.93633169e-01 -1.59089267e-01]]\n\n  [[ 6.01487637e-01 -2.37229705e-01  8.18106890e-01 ... -6.15804434e-01\n     8.79203498e-01  1.20912826e+00]\n   [-3.57045144e-01 -5.08658707e-01 -4.68377262e-01 ...  1.09971750e+00\n     1.75888345e-01  8.27478111e-01]\n   [-8.96502256e-01 -2.42691845e-01 -6.84654117e-01 ...  6.78359747e-01\n    -1.08821034e+00  1.00649631e+00]\n   ...\n   [-2.36663699e-01 -2.02354118e-02 -1.27195537e+00 ...  1.19090831e+00\n    -4.70652461e-01 -1.77716649e+00]\n   [-9.15824771e-01 -1.64313570e-01  1.02521372e+00 ... -4.68726128e-01\n    -1.58422709e+00 -1.78228244e-01]\n   [-2.91392970e+00 -4.32814658e-01  4.81758326e-01 ... -2.75747776e-01\n    -8.69198799e-01  2.72537053e-01]]]])], (nullptr), []), Constant(2.0)], (nullptr), [])], (nullptr), []), Constant([[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.66741014e-02\n    -1.36016917e+00 -2.64421725e+00]\n   [-4.11465645e-01 -1.32302451e+00 -1.27327085e+00 ... -1.14453161e+00\n    -5.43792248e-01  3.86265904e-01]\n   [ 4.55055267e-01 -2.26475727e-02  1.06080008e+00 ...  5.39571881e-01\n     6.99768841e-01  1.98648795e-01]\n   ...\n   [-9.26206529e-01 -1.84237456e+00  5.28953969e-01 ...  1.25595391e+00\n     9.79239464e-01  2.27478798e-03]\n   [-1.18007019e-01  1.36406839e+00 -3.67243737e-01 ...  5.43178013e-03\n     2.06253916e-01 -1.38207686e+00]\n   [ 4.52283978e-01  1.83468640e+00  1.61386681e+00 ...  1.05118561e+00\n    -2.40624007e-02 -1.70598209e-01]]\n\n  [[ 8.12199950e-01  8.40371668e-01 -1.98992276e+00 ...  1.36784673e+00\n    -1.53337136e-01  1.30889320e+00]\n   [-8.60946596e-01 -1.48725092e+00  9.49378490e-01 ... -2.11173203e-02\n     3.79850082e-02 -2.23771378e-01]\n   [-4.73900378e-01 -7.52061427e-01  4.46268201e-01 ...  6.27109945e-01\n    -9.03462887e-01 -5.92898726e-01]\n   ...\n   [-3.31205511e+00 -1.13749492e+00 -6.45402730e-01 ...  2.15534282e+00\n    -1.37266638e-02 -1.38680589e+00]\n   [-7.37232566e-02  1.43600106e-02  1.35434127e+00 ...  7.44642839e-02\n    -3.06443185e-01 -6.96511507e-01]\n   [ 4.58851963e-01 -2.86112934e-01  5.40829003e-02 ...  7.77311549e-02\n     2.63538122e-01  7.47988760e-01]]\n\n  [[-5.83339453e-01  6.27477705e-01  1.26614416e+00 ... -5.21698236e-01\n    -2.20017242e+00  8.03836528e-03]\n   [-1.46706298e-01  7.34090447e-01 -1.15049696e+00 ...  6.03682637e-01\n     1.33468047e-01  6.11706600e-02]\n   [-5.25465965e-01 -1.35533881e+00 -2.71581084e-01 ...  7.63601601e-01\n    -4.24022749e-02 -3.53040576e-01]\n   ...\n   [-6.14963114e-01 -4.30240542e-01 -7.32103229e-01 ...  1.94113895e-01\n     7.97250271e-01  1.27782667e+00]\n   [-1.30445808e-01  2.30272591e-01  8.17684457e-02 ... -9.25382912e-01\n     6.87287569e-01 -2.44663930e+00]\n   [ 2.55437422e+00  6.56390071e-01 -5.17573893e-01 ...  5.17595828e-01\n     1.89075708e+00  6.28534257e-01]]\n\n  ...\n\n  [[ 1.60160387e+00  1.48001301e+00  1.00542390e+00 ...  4.13084388e-01\n     7.41505206e-01  1.28634834e+00]\n   [ 1.75255075e-01 -4.19689298e-01  4.79065850e-02 ... -3.07148665e-01\n     1.97522259e+00  5.13377070e-01]\n   [ 6.13370895e-01 -1.14634424e-01 -6.22611582e-01 ...  4.53738391e-01\n     4.50554192e-01 -2.93912649e+00]\n   ...\n   [ 8.09042156e-02 -1.08365357e+00  1.29005861e+00 ...  1.41440660e-01\n    -1.13513753e-01  6.24023736e-01]\n   [-1.59668589e+00  4.94843215e-01  8.33825707e-01 ...  7.33713031e-01\n    -2.99754292e-01 -5.36540091e-01]\n   [ 1.34503484e-01 -5.59379458e-01 -9.36695635e-01 ... -2.43555278e-01\n     5.67731023e-01 -1.76169109e+00]]\n\n  [[-1.49234831e-01 -1.14689171e-01 -1.36011660e+00 ... -1.99518478e+00\n    -3.77886921e-01  1.48089498e-01]\n   [ 7.36416698e-01 -6.57431841e-01 -3.52052212e-01 ...  8.32062304e-01\n     1.29972053e+00  9.28789556e-01]\n   [-1.41303909e+00  1.18253279e+00  3.44581068e-01 ... -1.96349728e+00\n     9.73015249e-01  9.27874371e-02]\n   ...\n   [-2.02912569e-01  8.56642842e-01 -2.66288847e-01 ...  6.99093819e-01\n     9.19181108e-01  1.01225996e+00]\n   [-4.15730953e-01  8.51417840e-01 -2.94560678e-02 ... -1.59633803e+00\n    -3.28884393e-01 -1.25070572e+00]\n   [-3.74630690e-01 -1.48265600e-01 -6.77288413e-01 ... -1.47698379e+00\n     1.93633169e-01 -1.59089267e-01]]\n\n  [[ 6.01487637e-01 -2.37229705e-01  8.18106890e-01 ... -6.15804434e-01\n     8.79203498e-01  1.20912826e+00]\n   [-3.57045144e-01 -5.08658707e-01 -4.68377262e-01 ...  1.09971750e+00\n     1.75888345e-01  8.27478111e-01]\n   [-8.96502256e-01 -2.42691845e-01 -6.84654117e-01 ...  6.78359747e-01\n    -1.08821034e+00  1.00649631e+00]\n   ...\n   [-2.36663699e-01 -2.02354118e-02 -1.27195537e+00 ...  1.19090831e+00\n    -4.70652461e-01 -1.77716649e+00]\n   [-9.15824771e-01 -1.64313570e-01  1.02521372e+00 ... -4.68726128e-01\n    -1.58422709e+00 -1.78228244e-01]\n   [-2.91392970e+00 -4.32814658e-01  4.81758326e-01 ... -2.75747776e-01\n    -8.69198799e-01  2.72537053e-01]]]])], (nullptr), [])], (nullptr), [])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000019vscode-remote?line=0'>1</a>\u001b[0m mod \u001b[39m=\u001b[39m relay\u001b[39m.\u001b[39;49mtransform\u001b[39m.\u001b[39;49mFuseOps(fuse_opt_level\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)(mod)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000019vscode-remote?line=2'>3</a>\u001b[0m \u001b[39m# We can observe that the optimized module contains functions that only have\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000019vscode-remote?line=3'>4</a>\u001b[0m \u001b[39m# a signle primitive op.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000019vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(mod)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/ir/transform.py:161\u001b[0m, in \u001b[0;36mPass.__call__\u001b[0;34m(self, mod)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/transform.py?line=146'>147</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, mod):\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/transform.py?line=147'>148</a>\u001b[0m     \u001b[39m\"\"\"Execute the pass. Note that for sequential pass, the dependency among\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/transform.py?line=148'>149</a>\u001b[0m \u001b[39m    different passes will be resolved in the backend.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/transform.py?line=149'>150</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/transform.py?line=158'>159</a>\u001b[0m \u001b[39m        The updated module after applying this pass.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/transform.py?line=159'>160</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/transform.py?line=160'>161</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _ffi_transform_api\u001b[39m.\u001b[39;49mRunPass(\u001b[39mself\u001b[39;49m, mod)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=224'>225</a>\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=225'>226</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=226'>227</a>\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=227'>228</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=234'>235</a>\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=235'>236</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=236'>237</a>\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=237'>238</a>\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=238'>239</a>\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  14: TVMFuncCall\n  13: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::transform::Pass, tvm::IRModule)>::AssignTypedLambda<tvm::transform::{lambda(tvm::transform::Pass, tvm::IRModule)#7}>(tvm::transform::{lambda(tvm::transform::Pass, tvm::IRModule)#7}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  12: tvm::transform::Pass::operator()(tvm::IRModule) const\n  11: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  10: tvm::relay::transform::FunctionPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  9: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::relay::Function (tvm::relay::Function, tvm::IRModule, tvm::transform::PassContext)>::AssignTypedLambda<tvm::relay::transform::FuseOps(int)::{lambda(tvm::relay::Function, tvm::IRModule, tvm::transform::PassContext)#1}>(tvm::relay::transform::FuseOps(int)::{lambda(tvm::relay::Function, tvm::IRModule, tvm::transform::PassContext)#1})::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)\n  8: tvm::relay::FuseOps(tvm::RelayExpr const&, int, unsigned long, tvm::IRModule const&)\n  7: tvm::relay::FuseMutator::Transform(tvm::RelayExpr const&, int, unsigned long)\n  6: tvm::relay::IndexedForwardGraph::Create(tvm::support::GenericArena<tvm::support::SimplePageAllocator>*, tvm::RelayExpr const&)\n  5: tvm::relay::ExprVisitor::VisitExpr(tvm::RelayExpr const&)\n  4: tvm::relay::IndexedForwardGraph::Creator::VisitExpr_(tvm::relay::FunctionNode const*)\n  3: tvm::relay::ExprVisitor::VisitExpr_(tvm::relay::FunctionNode const*)\n  2: tvm::relay::ExprVisitor::VisitExpr(tvm::RelayExpr const&)\n  1: tvm::relay::IndexedForwardGraph::Creator::VisitExpr_(tvm::relay::CallNode const*)\n  0: tvm::RelayExprNode::checked_type() const\n  File \"/home/j/tvm-slicer/include/tvm/include/tvm/ir/expr.h\", line 475\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (checked_type_.defined()) is false: internal error: the type checker has not populated the checked_type field for CallNode(Op(add), [CallNode(Op(add), [CallNode(Op(add), [CallNode(Op(nn.conv2d), [Var(x, ty=TensorType([1, 64, 56, 56], float32)), Var(weight, ty=TensorType([64, 64, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x1a1956e8), []), CallNode(Op(multiply), [CallNode(Op(add), [Constant([[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.66741014e-02\n    -1.36016917e+00 -2.64421725e+00]\n   [-4.11465645e-01 -1.32302451e+00 -1.27327085e+00 ... -1.14453161e+00\n    -5.43792248e-01  3.86265904e-01]\n   [ 4.55055267e-01 -2.26475727e-02  1.06080008e+00 ...  5.39571881e-01\n     6.99768841e-01  1.98648795e-01]\n   ...\n   [-9.26206529e-01 -1.84237456e+00  5.28953969e-01 ...  1.25595391e+00\n     9.79239464e-01  2.27478798e-03]\n   [-1.18007019e-01  1.36406839e+00 -3.67243737e-01 ...  5.43178013e-03\n     2.06253916e-01 -1.38207686e+00]\n   [ 4.52283978e-01  1.83468640e+00  1.61386681e+00 ...  1.05118561e+00\n    -2.40624007e-02 -1.70598209e-01]]\n\n  [[ 8.12199950e-01  8.40371668e-01 -1.98992276e+00 ...  1.36784673e+00\n    -1.53337136e-01  1.30889320e+00]\n   [-8.60946596e-01 -1.48725092e+00  9.49378490e-01 ... -2.11173203e-02\n     3.79850082e-02 -2.23771378e-01]\n   [-4.73900378e-01 -7.52061427e-01  4.46268201e-01 ...  6.27109945e-01\n    -9.03462887e-01 -5.92898726e-01]\n   ...\n   [-3.31205511e+00 -1.13749492e+00 -6.45402730e-01 ...  2.15534282e+00\n    -1.37266638e-02 -1.38680589e+00]\n   [-7.37232566e-02  1.43600106e-02  1.35434127e+00 ...  7.44642839e-02\n    -3.06443185e-01 -6.96511507e-01]\n   [ 4.58851963e-01 -2.86112934e-01  5.40829003e-02 ...  7.77311549e-02\n     2.63538122e-01  7.47988760e-01]]\n\n  [[-5.83339453e-01  6.27477705e-01  1.26614416e+00 ... -5.21698236e-01\n    -2.20017242e+00  8.03836528e-03]\n   [-1.46706298e-01  7.34090447e-01 -1.15049696e+00 ...  6.03682637e-01\n     1.33468047e-01  6.11706600e-02]\n   [-5.25465965e-01 -1.35533881e+00 -2.71581084e-01 ...  7.63601601e-01\n    -4.24022749e-02 -3.53040576e-01]\n   ...\n   [-6.14963114e-01 -4.30240542e-01 -7.32103229e-01 ...  1.94113895e-01\n     7.97250271e-01  1.27782667e+00]\n   [-1.30445808e-01  2.30272591e-01  8.17684457e-02 ... -9.25382912e-01\n     6.87287569e-01 -2.44663930e+00]\n   [ 2.55437422e+00  6.56390071e-01 -5.17573893e-01 ...  5.17595828e-01\n     1.89075708e+00  6.28534257e-01]]\n\n  ...\n\n  [[ 1.60160387e+00  1.48001301e+00  1.00542390e+00 ...  4.13084388e-01\n     7.41505206e-01  1.28634834e+00]\n   [ 1.75255075e-01 -4.19689298e-01  4.79065850e-02 ... -3.07148665e-01\n     1.97522259e+00  5.13377070e-01]\n   [ 6.13370895e-01 -1.14634424e-01 -6.22611582e-01 ...  4.53738391e-01\n     4.50554192e-01 -2.93912649e+00]\n   ...\n   [ 8.09042156e-02 -1.08365357e+00  1.29005861e+00 ...  1.41440660e-01\n    -1.13513753e-01  6.24023736e-01]\n   [-1.59668589e+00  4.94843215e-01  8.33825707e-01 ...  7.33713031e-01\n    -2.99754292e-01 -5.36540091e-01]\n   [ 1.34503484e-01 -5.59379458e-01 -9.36695635e-01 ... -2.43555278e-01\n     5.67731023e-01 -1.76169109e+00]]\n\n  [[-1.49234831e-01 -1.14689171e-01 -1.36011660e+00 ... -1.99518478e+00\n    -3.77886921e-01  1.48089498e-01]\n   [ 7.36416698e-01 -6.57431841e-01 -3.52052212e-01 ...  8.32062304e-01\n     1.29972053e+00  9.28789556e-01]\n   [-1.41303909e+00  1.18253279e+00  3.44581068e-01 ... -1.96349728e+00\n     9.73015249e-01  9.27874371e-02]\n   ...\n   [-2.02912569e-01  8.56642842e-01 -2.66288847e-01 ...  6.99093819e-01\n     9.19181108e-01  1.01225996e+00]\n   [-4.15730953e-01  8.51417840e-01 -2.94560678e-02 ... -1.59633803e+00\n    -3.28884393e-01 -1.25070572e+00]\n   [-3.74630690e-01 -1.48265600e-01 -6.77288413e-01 ... -1.47698379e+00\n     1.93633169e-01 -1.59089267e-01]]\n\n  [[ 6.01487637e-01 -2.37229705e-01  8.18106890e-01 ... -6.15804434e-01\n     8.79203498e-01  1.20912826e+00]\n   [-3.57045144e-01 -5.08658707e-01 -4.68377262e-01 ...  1.09971750e+00\n     1.75888345e-01  8.27478111e-01]\n   [-8.96502256e-01 -2.42691845e-01 -6.84654117e-01 ...  6.78359747e-01\n    -1.08821034e+00  1.00649631e+00]\n   ...\n   [-2.36663699e-01 -2.02354118e-02 -1.27195537e+00 ...  1.19090831e+00\n    -4.70652461e-01 -1.77716649e+00]\n   [-9.15824771e-01 -1.64313570e-01  1.02521372e+00 ... -4.68726128e-01\n    -1.58422709e+00 -1.78228244e-01]\n   [-2.91392970e+00 -4.32814658e-01  4.81758326e-01 ... -2.75747776e-01\n    -8.69198799e-01  2.72537053e-01]]]]), Constant([[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.66741014e-02\n    -1.36016917e+00 -2.64421725e+00]\n   [-4.11465645e-01 -1.32302451e+00 -1.27327085e+00 ... -1.14453161e+00\n    -5.43792248e-01  3.86265904e-01]\n   [ 4.55055267e-01 -2.26475727e-02  1.06080008e+00 ...  5.39571881e-01\n     6.99768841e-01  1.98648795e-01]\n   ...\n   [-9.26206529e-01 -1.84237456e+00  5.28953969e-01 ...  1.25595391e+00\n     9.79239464e-01  2.27478798e-03]\n   [-1.18007019e-01  1.36406839e+00 -3.67243737e-01 ...  5.43178013e-03\n     2.06253916e-01 -1.38207686e+00]\n   [ 4.52283978e-01  1.83468640e+00  1.61386681e+00 ...  1.05118561e+00\n    -2.40624007e-02 -1.70598209e-01]]\n\n  [[ 8.12199950e-01  8.40371668e-01 -1.98992276e+00 ...  1.36784673e+00\n    -1.53337136e-01  1.30889320e+00]\n   [-8.60946596e-01 -1.48725092e+00  9.49378490e-01 ... -2.11173203e-02\n     3.79850082e-02 -2.23771378e-01]\n   [-4.73900378e-01 -7.52061427e-01  4.46268201e-01 ...  6.27109945e-01\n    -9.03462887e-01 -5.92898726e-01]\n   ...\n   [-3.31205511e+00 -1.13749492e+00 -6.45402730e-01 ...  2.15534282e+00\n    -1.37266638e-02 -1.38680589e+00]\n   [-7.37232566e-02  1.43600106e-02  1.35434127e+00 ...  7.44642839e-02\n    -3.06443185e-01 -6.96511507e-01]\n   [ 4.58851963e-01 -2.86112934e-01  5.40829003e-02 ...  7.77311549e-02\n     2.63538122e-01  7.47988760e-01]]\n\n  [[-5.83339453e-01  6.27477705e-01  1.26614416e+00 ... -5.21698236e-01\n    -2.20017242e+00  8.03836528e-03]\n   [-1.46706298e-01  7.34090447e-01 -1.15049696e+00 ...  6.03682637e-01\n     1.33468047e-01  6.11706600e-02]\n   [-5.25465965e-01 -1.35533881e+00 -2.71581084e-01 ...  7.63601601e-01\n    -4.24022749e-02 -3.53040576e-01]\n   ...\n   [-6.14963114e-01 -4.30240542e-01 -7.32103229e-01 ...  1.94113895e-01\n     7.97250271e-01  1.27782667e+00]\n   [-1.30445808e-01  2.30272591e-01  8.17684457e-02 ... -9.25382912e-01\n     6.87287569e-01 -2.44663930e+00]\n   [ 2.55437422e+00  6.56390071e-01 -5.17573893e-01 ...  5.17595828e-01\n     1.89075708e+00  6.28534257e-01]]\n\n  ...\n\n  [[ 1.60160387e+00  1.48001301e+00  1.00542390e+00 ...  4.13084388e-01\n     7.41505206e-01  1.28634834e+00]\n   [ 1.75255075e-01 -4.19689298e-01  4.79065850e-02 ... -3.07148665e-01\n     1.97522259e+00  5.13377070e-01]\n   [ 6.13370895e-01 -1.14634424e-01 -6.22611582e-01 ...  4.53738391e-01\n     4.50554192e-01 -2.93912649e+00]\n   ...\n   [ 8.09042156e-02 -1.08365357e+00  1.29005861e+00 ...  1.41440660e-01\n    -1.13513753e-01  6.24023736e-01]\n   [-1.59668589e+00  4.94843215e-01  8.33825707e-01 ...  7.33713031e-01\n    -2.99754292e-01 -5.36540091e-01]\n   [ 1.34503484e-01 -5.59379458e-01 -9.36695635e-01 ... -2.43555278e-01\n     5.67731023e-01 -1.76169109e+00]]\n\n  [[-1.49234831e-01 -1.14689171e-01 -1.36011660e+00 ... -1.99518478e+00\n    -3.77886921e-01  1.48089498e-01]\n   [ 7.36416698e-01 -6.57431841e-01 -3.52052212e-01 ...  8.32062304e-01\n     1.29972053e+00  9.28789556e-01]\n   [-1.41303909e+00  1.18253279e+00  3.44581068e-01 ... -1.96349728e+00\n     9.73015249e-01  9.27874371e-02]\n   ...\n   [-2.02912569e-01  8.56642842e-01 -2.66288847e-01 ...  6.99093819e-01\n     9.19181108e-01  1.01225996e+00]\n   [-4.15730953e-01  8.51417840e-01 -2.94560678e-02 ... -1.59633803e+00\n    -3.28884393e-01 -1.25070572e+00]\n   [-3.74630690e-01 -1.48265600e-01 -6.77288413e-01 ... -1.47698379e+00\n     1.93633169e-01 -1.59089267e-01]]\n\n  [[ 6.01487637e-01 -2.37229705e-01  8.18106890e-01 ... -6.15804434e-01\n     8.79203498e-01  1.20912826e+00]\n   [-3.57045144e-01 -5.08658707e-01 -4.68377262e-01 ...  1.09971750e+00\n     1.75888345e-01  8.27478111e-01]\n   [-8.96502256e-01 -2.42691845e-01 -6.84654117e-01 ...  6.78359747e-01\n    -1.08821034e+00  1.00649631e+00]\n   ...\n   [-2.36663699e-01 -2.02354118e-02 -1.27195537e+00 ...  1.19090831e+00\n    -4.70652461e-01 -1.77716649e+00]\n   [-9.15824771e-01 -1.64313570e-01  1.02521372e+00 ... -4.68726128e-01\n    -1.58422709e+00 -1.78228244e-01]\n   [-2.91392970e+00 -4.32814658e-01  4.81758326e-01 ... -2.75747776e-01\n    -8.69198799e-01  2.72537053e-01]]]])], (nullptr), []), Constant(2.0)], (nullptr), [])], (nullptr), []), Constant([[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.66741014e-02\n    -1.36016917e+00 -2.64421725e+00]\n   [-4.11465645e-01 -1.32302451e+00 -1.27327085e+00 ... -1.14453161e+00\n    -5.43792248e-01  3.86265904e-01]\n   [ 4.55055267e-01 -2.26475727e-02  1.06080008e+00 ...  5.39571881e-01\n     6.99768841e-01  1.98648795e-01]\n   ...\n   [-9.26206529e-01 -1.84237456e+00  5.28953969e-01 ...  1.25595391e+00\n     9.79239464e-01  2.27478798e-03]\n   [-1.18007019e-01  1.36406839e+00 -3.67243737e-01 ...  5.43178013e-03\n     2.06253916e-01 -1.38207686e+00]\n   [ 4.52283978e-01  1.83468640e+00  1.61386681e+00 ...  1.05118561e+00\n    -2.40624007e-02 -1.70598209e-01]]\n\n  [[ 8.12199950e-01  8.40371668e-01 -1.98992276e+00 ...  1.36784673e+00\n    -1.53337136e-01  1.30889320e+00]\n   [-8.60946596e-01 -1.48725092e+00  9.49378490e-01 ... -2.11173203e-02\n     3.79850082e-02 -2.23771378e-01]\n   [-4.73900378e-01 -7.52061427e-01  4.46268201e-01 ...  6.27109945e-01\n    -9.03462887e-01 -5.92898726e-01]\n   ...\n   [-3.31205511e+00 -1.13749492e+00 -6.45402730e-01 ...  2.15534282e+00\n    -1.37266638e-02 -1.38680589e+00]\n   [-7.37232566e-02  1.43600106e-02  1.35434127e+00 ...  7.44642839e-02\n    -3.06443185e-01 -6.96511507e-01]\n   [ 4.58851963e-01 -2.86112934e-01  5.40829003e-02 ...  7.77311549e-02\n     2.63538122e-01  7.47988760e-01]]\n\n  [[-5.83339453e-01  6.27477705e-01  1.26614416e+00 ... -5.21698236e-01\n    -2.20017242e+00  8.03836528e-03]\n   [-1.46706298e-01  7.34090447e-01 -1.15049696e+00 ...  6.03682637e-01\n     1.33468047e-01  6.11706600e-02]\n   [-5.25465965e-01 -1.35533881e+00 -2.71581084e-01 ...  7.63601601e-01\n    -4.24022749e-02 -3.53040576e-01]\n   ...\n   [-6.14963114e-01 -4.30240542e-01 -7.32103229e-01 ...  1.94113895e-01\n     7.97250271e-01  1.27782667e+00]\n   [-1.30445808e-01  2.30272591e-01  8.17684457e-02 ... -9.25382912e-01\n     6.87287569e-01 -2.44663930e+00]\n   [ 2.55437422e+00  6.56390071e-01 -5.17573893e-01 ...  5.17595828e-01\n     1.89075708e+00  6.28534257e-01]]\n\n  ...\n\n  [[ 1.60160387e+00  1.48001301e+00  1.00542390e+00 ...  4.13084388e-01\n     7.41505206e-01  1.28634834e+00]\n   [ 1.75255075e-01 -4.19689298e-01  4.79065850e-02 ... -3.07148665e-01\n     1.97522259e+00  5.13377070e-01]\n   [ 6.13370895e-01 -1.14634424e-01 -6.22611582e-01 ...  4.53738391e-01\n     4.50554192e-01 -2.93912649e+00]\n   ...\n   [ 8.09042156e-02 -1.08365357e+00  1.29005861e+00 ...  1.41440660e-01\n    -1.13513753e-01  6.24023736e-01]\n   [-1.59668589e+00  4.94843215e-01  8.33825707e-01 ...  7.33713031e-01\n    -2.99754292e-01 -5.36540091e-01]\n   [ 1.34503484e-01 -5.59379458e-01 -9.36695635e-01 ... -2.43555278e-01\n     5.67731023e-01 -1.76169109e+00]]\n\n  [[-1.49234831e-01 -1.14689171e-01 -1.36011660e+00 ... -1.99518478e+00\n    -3.77886921e-01  1.48089498e-01]\n   [ 7.36416698e-01 -6.57431841e-01 -3.52052212e-01 ...  8.32062304e-01\n     1.29972053e+00  9.28789556e-01]\n   [-1.41303909e+00  1.18253279e+00  3.44581068e-01 ... -1.96349728e+00\n     9.73015249e-01  9.27874371e-02]\n   ...\n   [-2.02912569e-01  8.56642842e-01 -2.66288847e-01 ...  6.99093819e-01\n     9.19181108e-01  1.01225996e+00]\n   [-4.15730953e-01  8.51417840e-01 -2.94560678e-02 ... -1.59633803e+00\n    -3.28884393e-01 -1.25070572e+00]\n   [-3.74630690e-01 -1.48265600e-01 -6.77288413e-01 ... -1.47698379e+00\n     1.93633169e-01 -1.59089267e-01]]\n\n  [[ 6.01487637e-01 -2.37229705e-01  8.18106890e-01 ... -6.15804434e-01\n     8.79203498e-01  1.20912826e+00]\n   [-3.57045144e-01 -5.08658707e-01 -4.68377262e-01 ...  1.09971750e+00\n     1.75888345e-01  8.27478111e-01]\n   [-8.96502256e-01 -2.42691845e-01 -6.84654117e-01 ...  6.78359747e-01\n    -1.08821034e+00  1.00649631e+00]\n   ...\n   [-2.36663699e-01 -2.02354118e-02 -1.27195537e+00 ...  1.19090831e+00\n    -4.70652461e-01 -1.77716649e+00]\n   [-9.15824771e-01 -1.64313570e-01  1.02521372e+00 ... -4.68726128e-01\n    -1.58422709e+00 -1.78228244e-01]\n   [-2.91392970e+00 -4.32814658e-01  4.81758326e-01 ... -2.75747776e-01\n    -8.69198799e-01  2.72537053e-01]]]])], (nullptr), []), CallNode(Op(add), [CallNode(Op(add), [CallNode(Op(nn.conv2d), [Var(x, ty=TensorType([1, 64, 56, 56], float32)), Var(weight, ty=TensorType([64, 64, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x1a1956e8), []), CallNode(Op(multiply), [CallNode(Op(add), [Constant([[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.66741014e-02\n    -1.36016917e+00 -2.64421725e+00]\n   [-4.11465645e-01 -1.32302451e+00 -1.27327085e+00 ... -1.14453161e+00\n    -5.43792248e-01  3.86265904e-01]\n   [ 4.55055267e-01 -2.26475727e-02  1.06080008e+00 ...  5.39571881e-01\n     6.99768841e-01  1.98648795e-01]\n   ...\n   [-9.26206529e-01 -1.84237456e+00  5.28953969e-01 ...  1.25595391e+00\n     9.79239464e-01  2.27478798e-03]\n   [-1.18007019e-01  1.36406839e+00 -3.67243737e-01 ...  5.43178013e-03\n     2.06253916e-01 -1.38207686e+00]\n   [ 4.52283978e-01  1.83468640e+00  1.61386681e+00 ...  1.05118561e+00\n    -2.40624007e-02 -1.70598209e-01]]\n\n  [[ 8.12199950e-01  8.40371668e-01 -1.98992276e+00 ...  1.36784673e+00\n    -1.53337136e-01  1.30889320e+00]\n   [-8.60946596e-01 -1.48725092e+00  9.49378490e-01 ... -2.11173203e-02\n     3.79850082e-02 -2.23771378e-01]\n   [-4.73900378e-01 -7.52061427e-01  4.46268201e-01 ...  6.27109945e-01\n    -9.03462887e-01 -5.92898726e-01]\n   ...\n   [-3.31205511e+00 -1.13749492e+00 -6.45402730e-01 ...  2.15534282e+00\n    -1.37266638e-02 -1.38680589e+00]\n   [-7.37232566e-02  1.43600106e-02  1.35434127e+00 ...  7.44642839e-02\n    -3.06443185e-01 -6.96511507e-01]\n   [ 4.58851963e-01 -2.86112934e-01  5.40829003e-02 ...  7.77311549e-02\n     2.63538122e-01  7.47988760e-01]]\n\n  [[-5.83339453e-01  6.27477705e-01  1.26614416e+00 ... -5.21698236e-01\n    -2.20017242e+00  8.03836528e-03]\n   [-1.46706298e-01  7.34090447e-01 -1.15049696e+00 ...  6.03682637e-01\n     1.33468047e-01  6.11706600e-02]\n   [-5.25465965e-01 -1.35533881e+00 -2.71581084e-01 ...  7.63601601e-01\n    -4.24022749e-02 -3.53040576e-01]\n   ...\n   [-6.14963114e-01 -4.30240542e-01 -7.32103229e-01 ...  1.94113895e-01\n     7.97250271e-01  1.27782667e+00]\n   [-1.30445808e-01  2.30272591e-01  8.17684457e-02 ... -9.25382912e-01\n     6.87287569e-01 -2.44663930e+00]\n   [ 2.55437422e+00  6.56390071e-01 -5.17573893e-01 ...  5.17595828e-01\n     1.89075708e+00  6.28534257e-01]]\n\n  ...\n\n  [[ 1.60160387e+00  1.48001301e+00  1.00542390e+00 ...  4.13084388e-01\n     7.41505206e-01  1.28634834e+00]\n   [ 1.75255075e-01 -4.19689298e-01  4.79065850e-02 ... -3.07148665e-01\n     1.97522259e+00  5.13377070e-01]\n   [ 6.13370895e-01 -1.14634424e-01 -6.22611582e-01 ...  4.53738391e-01\n     4.50554192e-01 -2.93912649e+00]\n   ...\n   [ 8.09042156e-02 -1.08365357e+00  1.29005861e+00 ...  1.41440660e-01\n    -1.13513753e-01  6.24023736e-01]\n   [-1.59668589e+00  4.94843215e-01  8.33825707e-01 ...  7.33713031e-01\n    -2.99754292e-01 -5.36540091e-01]\n   [ 1.34503484e-01 -5.59379458e-01 -9.36695635e-01 ... -2.43555278e-01\n     5.67731023e-01 -1.76169109e+00]]\n\n  [[-1.49234831e-01 -1.14689171e-01 -1.36011660e+00 ... -1.99518478e+00\n    -3.77886921e-01  1.48089498e-01]\n   [ 7.36416698e-01 -6.57431841e-01 -3.52052212e-01 ...  8.32062304e-01\n     1.29972053e+00  9.28789556e-01]\n   [-1.41303909e+00  1.18253279e+00  3.44581068e-01 ... -1.96349728e+00\n     9.73015249e-01  9.27874371e-02]\n   ...\n   [-2.02912569e-01  8.56642842e-01 -2.66288847e-01 ...  6.99093819e-01\n     9.19181108e-01  1.01225996e+00]\n   [-4.15730953e-01  8.51417840e-01 -2.94560678e-02 ... -1.59633803e+00\n    -3.28884393e-01 -1.25070572e+00]\n   [-3.74630690e-01 -1.48265600e-01 -6.77288413e-01 ... -1.47698379e+00\n     1.93633169e-01 -1.59089267e-01]]\n\n  [[ 6.01487637e-01 -2.37229705e-01  8.18106890e-01 ... -6.15804434e-01\n     8.79203498e-01  1.20912826e+00]\n   [-3.57045144e-01 -5.08658707e-01 -4.68377262e-01 ...  1.09971750e+00\n     1.75888345e-01  8.27478111e-01]\n   [-8.96502256e-01 -2.42691845e-01 -6.84654117e-01 ...  6.78359747e-01\n    -1.08821034e+00  1.00649631e+00]\n   ...\n   [-2.36663699e-01 -2.02354118e-02 -1.27195537e+00 ...  1.19090831e+00\n    -4.70652461e-01 -1.77716649e+00]\n   [-9.15824771e-01 -1.64313570e-01  1.02521372e+00 ... -4.68726128e-01\n    -1.58422709e+00 -1.78228244e-01]\n   [-2.91392970e+00 -4.32814658e-01  4.81758326e-01 ... -2.75747776e-01\n    -8.69198799e-01  2.72537053e-01]]]]), Constant([[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.66741014e-02\n    -1.36016917e+00 -2.64421725e+00]\n   [-4.11465645e-01 -1.32302451e+00 -1.27327085e+00 ... -1.14453161e+00\n    -5.43792248e-01  3.86265904e-01]\n   [ 4.55055267e-01 -2.26475727e-02  1.06080008e+00 ...  5.39571881e-01\n     6.99768841e-01  1.98648795e-01]\n   ...\n   [-9.26206529e-01 -1.84237456e+00  5.28953969e-01 ...  1.25595391e+00\n     9.79239464e-01  2.27478798e-03]\n   [-1.18007019e-01  1.36406839e+00 -3.67243737e-01 ...  5.43178013e-03\n     2.06253916e-01 -1.38207686e+00]\n   [ 4.52283978e-01  1.83468640e+00  1.61386681e+00 ...  1.05118561e+00\n    -2.40624007e-02 -1.70598209e-01]]\n\n  [[ 8.12199950e-01  8.40371668e-01 -1.98992276e+00 ...  1.36784673e+00\n    -1.53337136e-01  1.30889320e+00]\n   [-8.60946596e-01 -1.48725092e+00  9.49378490e-01 ... -2.11173203e-02\n     3.79850082e-02 -2.23771378e-01]\n   [-4.73900378e-01 -7.52061427e-01  4.46268201e-01 ...  6.27109945e-01\n    -9.03462887e-01 -5.92898726e-01]\n   ...\n   [-3.31205511e+00 -1.13749492e+00 -6.45402730e-01 ...  2.15534282e+00\n    -1.37266638e-02 -1.38680589e+00]\n   [-7.37232566e-02  1.43600106e-02  1.35434127e+00 ...  7.44642839e-02\n    -3.06443185e-01 -6.96511507e-01]\n   [ 4.58851963e-01 -2.86112934e-01  5.40829003e-02 ...  7.77311549e-02\n     2.63538122e-01  7.47988760e-01]]\n\n  [[-5.83339453e-01  6.27477705e-01  1.26614416e+00 ... -5.21698236e-01\n    -2.20017242e+00  8.03836528e-03]\n   [-1.46706298e-01  7.34090447e-01 -1.15049696e+00 ...  6.03682637e-01\n     1.33468047e-01  6.11706600e-02]\n   [-5.25465965e-01 -1.35533881e+00 -2.71581084e-01 ...  7.63601601e-01\n    -4.24022749e-02 -3.53040576e-01]\n   ...\n   [-6.14963114e-01 -4.30240542e-01 -7.32103229e-01 ...  1.94113895e-01\n     7.97250271e-01  1.27782667e+00]\n   [-1.30445808e-01  2.30272591e-01  8.17684457e-02 ... -9.25382912e-01\n     6.87287569e-01 -2.44663930e+00]\n   [ 2.55437422e+00  6.56390071e-01 -5.17573893e-01 ...  5.17595828e-01\n     1.89075708e+00  6.28534257e-01]]\n\n  ...\n\n  [[ 1.60160387e+00  1.48001301e+00  1.00542390e+00 ...  4.13084388e-01\n     7.41505206e-01  1.28634834e+00]\n   [ 1.75255075e-01 -4.19689298e-01  4.79065850e-02 ... -3.07148665e-01\n     1.97522259e+00  5.13377070e-01]\n   [ 6.13370895e-01 -1.14634424e-01 -6.22611582e-01 ...  4.53738391e-01\n     4.50554192e-01 -2.93912649e+00]\n   ...\n   [ 8.09042156e-02 -1.08365357e+00  1.29005861e+00 ...  1.41440660e-01\n    -1.13513753e-01  6.24023736e-01]\n   [-1.59668589e+00  4.94843215e-01  8.33825707e-01 ...  7.33713031e-01\n    -2.99754292e-01 -5.36540091e-01]\n   [ 1.34503484e-01 -5.59379458e-01 -9.36695635e-01 ... -2.43555278e-01\n     5.67731023e-01 -1.76169109e+00]]\n\n  [[-1.49234831e-01 -1.14689171e-01 -1.36011660e+00 ... -1.99518478e+00\n    -3.77886921e-01  1.48089498e-01]\n   [ 7.36416698e-01 -6.57431841e-01 -3.52052212e-01 ...  8.32062304e-01\n     1.29972053e+00  9.28789556e-01]\n   [-1.41303909e+00  1.18253279e+00  3.44581068e-01 ... -1.96349728e+00\n     9.73015249e-01  9.27874371e-02]\n   ...\n   [-2.02912569e-01  8.56642842e-01 -2.66288847e-01 ...  6.99093819e-01\n     9.19181108e-01  1.01225996e+00]\n   [-4.15730953e-01  8.51417840e-01 -2.94560678e-02 ... -1.59633803e+00\n    -3.28884393e-01 -1.25070572e+00]\n   [-3.74630690e-01 -1.48265600e-01 -6.77288413e-01 ... -1.47698379e+00\n     1.93633169e-01 -1.59089267e-01]]\n\n  [[ 6.01487637e-01 -2.37229705e-01  8.18106890e-01 ... -6.15804434e-01\n     8.79203498e-01  1.20912826e+00]\n   [-3.57045144e-01 -5.08658707e-01 -4.68377262e-01 ...  1.09971750e+00\n     1.75888345e-01  8.27478111e-01]\n   [-8.96502256e-01 -2.42691845e-01 -6.84654117e-01 ...  6.78359747e-01\n    -1.08821034e+00  1.00649631e+00]\n   ...\n   [-2.36663699e-01 -2.02354118e-02 -1.27195537e+00 ...  1.19090831e+00\n    -4.70652461e-01 -1.77716649e+00]\n   [-9.15824771e-01 -1.64313570e-01  1.02521372e+00 ... -4.68726128e-01\n    -1.58422709e+00 -1.78228244e-01]\n   [-2.91392970e+00 -4.32814658e-01  4.81758326e-01 ... -2.75747776e-01\n    -8.69198799e-01  2.72537053e-01]]]])], (nullptr), []), Constant(2.0)], (nullptr), [])], (nullptr), []), Constant([[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -1.66741014e-02\n    -1.36016917e+00 -2.64421725e+00]\n   [-4.11465645e-01 -1.32302451e+00 -1.27327085e+00 ... -1.14453161e+00\n    -5.43792248e-01  3.86265904e-01]\n   [ 4.55055267e-01 -2.26475727e-02  1.06080008e+00 ...  5.39571881e-01\n     6.99768841e-01  1.98648795e-01]\n   ...\n   [-9.26206529e-01 -1.84237456e+00  5.28953969e-01 ...  1.25595391e+00\n     9.79239464e-01  2.27478798e-03]\n   [-1.18007019e-01  1.36406839e+00 -3.67243737e-01 ...  5.43178013e-03\n     2.06253916e-01 -1.38207686e+00]\n   [ 4.52283978e-01  1.83468640e+00  1.61386681e+00 ...  1.05118561e+00\n    -2.40624007e-02 -1.70598209e-01]]\n\n  [[ 8.12199950e-01  8.40371668e-01 -1.98992276e+00 ...  1.36784673e+00\n    -1.53337136e-01  1.30889320e+00]\n   [-8.60946596e-01 -1.48725092e+00  9.49378490e-01 ... -2.11173203e-02\n     3.79850082e-02 -2.23771378e-01]\n   [-4.73900378e-01 -7.52061427e-01  4.46268201e-01 ...  6.27109945e-01\n    -9.03462887e-01 -5.92898726e-01]\n   ...\n   [-3.31205511e+00 -1.13749492e+00 -6.45402730e-01 ...  2.15534282e+00\n    -1.37266638e-02 -1.38680589e+00]\n   [-7.37232566e-02  1.43600106e-02  1.35434127e+00 ...  7.44642839e-02\n    -3.06443185e-01 -6.96511507e-01]\n   [ 4.58851963e-01 -2.86112934e-01  5.40829003e-02 ...  7.77311549e-02\n     2.63538122e-01  7.47988760e-01]]\n\n  [[-5.83339453e-01  6.27477705e-01  1.26614416e+00 ... -5.21698236e-01\n    -2.20017242e+00  8.03836528e-03]\n   [-1.46706298e-01  7.34090447e-01 -1.15049696e+00 ...  6.03682637e-01\n     1.33468047e-01  6.11706600e-02]\n   [-5.25465965e-01 -1.35533881e+00 -2.71581084e-01 ...  7.63601601e-01\n    -4.24022749e-02 -3.53040576e-01]\n   ...\n   [-6.14963114e-01 -4.30240542e-01 -7.32103229e-01 ...  1.94113895e-01\n     7.97250271e-01  1.27782667e+00]\n   [-1.30445808e-01  2.30272591e-01  8.17684457e-02 ... -9.25382912e-01\n     6.87287569e-01 -2.44663930e+00]\n   [ 2.55437422e+00  6.56390071e-01 -5.17573893e-01 ...  5.17595828e-01\n     1.89075708e+00  6.28534257e-01]]\n\n  ...\n\n  [[ 1.60160387e+00  1.48001301e+00  1.00542390e+00 ...  4.13084388e-01\n     7.41505206e-01  1.28634834e+00]\n   [ 1.75255075e-01 -4.19689298e-01  4.79065850e-02 ... -3.07148665e-01\n     1.97522259e+00  5.13377070e-01]\n   [ 6.13370895e-01 -1.14634424e-01 -6.22611582e-01 ...  4.53738391e-01\n     4.50554192e-01 -2.93912649e+00]\n   ...\n   [ 8.09042156e-02 -1.08365357e+00  1.29005861e+00 ...  1.41440660e-01\n    -1.13513753e-01  6.24023736e-01]\n   [-1.59668589e+00  4.94843215e-01  8.33825707e-01 ...  7.33713031e-01\n    -2.99754292e-01 -5.36540091e-01]\n   [ 1.34503484e-01 -5.59379458e-01 -9.36695635e-01 ... -2.43555278e-01\n     5.67731023e-01 -1.76169109e+00]]\n\n  [[-1.49234831e-01 -1.14689171e-01 -1.36011660e+00 ... -1.99518478e+00\n    -3.77886921e-01  1.48089498e-01]\n   [ 7.36416698e-01 -6.57431841e-01 -3.52052212e-01 ...  8.32062304e-01\n     1.29972053e+00  9.28789556e-01]\n   [-1.41303909e+00  1.18253279e+00  3.44581068e-01 ... -1.96349728e+00\n     9.73015249e-01  9.27874371e-02]\n   ...\n   [-2.02912569e-01  8.56642842e-01 -2.66288847e-01 ...  6.99093819e-01\n     9.19181108e-01  1.01225996e+00]\n   [-4.15730953e-01  8.51417840e-01 -2.94560678e-02 ... -1.59633803e+00\n    -3.28884393e-01 -1.25070572e+00]\n   [-3.74630690e-01 -1.48265600e-01 -6.77288413e-01 ... -1.47698379e+00\n     1.93633169e-01 -1.59089267e-01]]\n\n  [[ 6.01487637e-01 -2.37229705e-01  8.18106890e-01 ... -6.15804434e-01\n     8.79203498e-01  1.20912826e+00]\n   [-3.57045144e-01 -5.08658707e-01 -4.68377262e-01 ...  1.09971750e+00\n     1.75888345e-01  8.27478111e-01]\n   [-8.96502256e-01 -2.42691845e-01 -6.84654117e-01 ...  6.78359747e-01\n    -1.08821034e+00  1.00649631e+00]\n   ...\n   [-2.36663699e-01 -2.02354118e-02 -1.27195537e+00 ...  1.19090831e+00\n    -4.70652461e-01 -1.77716649e+00]\n   [-9.15824771e-01 -1.64313570e-01  1.02521372e+00 ... -4.68726128e-01\n    -1.58422709e+00 -1.78228244e-01]\n   [-2.91392970e+00 -4.32814658e-01  4.81758326e-01 ... -2.75747776e-01\n    -8.69198799e-01  2.72537053e-01]]]])], (nullptr), [])], (nullptr), [])"
     ]
    }
   ],
   "source": [
    "mod = relay.transform.FuseOps(fuse_opt_level=0)(mod)\n",
    "\n",
    "# We can observe that the optimized module contains functions that only have\n",
    "# a signle primitive op.\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tvm.relay' has no attribute 'fromtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000022vscode-remote?line=0'>1</a>\u001b[0m relay\u001b[39m.\u001b[39;49mfromtext()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tvm.relay' has no attribute 'fromtext'"
     ]
    }
   ],
   "source": [
    "relay.fromtext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.import_from_std(mod1.astext())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.relay.build_module import bind_params_by_name\n",
    "from tvm.relay.dataflow_pattern import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input;\n",
      "free_var %weight;\n",
      "%0 = nn.conv2d(%input, %weight, padding=[0, 0, 0, 0]);\n",
      "nn.relu(%0)\n"
     ]
    }
   ],
   "source": [
    "pattern = is_op(\"nn.relu\")(is_op(\"nn.conv2d\")(wildcard(), wildcard()))\n",
    "\n",
    "# A graph.\n",
    "x = relay.var('input')\n",
    "w = relay.var('weight')\n",
    "conv2d = relay.op.nn.conv2d(x, w)\n",
    "relu = relay.op.nn.relu(conv2d)\n",
    "print(relu)\n",
    "# free_var %x: Tensor[(1, 3, 224, 224), float32]\n",
    "# free_var %w: Tensor[(3, 3, 3, 3), float32]\n",
    "# %0 = nn.conv2d(%x, %w, padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 3, 222, 222), float32] */;\n",
    "# free_var %b: Tensor[(3), float32]\n",
    "# nn.bias_add(%0, %b) /* ty=Tensor[(1, 3, 222, 222), float32] */\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input;\n",
      "free_var %weight;\n",
      "%1 = fn (%FunctionVar_0_0, %FunctionVar_0_1, PartitionedFromPattern=\"nn.conv2d_nn.relu_\") {\n",
      "  %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[0, 0, 0, 0]);\n",
      "  nn.relu(%0)\n",
      "};\n",
      "%1(%input, %weight)\n"
     ]
    }
   ],
   "source": [
    "# After partition.\n",
    "print(pattern.partition(relu))\n",
    "# free_var %x: Tensor[(1, 3, 224, 224), float32]\n",
    "# free_var %w: Tensor[(3, 3, 3, 3), float32]\n",
    "# free_var %b: Tensor[(3), float32]\n",
    "# %1 = fn (%FunctionVar_0_0, %FunctionVar_0_1,\n",
    "#          %FunctionVar_0_2, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_\") {\n",
    "#   %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[0, 0, 0, 0]);\n",
    "#   nn.bias_add(%0, %FunctionVar_0_2)\n",
    "# };\n",
    "# %1(%x, %w, %b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input;\n",
      "free_var %weight;\n",
      "%1 = fn (%FunctionVar_0_0, %FunctionVar_0_1, PartitionedFromPattern=\"nn.conv2d_nn.relu_\", Composite=\"one_layer\") {\n",
      "  %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[0, 0, 0, 0]);\n",
      "  nn.relu(%0)\n",
      "};\n",
      "%1(%input, %weight)\n"
     ]
    }
   ],
   "source": [
    "print(pattern.partition(relu, {'Composite': 'one_layer'}))\n",
    "# free_var %x: Tensor[(1, 3, 224, 224), float32]\n",
    "# free_var %w: Tensor[(3, 3, 3, 3), float32]\n",
    "# free_var %b: Tensor[(3), float32]\n",
    "# %1 = fn (%FunctionVar_0_0, %FunctionVar_0_1,\n",
    "#          %FunctionVar_0_2, Composite=\"one_layer\",\n",
    "#                            PartitionedFromPattern=\"nn.conv2d_nn.bias_add_\") {\n",
    "#   %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[0, 0, 0, 0]);\n",
    "#   nn.bias_add(%0, %FunctionVar_0_2)\n",
    "# };\n",
    "# %1(%x, %w, %b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  7: TVMFuncCall\n  6: tvm::runtime::TypedPackedFunc<tvm::RelayExpr (tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)>::AssignTypedLambda<tvm::relay::{lambda(tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)#3}>(tvm::relay::{lambda(tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)#3}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const\n  5: tvm::relay::PartitionPattern(tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)\n  4: tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)\n  3: void tvm::relay::ExpandDataflow<tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}) [clone .isra.0]\n  2: tvm::relay::MixedModeMutator::VisitLeaf(tvm::RelayExpr const&)\n  1: tvm::relay::PatternPartitioner::DispatchVisitExpr(tvm::RelayExpr const&)\n  0: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&) [clone .cold]\n  File \"/home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py\", line 81, in cfun\n    rv = local_pyfunc(*pyargs)\n  File \"/tmp/ipykernel_130642/923390876.py\", line 3, in check\n    return (conv.attrs.data_layout == \"NCHW\") and bool(conv.checked_type.shape[0] == 1)\n  File \"/home/j/tvm-slicer/include/tvm/python/tvm/ir/expr.py\", line 50, in checked_type\n    raise ValueError(\"The type checker has not populated\" \" the checked_type for this node\")\nValueError: The type checker has not populated the checked_type for this node",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000026vscode-remote?line=1'>2</a>\u001b[0m     conv \u001b[39m=\u001b[39m pre\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000026vscode-remote?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (conv\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mdata_layout \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNCHW\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(conv\u001b[39m.\u001b[39mchecked_type\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000026vscode-remote?line=4'>5</a>\u001b[0m pattern\u001b[39m.\u001b[39;49mpartition(relu, check\u001b[39m=\u001b[39;49mcheck)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py:174\u001b[0m, in \u001b[0;36mDFPattern.partition\u001b[0;34m(self, expr, attrs, check)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=149'>150</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpartition\u001b[39m(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=150'>151</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=151'>152</a>\u001b[0m     expr: Expr,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=152'>153</a>\u001b[0m     attrs: Optional[Dict[\u001b[39mstr\u001b[39m, Object]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=153'>154</a>\u001b[0m     check: Callable[[Expr], \u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=154'>155</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Expr:\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=155'>156</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=156'>157</a>\u001b[0m \u001b[39m    Partition the expression into functions defined by this pattern\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=157'>158</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m        The Expression with matched subgraphs replaced by function calls to that subgraph\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=172'>173</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=173'>174</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m partition(\u001b[39mself\u001b[39;49m, expr, attrs, check)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py:913\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(pattern, expr, attrs, check)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=886'>887</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpartition\u001b[39m(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=887'>888</a>\u001b[0m     pattern: \u001b[39m\"\u001b[39m\u001b[39mDFPattern\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=888'>889</a>\u001b[0m     expr: Expr,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=889'>890</a>\u001b[0m     attrs: Optional[Dict[\u001b[39mstr\u001b[39m, Object]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=890'>891</a>\u001b[0m     check: Callable[[Expr], \u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=891'>892</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Expr:\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=892'>893</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=893'>894</a>\u001b[0m \u001b[39m    Parition the expression into a series of functions that match the pattern\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=894'>895</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=910'>911</a>\u001b[0m \u001b[39m        The Expression with matched subgraphs replaced by function calls to that subgraph\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=911'>912</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=912'>913</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ffi\u001b[39m.\u001b[39;49mpartition(pattern, expr, attrs, check)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=224'>225</a>\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=225'>226</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=226'>227</a>\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=227'>228</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=234'>235</a>\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=235'>236</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=236'>237</a>\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=237'>238</a>\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=238'>239</a>\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  7: TVMFuncCall\n  6: tvm::runtime::TypedPackedFunc<tvm::RelayExpr (tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)>::AssignTypedLambda<tvm::relay::{lambda(tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)#3}>(tvm::relay::{lambda(tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)#3}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const, tvm::runtime::TVMRetValue) const\n  5: tvm::relay::PartitionPattern(tvm::relay::DFPattern, tvm::RelayExpr, tvm::runtime::Map<tvm::runtime::String, tvm::runtime::ObjectRef, void, void>, tvm::runtime::PackedFunc)\n  4: tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)\n  3: void tvm::relay::ExpandDataflow<tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeMutator::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}) [clone .isra.0]\n  2: tvm::relay::MixedModeMutator::VisitLeaf(tvm::RelayExpr const&)\n  1: tvm::relay::PatternPartitioner::DispatchVisitExpr(tvm::RelayExpr const&)\n  0: std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&) [clone .cold]\n  File \"/home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py\", line 81, in cfun\n    rv = local_pyfunc(*pyargs)\n  File \"/tmp/ipykernel_130642/923390876.py\", line 3, in check\n    return (conv.attrs.data_layout == \"NCHW\") and bool(conv.checked_type.shape[0] == 1)\n  File \"/home/j/tvm-slicer/include/tvm/python/tvm/ir/expr.py\", line 50, in checked_type\n    raise ValueError(\"The type checker has not populated\" \" the checked_type for this node\")\nValueError: The type checker has not populated the checked_type for this node"
     ]
    }
   ],
   "source": [
    "def check(pre):\n",
    "    conv = pre.args[0]\n",
    "    return (conv.attrs.data_layout == \"NCHW\") and bool(conv.checked_type.shape[0] == 1)\n",
    "\n",
    "pattern.partition(relu, check=check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %x;\n",
      "free_var %mean;\n",
      "free_var %gamma;\n",
      "%0 = subtract(%x, %mean);\n",
      "free_var %var;\n",
      "%1 = add(%var, %var);\n",
      "%2 = add(%1, 1e-05f);\n",
      "%3 = multiply(%gamma, %0);\n",
      "%4 = sqrt(%2);\n",
      "%5 = divide(%3, %4);\n",
      "free_var %beta;\n",
      "add(%5, %beta)\n",
      "1\n"
     ]
    },
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  10: TVMFuncCall\n  9: tvm::runtime::TypedPackedFunc<tvm::RelayExpr (tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule)>::AssignTypedLambda<tvm::RelayExpr (*)(tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule)>(tvm::RelayExpr (*)(tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const\n  8: tvm::relay::RewritePatterns(tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule)\n  7: tvm::relay::PatternRewriter::Rewrite(tvm::runtime::Array<tvm::relay::DFPatternCallback, void> const&, tvm::RelayExpr const&)\n  6: tvm::relay::PatternGrouper::GroupMatches(tvm::relay::DFPattern const&, tvm::RelayExpr const&)\n  5: tvm::relay::CreateIndexedGraph(tvm::RelayExpr const&)\n  4: tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)\n  3: void tvm::relay::ExpandDataflow<tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}) [clone .isra.0]\n  2: tvm::relay::CreateIndexedGraph(tvm::RelayExpr const&)::Creator::VisitLeaf(tvm::RelayExpr const&)\n  1: tvm::relay::MixedModeVisitor::VisitLeaf(tvm::RelayExpr const&)\n  0: tvm::relay::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>::VisitExpr(tvm::RelayExpr const&) [clone .part.0]\n  File \"/home/j/tvm-slicer/include/tvm/include/tvm/relay/expr_functor.h\", line 92\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (n.defined()) is false: Found null pointer node while traversing AST. The previous pass may have generated invalid data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000028vscode-remote?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(BN)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000028vscode-remote?line=40'>41</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtvm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrelay\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataflow_pattern\u001b[39;00m \u001b[39mimport\u001b[39;00m rewrite\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000028vscode-remote?line=41'>42</a>\u001b[0m out \u001b[39m=\u001b[39m rewrite(BatchnormCallback(), BN)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py:884\u001b[0m, in \u001b[0;36mrewrite\u001b[0;34m(callbacks, expr, mod)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=876'>877</a>\u001b[0m     \u001b[39massert\u001b[39;00m callback\u001b[39m.\u001b[39mpattern \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=877'>878</a>\u001b[0m     tmp\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=878'>879</a>\u001b[0m         _DFPatternCallback(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=879'>880</a>\u001b[0m             callback\u001b[39m.\u001b[39mpattern, callback\u001b[39m.\u001b[39mcallback, callback\u001b[39m.\u001b[39mrequire_type, callback\u001b[39m.\u001b[39mrewrite_once\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=880'>881</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=881'>882</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/relay/dataflow_pattern/__init__.py?line=883'>884</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ffi\u001b[39m.\u001b[39;49mrewrite(tmp, expr, mod)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=224'>225</a>\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=225'>226</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=226'>227</a>\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=227'>228</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=234'>235</a>\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=235'>236</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=236'>237</a>\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=237'>238</a>\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/_ffi/_ctypes/packed_func.py?line=238'>239</a>\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  10: TVMFuncCall\n  9: tvm::runtime::TypedPackedFunc<tvm::RelayExpr (tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule)>::AssignTypedLambda<tvm::RelayExpr (*)(tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule)>(tvm::RelayExpr (*)(tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const\n  8: tvm::relay::RewritePatterns(tvm::runtime::Array<tvm::relay::DFPatternCallback, void>, tvm::RelayExpr, tvm::IRModule)\n  7: tvm::relay::PatternRewriter::Rewrite(tvm::runtime::Array<tvm::relay::DFPatternCallback, void> const&, tvm::RelayExpr const&)\n  6: tvm::relay::PatternGrouper::GroupMatches(tvm::relay::DFPattern const&, tvm::RelayExpr const&)\n  5: tvm::relay::CreateIndexedGraph(tvm::RelayExpr const&)\n  4: tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)\n  3: void tvm::relay::ExpandDataflow<tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#2}, tvm::relay::ExpandDataflow, tvm::relay::ExpandDataflow<{lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1}>(tvm::RelayExpr, {lambda(tvm::RelayExpr const&)#1}, tvm::relay::MixedModeVisitor::VisitExpr(tvm::RelayExpr const&)::{lambda(tvm::RelayExpr const&)#1})::{lambda(tvm::RelayExpr const&)#1}) [clone .isra.0]\n  2: tvm::relay::CreateIndexedGraph(tvm::RelayExpr const&)::Creator::VisitLeaf(tvm::RelayExpr const&)\n  1: tvm::relay::MixedModeVisitor::VisitLeaf(tvm::RelayExpr const&)\n  0: tvm::relay::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>::VisitExpr(tvm::RelayExpr const&) [clone .part.0]\n  File \"/home/j/tvm-slicer/include/tvm/include/tvm/relay/expr_functor.h\", line 92\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (n.defined()) is false: Found null pointer node while traversing AST. The previous pass may have generated invalid data."
     ]
    }
   ],
   "source": [
    "class BatchnormCallback(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        self.x = wildcard()\n",
    "        self.var = wildcard()\n",
    "        self.mean = wildcard()\n",
    "        self.beta = wildcard()\n",
    "        self.gamma = wildcard()\n",
    "        self.eps = wildcard()\n",
    "\n",
    "        self.pattern = self.gamma * (self.x - self.mean)/is_op(\"sqrt\")(self.var + self.eps) + self.beta\n",
    "        self.cnt = 0\n",
    "        self.target = 2\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        if self.cnt != self.target:\n",
    "            print(1)\n",
    "            self.cnt += 1\n",
    "        else :    \n",
    "            # print(node_map)\n",
    "            x = node_map[self.x][0]\n",
    "            var = node_map[self.var][0]\n",
    "            mean = node_map[self.mean][0]\n",
    "            beta = node_map[self.beta][0]\n",
    "            gamma = node_map[self.gamma][0]\n",
    "            eps = node_map[self.eps][0]\n",
    "            return relay.op.nn.batch_norm(x, gamma, beta, mean, var, epsilon = eps.data.numpy().item())[0]\n",
    "\n",
    "# A graph of arithmetic operators that are functional equivalent to batch_norm.\n",
    "x = relay.var('x')\n",
    "var = relay.var('var')\n",
    "mean = relay.var('mean')\n",
    "beta = relay.var('beta')\n",
    "gamma = relay.var('gamma')\n",
    "\n",
    "JUNO = var + var + var + var + var + var + var \n",
    "\n",
    "BN = gamma * (x - mean)/relay.op.sqrt(var + var + relay.const(1e-5)) + beta\n",
    "print(BN)\n",
    "from tvm.relay.dataflow_pattern import rewrite\n",
    "out = rewrite(BatchnormCallback(), BN)\n",
    "# assert tvm.ir.structural_equal(out, relay.op.nn.batch_norm(x, gamma, beta, mean, var, epsilon = 1e-5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===target relay===\n",
      "free_var %x1;\n",
      "free_var %x2;\n",
      "%0 = add(%x1, %x2);\n",
      "free_var %x3;\n",
      "%1 = add(%0, %x3);\n",
      "free_var %x4;\n",
      "%2 = add(%1, %x4);\n",
      "free_var %x5;\n",
      "add(%2, %x5)\n",
      "==================\n",
      "======OUT=========\n",
      "free_var %x1;\n",
      "free_var %x2;\n",
      "%0 = add(%x1, %x2);\n",
      "free_var %x3;\n",
      "%1 = add(%0, %x3);\n",
      "free_var %x4;\n",
      "%2 = add(%1, %x4);\n",
      "free_var %x5;\n",
      "add(%2, %x5)\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "class JUNOCallback(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        super().__init__(rewrite_once=True)\n",
    "        # self.x = wildcard()\n",
    "        self.var1 = wildcard()\n",
    "        self.var2 = wildcard()\n",
    "        # self.var = wildcard()\n",
    "        # self.meta = wildcard()\n",
    "        self.var = relay.var(\"var\")\n",
    "        self.meta = relay.var(\"meta\")\n",
    "        self.padding = [1, 1, 1, 1]\n",
    "        self.channels = 32 \n",
    "        self.kernel_size = [3, 3]\n",
    "        self.out_dtype = \"int32\"\n",
    "        # self.var = wildcard()\n",
    "        \n",
    "        # self.mean = wildcard()\n",
    "        # self.beta = wildcard()\n",
    "        # self.gamma = wildcard()\n",
    "        # self.eps = wildcard()\n",
    "\n",
    "        # self.pattern = self.gamma * (self.x - self.mean)/is_op(\"sqrt\")(self.var + self.eps) + self.beta\n",
    "        # self.pattern = is_op(relay.op.nn.max_pool2d)\n",
    "        # self.pattern = relay.op.nn.nn.max_pool2d(self.var1)\n",
    "        # self.pattern = relay.op.nn.conv2d(\n",
    "        #     self.var, \n",
    "        #     self.meta, \n",
    "        #     padding=self.padding, \n",
    "        #     channels=self.channels,\n",
    "        #     kernel_size=self.kernel_size,\n",
    "        #     out_dtype=self.out_dtype\n",
    "        # )\n",
    "        # self.pattern = is_op('nn.conv2d')(wildcard(), wildcard(), is_constant(), is_constant(), is_constant())\n",
    "        self.pattern = is_op('nn.bias_add')(self.var1, self.var2)\n",
    "        # self.pattern = is_op('max_pool2d')(self.var1, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0] )\n",
    "        self.cnt = 0\n",
    "        self.target = 0\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        var1 = node_map[self.var1][0]\n",
    "        var2 = node_map[self.var2][0]\n",
    "        original = relay.nn.bias_add(var1, var2)\n",
    "\n",
    "        if self.cnt != self.target:\n",
    "            print(self.cnt, self.target)\n",
    "            print(\"fuck\")\n",
    "            self.cnt += 1\n",
    "            return original\n",
    "        else:\n",
    "            self.cnt += 1\n",
    "            # return var1 * var2\n",
    "            original = relay.nn.bias_add(var1, var2)\n",
    "            cast_to_int8 = relay.cast(\n",
    "                relay.clip(\n",
    "                    relay.round(\n",
    "                        relay.multiply(original, relay.const(16.0))\n",
    "                    ), \n",
    "                    a_min=-127.0, a_max=127.0\n",
    "                ),\n",
    "                dtype=\"int8\"\n",
    "            )\n",
    "            print(cast_to_int8)\n",
    "            cast_to_float32 = relay.cast(\n",
    "                relay.clip(\n",
    "                    relay.right_shift(\n",
    "                        relay.add(relay.cast(cast_to_int8, dtype='int32'), relay.const(512)),\n",
    "                        relay.const(10)),\n",
    "                    a_min=-127.0, a_max=127.0), \n",
    "                dtype=\"float32\"\n",
    "            )\n",
    "            return relay.op.annotation.stop_fusion(cast_to_float32)\n",
    "            # return relay.cliprelay.right_shift(relay.add(relay.cast(\n",
    "            #     relay.clip(\n",
    "            #         relay.round(\n",
    "            #             var1 * relay.const(16.0)\n",
    "            #         ), \n",
    "            #         a_min=relay.const(-127.0), \n",
    "            #         a_max=relay.const(127.0)\n",
    "            #     ), dtype=\"int8\"), relay.const(512)), 10)\n",
    "\n",
    "x1 = relay.var('x1')\n",
    "x2 = relay.var('x2')\n",
    "x3 = relay.var('x3')\n",
    "x4 = relay.var('x4')\n",
    "x5 = relay.var('x5')\n",
    "\n",
    "JUNO = x1 + x2 + x3 + x4 + x5\n",
    "\n",
    "print(\"===target relay===\")\n",
    "print(JUNO)\n",
    "print(\"==================\")\n",
    "bc = JUNOCallback()\n",
    "# print(\"target pattern\")\n",
    "# print(bc.pattern)\n",
    "out = rewrite(JUNOCallback(), JUNO)\n",
    "print(\"======OUT=========\")\n",
    "print(out)\n",
    "print(\"==================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input_1: Tensor[(1, 3, 256, 256), float32];\n",
      "free_var %v_param_1: Tensor[(16, 3, 3, 3), float32];\n",
      "%0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "free_var %v_param_2: Tensor[(16), float32];\n",
      "%1 = nn.bias_add(%0, %v_param_2);\n",
      "%2 = multiply(%1, 16f);\n",
      "%3 = round(%2);\n",
      "%4 = clip(%3, a_min=-127f, a_max=127f);\n",
      "cast(%4, dtype=\"int8\")\n",
      "1 0\n",
      "fuck\n",
      "2 0\n",
      "fuck\n"
     ]
    }
   ],
   "source": [
    "out = rewrite(JUNOCallback(), mod['main'])\n",
    "# out_body = rewrite(JUNOCallback(), mod['main'].body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fn (%input_1: Tensor[(1, 3, 256, 256), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(32, 16, 3, 3), float32], %v_param_4: Tensor[(32), float32], %v_param_5: Tensor[(64, 32, 3, 3), float32], %v_param_6: Tensor[(64), float32]) {\n",
       "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %1 = nn.bias_add(%0, %v_param_2);\n",
       "  %2 = multiply(%1, 16f);\n",
       "  %3 = round(%2);\n",
       "  %4 = clip(%3, a_min=-127f, a_max=127f);\n",
       "  %5 = cast(%4, dtype=\"int8\");\n",
       "  %6 = cast(%5, dtype=\"int32\");\n",
       "  %7 = add(%6, 512);\n",
       "  %8 = right_shift(%7, 10);\n",
       "  %9 = clip(%8, a_min=-127f, a_max=127f);\n",
       "  %10 = cast(%9, dtype=\"float32\");\n",
       "  %11 = annotation.stop_fusion(%10);\n",
       "  %12 = nn.max_pool2d(%11, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %13 = nn.conv2d(%12, %v_param_3, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %14 = nn.bias_add(%13, %v_param_4);\n",
       "  %15 = nn.max_pool2d(%14, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %16 = nn.conv2d(%15, %v_param_5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %17 = nn.bias_add(%16, %v_param_6);\n",
       "  nn.max_pool2d(%17, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0])\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod['main'] = out\n",
    "# mod['main'].body = out_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%input_1: Tensor[(1, 3, 256, 256), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(32, 16, 3, 3), float32], %v_param_4: Tensor[(32), float32], %v_param_5: Tensor[(64, 32, 3, 3), float32], %v_param_6: Tensor[(64), float32]) {\n",
      "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %1 = nn.bias_add(%0, %v_param_2);\n",
      "  %2 = nn.max_pool2d(%1, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %3 = multiply(%2, 16f);\n",
      "  %4 = round(%3);\n",
      "  %5 = clip(%4, a_min=-127f, a_max=127f);\n",
      "  %6 = cast(%5, dtype=\"int8\");\n",
      "  %7 = cast(%6, dtype=\"int32\");\n",
      "  %8 = add(%7, 512);\n",
      "  %9 = right_shift(%8, 10);\n",
      "  %10 = clip(%9, a_min=-127f, a_max=127f);\n",
      "  %11 = cast(%10, dtype=\"float32\");\n",
      "  %12 = nn.conv2d(%11, %v_param_3, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %13 = nn.bias_add(%12, %v_param_4);\n",
      "  %14 = nn.max_pool2d(%13, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %15 = nn.conv2d(%14, %v_param_5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %16 = nn.bias_add(%15, %v_param_6);\n",
      "  nn.max_pool2d(%16, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0])\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "mod, params = relay.frontend.from_keras(model_keras, shape_dict)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod['main'] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_fun = tvm.IRModule.from_expr(out)\n",
    "# mod['main'].body = out\n",
    "\n",
    "target = 'cuda'\n",
    "dev = tvm.cuda()\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=0):\n",
    "    lib = relay.build(mod, target, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fn (%input_1: Tensor[(1, 3, 256, 256), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(32, 16, 3, 3), float32], %v_param_4: Tensor[(32), float32], %v_param_5: Tensor[(64, 32, 3, 3), float32], %v_param_6: Tensor[(64), float32]) {\n",
       "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %1 = nn.bias_add(%0, %v_param_2);\n",
       "  %2 = nn.max_pool2d(%1, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %3 = nn.conv2d(%2, %v_param_3, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %4 = nn.bias_add(%3, %v_param_4);\n",
       "  %5 = nn.max_pool2d(%4, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %6 = nn.conv2d(%5, %v_param_5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %7 = nn.bias_add(%6, %v_param_6);\n",
       "  nn.max_pool2d(%7, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0])\n",
       "}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pygraphviz as pgv\n",
    "\n",
    "def show_graph(json_data, file_name=None):\n",
    "    if type(json_data) == str:\n",
    "        json_data = json.loads(json_data)\n",
    "    A = pgv.AGraph(directed=True)\n",
    "    for node_idx, node in enumerate(json_data['nodes']):\n",
    "        for src in node['inputs']:\n",
    "            A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]), node['name'] + '[{}]'.format(node_idx) + '{}'.format(json_data['attrs']['dltype'][1][node_idx]))\n",
    "            #A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(shape_size(json_data['attrs']['shape'][1][src[0]])) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]), node['name'] + '[{}]'.format(node_idx) + '{}'.format(shape_size(json_data['attrs']['shape'][1][node_idx])) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]))\n",
    "    if file_name:\n",
    "        A.draw(file_name + '.png', format='png', prog='dot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(lib['get_graph_json'](), \"quanadd_0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "model_keras = tf.keras.models.load_model('./very_simple_model.h5')\n",
    "\n",
    "input_data = np.random.normal(0,1,(1,256,256,3)).astype(np.float32)\n",
    "input_data = input_data.transpose([0, 3, 1, 2])\n",
    "shape_dict = {\"input_1\": input_data.shape}\n",
    "mod, params = relay.frontend.from_keras(model_keras, shape_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JUNOCallback(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        super().__init__(rewrite_once=True)\n",
    "        # self.x = wildcard()\n",
    "        self.var = wildcard()\n",
    "        self.var2 = wildcard()\n",
    "        self.pool_size = is_constant()\n",
    "        self.strides = is_constant()\n",
    "        self.padding = is_constant()\n",
    "        # self.pool_size = wildcard()\n",
    "        # self.strides = wildcard()\n",
    "        # self.padding = wildcard()\n",
    "        # self.var = wildcard()\n",
    "        # self.meta = wildcard()\n",
    "        # self.var = relay.var(\"var\")\n",
    "        # self.meta = relay.var(\"meta\")\n",
    "        self.padding = [1, 1, 1, 1]\n",
    "        # self.channels = 32 \n",
    "        # self.kernel_size = [3, 3]\n",
    "        # self.out_dtype = \"int32\"\n",
    "        # self.var = wildcard()\n",
    "        \n",
    "        # self.mean = wildcard()\n",
    "        # self.beta = wildcard()\n",
    "        # self.gamma = wildcard()\n",
    "        # self.eps = wildcard()\n",
    "\n",
    "        # self.pattern = self.gamma * (self.x - self.mean)/is_op(\"sqrt\")(self.var + self.eps) + self.beta\n",
    "        # self.pattern = is_op(relay.op.nn.max_pool2d)\n",
    "        # self.pattern = relay.op.nn.nn.max_pool2d(self.var1)\n",
    "        # self.pattern = relay.op.nn.conv2d(\n",
    "        #     self.var, \n",
    "        #     self.meta, \n",
    "        #     padding=self.padding, \n",
    "        #     channels=self.channels,\n",
    "        #     kernel_size=self.kernel_size,\n",
    "        #     out_dtype=self.out_dtype\n",
    "        # )\n",
    "        # relay.nn.batch_norm()\n",
    "        # self.pattern = is_op('nn.conv2d')(wildcard(), wildcard(), is_constant(), is_constant(), is_constant())\n",
    "        # self.pattern = is_op('nn.bias_add')(self.var1, self.var2)\n",
    "        self.pattern = is_op('nn.max_pool2d')(self.var)\n",
    "        # self.pattern = is_op('nn.max_pool2d')(self.var, pool_size=self.pool_size, strides=self.strides, padding=self.padding)\n",
    "        # op = is_op('nn.max_pool2d').has_attr({\"pool_size\" : self.pool_size, \"strides\":self.strides, \"padding\":self.padding})\n",
    "        # op = is_op('nn.max_pool2d')\n",
    "        # self.pattern = op(self.var)\n",
    "        # self.pattern = relay.nn.max_pool2d(self.var, pool_size=self.pool_size, strides=self.strides, padding=self.padding)\n",
    "        self.cnt = 0\n",
    "        self.target = 0\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        # print(\"asd\")\n",
    "        # print(type(node_map[self.var][0]))\n",
    "        # print(\"asd\")\n",
    "        var = node_map[self.var]\n",
    "        \n",
    "        # print(1)\n",
    "        # print(var)\n",
    "        # print(2)\n",
    "        # original = relay.nn.max_pool2d(var, pool_size=(2, 2), strides=(2, 2), padding=(0, 0, 0, 0))\n",
    "        original = post\n",
    "        # pool_size = node_map[self.pool_size][0]\n",
    "        # strides = node_map[self.strides][0]\n",
    "        # padding = self.padding\n",
    "        \n",
    "        # padding = node_map[self.padding][0]\n",
    "        # var2 = node_map[self.var2][0]\n",
    "        # original = relay.nn.max_pool2d(var1, pool_size=(2, 2), strides=(2, 2), padding=(0, 0, 0, 0))\n",
    "\n",
    "        if self.cnt != self.target:\n",
    "            # print(self.cnt, self.target)\n",
    "            print(\"pass\")\n",
    "            self.cnt += 1\n",
    "            return post\n",
    "        else:\n",
    "            self.cnt += 1\n",
    "            # return var1 * var2\n",
    "            # original = relay.nn.bias_add(var1, var2)\n",
    "            cast_to_int8 = relay.cast(\n",
    "                relay.clip(\n",
    "                    relay.round(\n",
    "                        relay.multiply(post, relay.const(16.0))\n",
    "                    ), \n",
    "                    a_min=-127.0, a_max=127.0\n",
    "                ),\n",
    "                dtype=\"int8\"\n",
    "            )\n",
    "            cast_to_float32 = relay.cast(\n",
    "                relay.clip(\n",
    "                    relay.right_shift(\n",
    "                        relay.add(relay.cast(relay.annotation.stop_fusion(cast_to_int8), dtype='int32'), relay.const(512)),\n",
    "                        relay.const(10)),\n",
    "                    a_min=-127.0, a_max=127.0), \n",
    "                dtype=\"float32\"\n",
    "            )\n",
    "            print(cast_to_float32)\n",
    "            return cast_to_float32\n",
    "            # return relay.cliprelay.right_shift(relay.add(relay.cast(\n",
    "            #     relay.clip(\n",
    "            #         relay.round(\n",
    "            #             var1 * relay.const(16.0)\n",
    "            #         ), \n",
    "            #         a_min=relay.const(-127.0), \n",
    "            #         a_max=relay.const(127.0)\n",
    "            #     ), dtype=\"int8\"), relay.const(512)), 10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input_1: Tensor[(1, 3, 256, 256), float32];\n",
      "free_var %v_param_1: Tensor[(16, 3, 3, 3), float32];\n",
      "%0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "free_var %v_param_2: Tensor[(16), float32];\n",
      "%1 = nn.bias_add(%0, %v_param_2);\n",
      "%2 = nn.max_pool2d(%1, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "%3 = multiply(%2, 16f);\n",
      "%4 = round(%3);\n",
      "%5 = clip(%4, a_min=-127f, a_max=127f);\n",
      "%6 = cast(%5, dtype=\"int8\");\n",
      "%7 = annotation.stop_fusion(%6);\n",
      "%8 = cast(%7, dtype=\"int32\");\n",
      "%9 = add(%8, 512);\n",
      "%10 = right_shift(%9, 10);\n",
      "%11 = clip(%10, a_min=-127f, a_max=127f);\n",
      "cast(%11, dtype=\"float32\")\n",
      "pass\n",
      "pass\n"
     ]
    }
   ],
   "source": [
    "out = rewrite(JUNOCallback(), mod['main'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%input_1: Tensor[(1, 3, 256, 256), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(32, 16, 3, 3), float32], %v_param_4: Tensor[(32), float32], %v_param_5: Tensor[(64, 32, 3, 3), float32], %v_param_6: Tensor[(64), float32]) {\n",
      "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %1 = nn.bias_add(%0, %v_param_2);\n",
      "  %2 = nn.max_pool2d(%1, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %3 = nn.conv2d(%2, %v_param_3, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %4 = nn.bias_add(%3, %v_param_4);\n",
      "  %5 = nn.max_pool2d(%4, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %6 = nn.conv2d(%5, %v_param_5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %7 = nn.bias_add(%6, %v_param_6);\n",
      "  nn.max_pool2d(%7, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0])\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(mod['main'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%input_1: Tensor[(1, 3, 256, 256), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(32, 16, 3, 3), float32], %v_param_4: Tensor[(32), float32], %v_param_5: Tensor[(64, 32, 3, 3), float32], %v_param_6: Tensor[(64), float32]) {\n",
      "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %1 = nn.bias_add(%0, %v_param_2);\n",
      "  %2 = nn.max_pool2d(%1, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %3 = multiply(%2, 16f);\n",
      "  %4 = round(%3);\n",
      "  %5 = clip(%4, a_min=-127f, a_max=127f);\n",
      "  %6 = cast(%5, dtype=\"int8\");\n",
      "  %7 = annotation.stop_fusion(%6);\n",
      "  %8 = cast(%7, dtype=\"int32\");\n",
      "  %9 = add(%8, 512);\n",
      "  %10 = right_shift(%9, 10);\n",
      "  %11 = clip(%10, a_min=-127f, a_max=127f);\n",
      "  %12 = cast(%11, dtype=\"float32\");\n",
      "  %13 = nn.conv2d(%12, %v_param_3, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %14 = nn.bias_add(%13, %v_param_4);\n",
      "  %15 = nn.max_pool2d(%14, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %16 = nn.conv2d(%15, %v_param_5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %17 = nn.bias_add(%16, %v_param_6);\n",
      "  nn.max_pool2d(%17, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0])\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%input_1: Tensor[(1, 3, 256, 256), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(32, 16, 3, 3), float32], %v_param_4: Tensor[(32), float32], %v_param_5: Tensor[(64, 32, 3, 3), float32], %v_param_6: Tensor[(64), float32]) {\n",
      "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %1 = nn.bias_add(%0, %v_param_2);\n",
      "  %2 = nn.max_pool2d(%1, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %3 = nn.conv2d(%2, %v_param_3, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %4 = nn.bias_add(%3, %v_param_4);\n",
      "  %5 = nn.max_pool2d(%4, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %6 = nn.conv2d(%5, %v_param_5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %7 = nn.bias_add(%6, %v_param_6);\n",
      "  nn.max_pool2d(%7, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0])\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_to_int8 = relay.cast(\n",
    "                relay.clip(\n",
    "                    relay.round(\n",
    "                        relay.multiply(mod['main'].body, relay.const(16.0))\n",
    "                    ), \n",
    "                    a_min=-127.0, a_max=127.0\n",
    "                ),\n",
    "                dtype=\"int8\"\n",
    "            )\n",
    "# tvm.relay.transform.InferType(cast_to_int8)\n",
    "# print(cast_to_int8)\n",
    "cast_to_float32 = relay.cast(\n",
    "    relay.clip(\n",
    "        relay.right_shift(\n",
    "            relay.add(relay.cast(cast_to_int8, dtype='int32'), relay.const(512)),\n",
    "            relay.const(10)),\n",
    "        a_min=-127.0, a_max=127.0), \n",
    "    dtype=\"float32\"\n",
    ")\n",
    "# with \n",
    "\n",
    "\n",
    "# print(cast_to_float32)\n",
    "# mod['main'] =cast_to_float32\n",
    "# seq = tvm.transform.Sequential([tvm.relay.transform.InferType()])\n",
    "# new_mod = seq(mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_part_mod = tvm.IRModule.from_expr(cast_to_float32)\n",
    "# print(cast_to_float32)\n",
    "# mod['main'] =cast_to_float32\n",
    "seq = tvm.transform.Sequential([tvm.relay.transform.InferType()])\n",
    "new_mod = seq(new_part_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input_1: Tensor[(1, 3, 256, 256), float32];\n",
      "free_var %v_param_1: Tensor[(16, 3, 3, 3), float32];\n",
      "%0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "free_var %v_param_2: Tensor[(16), float32];\n",
      "%1 = nn.bias_add(%0, %v_param_2);\n",
      "%2 = nn.max_pool2d(%1, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "free_var %v_param_3: Tensor[(32, 16, 3, 3), float32];\n",
      "%3 = nn.conv2d(%2, %v_param_3, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "free_var %v_param_4: Tensor[(32), float32];\n",
      "%4 = nn.bias_add(%3, %v_param_4);\n",
      "%5 = nn.max_pool2d(%4, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "free_var %v_param_5: Tensor[(64, 32, 3, 3), float32];\n",
      "%6 = nn.conv2d(%5, %v_param_5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "free_var %v_param_6: Tensor[(64), float32];\n",
      "%7 = nn.bias_add(%6, %v_param_6);\n",
      "%8 = nn.max_pool2d(%7, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "%9 = multiply(%8, 16f);\n",
      "%10 = round(%9);\n",
      "%11 = clip(%10, a_min=-127f, a_max=127f);\n",
      "%12 = cast(%11, dtype=\"int8\");\n",
      "%13 = cast(%12, dtype=\"int32\");\n",
      "%14 = add(%13, 512);\n",
      "%15 = right_shift(%14, 10);\n",
      "%16 = clip(%15, a_min=-127f, a_max=127f);\n",
      "cast(%16, dtype=\"float32\")\n"
     ]
    }
   ],
   "source": [
    "print(cast_to_float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb Cell 49'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000055vscode-remote?line=0'>1</a>\u001b[0m mod[\u001b[39m'\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m new_mod\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bj-server/home/j/tvm-slicer/tests/very_simple_model/rerelay.ipynb#ch0000055vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(mod[\u001b[39m'\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/ir/module.py:75\u001b[0m, in \u001b[0;36mIRModule.__setitem__\u001b[0;34m(self, var, val)\u001b[0m\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=63'>64</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__setitem__\u001b[39m(\u001b[39mself\u001b[39m, var, val):\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=64'>65</a>\u001b[0m     \u001b[39m\"\"\"Add a mapping to the module.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=65'>66</a>\u001b[0m \n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=66'>67</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=72'>73</a>\u001b[0m \u001b[39m        The value.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=73'>74</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=74'>75</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add(var, val, \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/tvm-slicer/include/tvm/python/tvm/ir/module.py:86\u001b[0m, in \u001b[0;36mIRModule._add\u001b[0;34m(self, var, val, update)\u001b[0m\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=83'>84</a>\u001b[0m     _ffi_api\u001b[39m.\u001b[39mModule_Add(\u001b[39mself\u001b[39m, var, val, update)\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=84'>85</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=85'>86</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(val, _ty\u001b[39m.\u001b[39mType)\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=86'>87</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(var, string_types):\n\u001b[1;32m     <a href='file:///home/j/tvm-slicer/include/tvm/python/tvm/ir/module.py?line=87'>88</a>\u001b[0m         var \u001b[39m=\u001b[39m _ty\u001b[39m.\u001b[39mGlobalTypeVar(var)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mod['main'] = new_mod\n",
    "print(mod['main'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "target = 'cuda'\n",
    "dev = tvm.cuda()\n",
    "# mod['main'] = cast_to_float32\n",
    "# mod['main'] = cast_to_float32\n",
    "with tvm.transform.PassContext(opt_level=0):\n",
    "    lib = relay.build(out, target, params=params)\n",
    "\n",
    "# show_graph(lib['get_graph_json'](), \"quant_end_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'cuda'\n",
    "dev = tvm.cuda()\n",
    "# mod['main'] = cast_to_float32\n",
    "with tvm.transform.PassContext(opt_level=0):\n",
    "    lib = relay.build(out, target, params=params)\n",
    "\n",
    "show_graph(lib['get_graph_json'](), \"quant_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(out, target, params=params)\n",
    "\n",
    "show_graph(lib['get_graph_json'](), \"quant_3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../tvm-slicer/src/model/unet_cuda_front_512_3_29.so\n"
     ]
    }
   ],
   "source": [
    "!ls ../../tvm-slicer/src/model/unet_cuda_front_512_3_29.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../tvm-slicer/src/model/unet_cuda_front_512_3_29.so\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_lib = tvm.runtime.load_module(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"nodes\": [{\"op\": \"null\", \"name\": \"input_0\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p0\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p1\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p2\", \"inputs\": []}, {\"op\": \"tvm_op\", \"name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu\", \"attrs\": {\"num_outputs\": \"1\", \"num_inputs\": \"4\", \"flatten_data\": \"0\", \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu\", \"out_layout\": \"\", \"kernel_layout\": \"OIHW\", \"data_layout\": \"NCHW\", \"hash\": \"3dbb7c2534747700\"}, \"inputs\": [[0, 0, 0], [1, 0, 0], [2, 0, 0], [3, 0, 0]]}, {\"op\": \"null\", \"name\": \"p3\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p4\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p5\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p6\", \"inputs\": []}, {\"op\": \"tvm_op\", \"name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add\", \"attrs\": {\"num_outputs\": \"1\", \"num_inputs\": \"5\", \"flatten_data\": \"0\", \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add\", \"out_layout\": \"\", \"kernel_layout\": \"OIHW\", \"data_layout\": \"NCHW\", \"hash\": \"eb263027e15e9ee8\"}, \"inputs\": [[4, 0, 0], [5, 0, 0], [6, 0, 0], [7, 0, 0], [8, 0, 0]]}, {\"op\": \"tvm_op\", \"name\": \"tvmgen_default_fused_nn_max_pool2d\", \"attrs\": {\"num_outputs\": \"1\", \"num_inputs\": \"1\", \"flatten_data\": \"0\", \"func_name\": \"tvmgen_default_fused_nn_max_pool2d\", \"out_layout\": \"\", \"layout\": \"NCHW\", \"hash\": \"c1a2714972f6995c\"}, \"inputs\": [[9, 0, 0]]}, {\"op\": \"null\", \"name\": \"p7\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p8\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p9\", \"inputs\": []}, {\"op\": \"tvm_op\", \"name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_1\", \"attrs\": {\"num_outputs\": \"1\", \"num_inputs\": \"4\", \"flatten_data\": \"0\", \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_1\", \"out_layout\": \"\", \"kernel_layout\": \"OIHW\", \"data_layout\": \"NCHW\", \"hash\": \"402a583eab6e9d86\"}, \"inputs\": [[10, 0, 0], [11, 0, 0], [12, 0, 0], [13, 0, 0]]}, {\"op\": \"null\", \"name\": \"p10\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p11\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p12\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p13\", \"inputs\": []}, {\"op\": \"tvm_op\", \"name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_1\", \"attrs\": {\"num_outputs\": \"1\", \"num_inputs\": \"5\", \"flatten_data\": \"0\", \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_1\", \"out_layout\": \"\", \"kernel_layout\": \"OIHW\", \"data_layout\": \"NCHW\", \"hash\": \"afcc3e3243b199b7\"}, \"inputs\": [[14, 0, 0], [15, 0, 0], [16, 0, 0], [17, 0, 0], [18, 0, 0]]}, {\"op\": \"tvm_op\", \"name\": \"tvmgen_default_fused_nn_max_pool2d_1\", \"attrs\": {\"num_outputs\": \"1\", \"num_inputs\": \"1\", \"flatten_data\": \"0\", \"func_name\": \"tvmgen_default_fused_nn_max_pool2d_1\", \"out_layout\": \"\", \"layout\": \"NCHW\", \"hash\": \"4ed73dc5cd3319f1\"}, \"inputs\": [[19, 0, 0]]}, {\"op\": \"null\", \"name\": \"p14\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p15\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p16\", \"inputs\": []}, {\"op\": \"tvm_op\", \"name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_2\", \"attrs\": {\"num_outputs\": \"1\", \"num_inputs\": \"4\", \"flatten_data\": \"0\", \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_2\", \"out_layout\": \"\", \"kernel_layout\": \"OIHW\", \"data_layout\": \"NCHW\", \"hash\": \"4a7612bf260c4695\"}, \"inputs\": [[20, 0, 0], [21, 0, 0], [22, 0, 0], [23, 0, 0]]}, {\"op\": \"null\", \"name\": \"p17\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p18\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p19\", \"inputs\": []}, {\"op\": \"null\", \"name\": \"p20\", \"inputs\": []}, {\"op\": \"tvm_op\", \"name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_2\", \"attrs\": {\"num_outputs\": \"1\", \"num_inputs\": \"5\", \"flatten_data\": \"0\", \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_2\", \"out_layout\": \"\", \"kernel_layout\": \"OIHW\", \"data_layout\": \"NCHW\", \"hash\": \"8cb240c20c9a61e5\"}, \"inputs\": [[24, 0, 0], [25, 0, 0], [26, 0, 0], [27, 0, 0], [28, 0, 0]]}], \"arg_nodes\": [0, 1, 2, 3, 5, 6, 7, 8, 11, 12, 13, 15, 16, 17, 18, 21, 22, 23, 25, 26, 27, 28], \"heads\": [[9, 0, 0], [19, 0, 0], [29, 0, 0]], \"attrs\": {\"dltype\": [\"list_str\", [\"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\", \"float32\"]], \"device_index\": [\"list_int\", [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], \"storage_id\": [\"list_int\", [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 10, 11, 12, 13, 14, 15, 16, 17, 4, 13, 18, 19, 20, 21, 22, 23, 24, 25, 13]], \"shape\": [\"list_shape\", [[1, 3, 512, 512], [16, 3, 3, 3], [1, 16, 1, 1], [1, 16, 1, 1], [1, 16, 512, 512], [16, 16, 3, 3], [1, 16, 1, 1], [1, 16, 1, 1], [1, 16, 1, 1], [1, 16, 512, 512], [1, 16, 256, 256], [32, 16, 3, 3], [1, 32, 1, 1], [1, 32, 1, 1], [1, 32, 256, 256], [32, 32, 3, 3], [1, 32, 1, 1], [1, 32, 1, 1], [1, 32, 1, 1], [1, 32, 256, 256], [1, 32, 128, 128], [64, 32, 3, 3], [1, 64, 1, 1], [1, 64, 1, 1], [1, 64, 128, 128], [64, 64, 3, 3], [1, 64, 1, 1], [1, 64, 1, 1], [1, 64, 1, 1], [1, 64, 128, 128]]]}, \"node_row_ptr\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]}\n"
     ]
    }
   ],
   "source": [
    "print(front_lib['get_graph_json']())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.contrib import graph_executor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_model = graph_executor.GraphModule(front_lib['default'](tvm.cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras = tf.keras.models.load_model('../../tvm-slicer/src/model/{}_{}.h5'.format(\"unet\", 512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.random.normal(0,1,(1,512,512,3)).astype(np.float32)\n",
    "input_data = input_data.transpose([0, 3, 1, 2])\n",
    "shape_dict = {\"input_1\": input_data.shape}\n",
    "mod, params = relay.frontend.from_keras(model_keras, shape_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'cuda'\n",
    "dev = tvm.cuda()\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#[version = \"0.0.5\"]\n",
      "def @main(%input_1: Tensor[(1, 3, 512, 512), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(16), float32], %v_param_4: Tensor[(16), float32], %v_param_5: Tensor[(16), float32], %v_param_6: Tensor[(16), float32], %v_param_7: Tensor[(16, 16, 3, 3), float32], %v_param_8: Tensor[(16), float32], %v_param_9: Tensor[(16), float32], %v_param_10: Tensor[(16), float32], %v_param_11: Tensor[(16), float32], %v_param_12: Tensor[(16), float32], %v_param_13: Tensor[(32, 16, 3, 3), float32], %v_param_14: Tensor[(32), float32], %v_param_15: Tensor[(32), float32], %v_param_16: Tensor[(32), float32], %v_param_17: Tensor[(32), float32], %v_param_18: Tensor[(32), float32], %v_param_19: Tensor[(32, 32, 3, 3), float32], %v_param_20: Tensor[(32), float32], %v_param_21: Tensor[(32), float32], %v_param_22: Tensor[(32), float32], %v_param_23: Tensor[(32), float32], %v_param_24: Tensor[(32), float32], %v_param_25: Tensor[(64, 32, 3, 3), float32], %v_param_26: Tensor[(64), float32], %v_param_27: Tensor[(64), float32], %v_param_28: Tensor[(64), float32], %v_param_29: Tensor[(64), float32], %v_param_30: Tensor[(64), float32], %v_param_31: Tensor[(64, 64, 3, 3), float32], %v_param_32: Tensor[(64), float32], %v_param_33: Tensor[(64), float32], %v_param_34: Tensor[(64), float32], %v_param_35: Tensor[(64), float32], %v_param_36: Tensor[(64), float32], %v_param_37: Tensor[(128, 64, 3, 3), float32], %v_param_38: Tensor[(128), float32], %v_param_39: Tensor[(128), float32], %v_param_40: Tensor[(128), float32], %v_param_41: Tensor[(128), float32], %v_param_42: Tensor[(128), float32], %v_param_43: Tensor[(128, 128, 3, 3), float32], %v_param_44: Tensor[(128), float32], %v_param_45: Tensor[(128), float32], %v_param_46: Tensor[(128), float32], %v_param_47: Tensor[(128), float32], %v_param_48: Tensor[(128), float32], %v_param_49: Tensor[(256, 128, 3, 3), float32], %v_param_50: Tensor[(256), float32], %v_param_51: Tensor[(256), float32], %v_param_52: Tensor[(256), float32], %v_param_53: Tensor[(256), float32], %v_param_54: Tensor[(256), float32], %v_param_55: Tensor[(256, 256, 3, 3), float32], %v_param_56: Tensor[(256), float32], %v_param_57: Tensor[(256), float32], %v_param_58: Tensor[(256), float32], %v_param_59: Tensor[(256), float32], %v_param_60: Tensor[(256), float32], %v_param_61: Tensor[(256, 128, 3, 3), float32], %v_param_62: Tensor[(128), float32], %v_param_63: Tensor[(128), float32], %v_param_64: Tensor[(128), float32], %v_param_65: Tensor[(128), float32], %v_param_66: Tensor[(128), float32], %v_param_67: Tensor[(128, 256, 3, 3), float32], %v_param_68: Tensor[(128), float32], %v_param_69: Tensor[(128), float32], %v_param_70: Tensor[(128), float32], %v_param_71: Tensor[(128), float32], %v_param_72: Tensor[(128), float32], %v_param_73: Tensor[(128, 128, 3, 3), float32], %v_param_74: Tensor[(128), float32], %v_param_75: Tensor[(128), float32], %v_param_76: Tensor[(128), float32], %v_param_77: Tensor[(128), float32], %v_param_78: Tensor[(128), float32], %v_param_79: Tensor[(128, 64, 3, 3), float32], %v_param_80: Tensor[(64), float32], %v_param_81: Tensor[(64), float32], %v_param_82: Tensor[(64), float32], %v_param_83: Tensor[(64), float32], %v_param_84: Tensor[(64), float32], %v_param_85: Tensor[(64, 128, 3, 3), float32], %v_param_86: Tensor[(64), float32], %v_param_87: Tensor[(64), float32], %v_param_88: Tensor[(64), float32], %v_param_89: Tensor[(64), float32], %v_param_90: Tensor[(64), float32], %v_param_91: Tensor[(64, 64, 3, 3), float32], %v_param_92: Tensor[(64), float32], %v_param_93: Tensor[(64), float32], %v_param_94: Tensor[(64), float32], %v_param_95: Tensor[(64), float32], %v_param_96: Tensor[(64), float32], %v_param_97: Tensor[(64, 32, 3, 3), float32], %v_param_98: Tensor[(32), float32], %v_param_99: Tensor[(32), float32], %v_param_100: Tensor[(32), float32], %v_param_101: Tensor[(32), float32], %v_param_102: Tensor[(32), float32], %v_param_103: Tensor[(32, 64, 3, 3), float32], %v_param_104: Tensor[(32), float32], %v_param_105: Tensor[(32), float32], %v_param_106: Tensor[(32), float32], %v_param_107: Tensor[(32), float32], %v_param_108: Tensor[(32), float32], %v_param_109: Tensor[(32, 32, 3, 3), float32], %v_param_110: Tensor[(32), float32], %v_param_111: Tensor[(32), float32], %v_param_112: Tensor[(32), float32], %v_param_113: Tensor[(32), float32], %v_param_114: Tensor[(32), float32], %v_param_115: Tensor[(32, 16, 3, 3), float32], %v_param_116: Tensor[(16), float32], %v_param_117: Tensor[(16), float32], %v_param_118: Tensor[(16), float32], %v_param_119: Tensor[(16), float32], %v_param_120: Tensor[(16), float32], %v_param_121: Tensor[(16, 32, 3, 3), float32], %v_param_122: Tensor[(16), float32], %v_param_123: Tensor[(16), float32], %v_param_124: Tensor[(16), float32], %v_param_125: Tensor[(16), float32], %v_param_126: Tensor[(16), float32], %v_param_127: Tensor[(16, 16, 3, 3), float32], %v_param_128: Tensor[(16), float32], %v_param_129: Tensor[(16), float32], %v_param_130: Tensor[(16), float32], %v_param_131: Tensor[(16), float32], %v_param_132: Tensor[(16), float32], %v_param_133: Tensor[(1, 16, 3, 3), float32], %v_param_134: Tensor[(1), float32]) {\n",
      "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %1 = nn.bias_add(%0, %v_param_2);\n",
      "  %2 = nn.batch_norm(%1, %v_param_3, %v_param_4, %v_param_5, %v_param_6, epsilon=0.001f);\n",
      "  %3 = %2.0;\n",
      "  %4 = nn.leaky_relu(%3, alpha=0.2f);\n",
      "  %5 = nn.conv2d(%4, %v_param_7, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %6 = nn.bias_add(%5, %v_param_8);\n",
      "  %7 = nn.batch_norm(%6, %v_param_9, %v_param_10, %v_param_11, %v_param_12, epsilon=0.001f);\n",
      "  %8 = %7.0;\n",
      "  %9 = nn.max_pool2d(%8, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %10 = nn.conv2d(%9, %v_param_13, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %11 = nn.bias_add(%10, %v_param_14);\n",
      "  %12 = nn.batch_norm(%11, %v_param_15, %v_param_16, %v_param_17, %v_param_18, epsilon=0.001f);\n",
      "  %13 = %12.0;\n",
      "  %14 = nn.leaky_relu(%13, alpha=0.2f);\n",
      "  %15 = nn.conv2d(%14, %v_param_19, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %16 = nn.bias_add(%15, %v_param_20);\n",
      "  %17 = nn.batch_norm(%16, %v_param_21, %v_param_22, %v_param_23, %v_param_24, epsilon=0.001f);\n",
      "  %18 = %17.0;\n",
      "  %19 = nn.max_pool2d(%18, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %20 = nn.conv2d(%19, %v_param_25, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %21 = nn.bias_add(%20, %v_param_26);\n",
      "  %22 = nn.batch_norm(%21, %v_param_27, %v_param_28, %v_param_29, %v_param_30, epsilon=0.001f);\n",
      "  %23 = %22.0;\n",
      "  %24 = nn.leaky_relu(%23, alpha=0.2f);\n",
      "  %25 = nn.conv2d(%24, %v_param_31, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %26 = nn.bias_add(%25, %v_param_32);\n",
      "  %27 = nn.batch_norm(%26, %v_param_33, %v_param_34, %v_param_35, %v_param_36, epsilon=0.001f);\n",
      "  %28 = %27.0;\n",
      "  %29 = nn.max_pool2d(%28, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %30 = nn.conv2d(%29, %v_param_37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %31 = nn.bias_add(%30, %v_param_38);\n",
      "  %32 = nn.batch_norm(%31, %v_param_39, %v_param_40, %v_param_41, %v_param_42, epsilon=0.001f);\n",
      "  %33 = %32.0;\n",
      "  %34 = nn.leaky_relu(%33, alpha=0.2f);\n",
      "  %35 = nn.conv2d(%34, %v_param_43, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %36 = nn.bias_add(%35, %v_param_44);\n",
      "  %37 = nn.batch_norm(%36, %v_param_45, %v_param_46, %v_param_47, %v_param_48, epsilon=0.001f);\n",
      "  %38 = %37.0;\n",
      "  %39 = nn.max_pool2d(%38, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %40 = nn.conv2d(%39, %v_param_49, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %41 = nn.bias_add(%40, %v_param_50);\n",
      "  %42 = nn.batch_norm(%41, %v_param_51, %v_param_52, %v_param_53, %v_param_54, epsilon=0.001f);\n",
      "  %43 = %42.0;\n",
      "  %44 = nn.leaky_relu(%43, alpha=0.2f);\n",
      "  %45 = nn.conv2d(%44, %v_param_55, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %46 = nn.bias_add(%45, %v_param_56);\n",
      "  %47 = nn.batch_norm(%46, %v_param_57, %v_param_58, %v_param_59, %v_param_60, epsilon=0.001f);\n",
      "  %48 = %47.0;\n",
      "  %49 = nn.conv2d_transpose(%48, %v_param_61, channels=128, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
      "  %50 = nn.bias_add(%49, %v_param_62);\n",
      "  %51 = nn.batch_norm(%50, %v_param_63, %v_param_64, %v_param_65, %v_param_66, epsilon=0.001f);\n",
      "  %52 = %51.0;\n",
      "  %53 = nn.leaky_relu(%52, alpha=0.2f);\n",
      "  %54 = (%53, %38);\n",
      "  %55 = concatenate(%54, axis=1);\n",
      "  %56 = nn.conv2d(%55, %v_param_67, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %57 = nn.bias_add(%56, %v_param_68);\n",
      "  %58 = nn.batch_norm(%57, %v_param_69, %v_param_70, %v_param_71, %v_param_72, epsilon=0.001f);\n",
      "  %59 = %58.0;\n",
      "  %60 = nn.leaky_relu(%59, alpha=0.2f);\n",
      "  %61 = nn.conv2d(%60, %v_param_73, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %62 = nn.bias_add(%61, %v_param_74);\n",
      "  %63 = nn.batch_norm(%62, %v_param_75, %v_param_76, %v_param_77, %v_param_78, epsilon=0.001f);\n",
      "  %64 = %63.0;\n",
      "  %65 = nn.conv2d_transpose(%64, %v_param_79, channels=64, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
      "  %66 = nn.bias_add(%65, %v_param_80);\n",
      "  %67 = nn.batch_norm(%66, %v_param_81, %v_param_82, %v_param_83, %v_param_84, epsilon=0.001f);\n",
      "  %68 = %67.0;\n",
      "  %69 = nn.leaky_relu(%68, alpha=0.2f);\n",
      "  %70 = (%69, %28);\n",
      "  %71 = concatenate(%70, axis=1);\n",
      "  %72 = nn.conv2d(%71, %v_param_85, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %73 = nn.bias_add(%72, %v_param_86);\n",
      "  %74 = nn.batch_norm(%73, %v_param_87, %v_param_88, %v_param_89, %v_param_90, epsilon=0.001f);\n",
      "  %75 = %74.0;\n",
      "  %76 = nn.leaky_relu(%75, alpha=0.2f);\n",
      "  %77 = nn.conv2d(%76, %v_param_91, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %78 = nn.bias_add(%77, %v_param_92);\n",
      "  %79 = nn.batch_norm(%78, %v_param_93, %v_param_94, %v_param_95, %v_param_96, epsilon=0.001f);\n",
      "  %80 = %79.0;\n",
      "  %81 = nn.conv2d_transpose(%80, %v_param_97, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
      "  %82 = nn.bias_add(%81, %v_param_98);\n",
      "  %83 = nn.batch_norm(%82, %v_param_99, %v_param_100, %v_param_101, %v_param_102, epsilon=0.001f);\n",
      "  %84 = %83.0;\n",
      "  %85 = nn.leaky_relu(%84, alpha=0.2f);\n",
      "  %86 = (%85, %18);\n",
      "  %87 = concatenate(%86, axis=1);\n",
      "  %88 = nn.conv2d(%87, %v_param_103, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %89 = nn.bias_add(%88, %v_param_104);\n",
      "  %90 = nn.batch_norm(%89, %v_param_105, %v_param_106, %v_param_107, %v_param_108, epsilon=0.001f);\n",
      "  %91 = %90.0;\n",
      "  %92 = nn.leaky_relu(%91, alpha=0.2f);\n",
      "  %93 = nn.conv2d(%92, %v_param_109, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %94 = nn.bias_add(%93, %v_param_110);\n",
      "  %95 = nn.batch_norm(%94, %v_param_111, %v_param_112, %v_param_113, %v_param_114, epsilon=0.001f);\n",
      "  %96 = %95.0;\n",
      "  %97 = nn.conv2d_transpose(%96, %v_param_115, channels=16, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
      "  %98 = nn.bias_add(%97, %v_param_116);\n",
      "  %99 = nn.batch_norm(%98, %v_param_117, %v_param_118, %v_param_119, %v_param_120, epsilon=0.001f);\n",
      "  %100 = %99.0;\n",
      "  %101 = nn.leaky_relu(%100, alpha=0.2f);\n",
      "  %102 = (%101, %8);\n",
      "  %103 = concatenate(%102, axis=1);\n",
      "  %104 = nn.conv2d(%103, %v_param_121, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %105 = nn.bias_add(%104, %v_param_122);\n",
      "  %106 = nn.batch_norm(%105, %v_param_123, %v_param_124, %v_param_125, %v_param_126, epsilon=0.001f);\n",
      "  %107 = %106.0;\n",
      "  %108 = nn.leaky_relu(%107, alpha=0.2f);\n",
      "  %109 = nn.conv2d(%108, %v_param_127, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %110 = nn.bias_add(%109, %v_param_128);\n",
      "  %111 = nn.batch_norm(%110, %v_param_129, %v_param_130, %v_param_131, %v_param_132, epsilon=0.001f);\n",
      "  %112 = %111.0;\n",
      "  %113 = nn.conv2d(%112, %v_param_133, padding=[1, 1, 1, 1], channels=1, kernel_size=[3, 3]);\n",
      "  %114 = nn.bias_add(%113, %v_param_134);\n",
      "  sigmoid(%114)\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mod.astext())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SlicingMachine import TVMSlicer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"nodes\": [\\n    {\\n      \"op\": \"null\", \\n      \"name\": \"input_1\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p0\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p1\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p2\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"4\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"3dbb7c2534747700\"\\n      }, \\n      \"inputs\": [\\n        [\\n          0, \\n          0, \\n          0\\n        ], \\n        [\\n          1, \\n          0, \\n          0\\n        ], \\n        [\\n          2, \\n          0, \\n          0\\n        ], \\n        [\\n          3, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p3\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p4\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p5\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p6\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"5\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"eb263027e15e9ee8\"\\n      }, \\n      \"inputs\": [\\n        [\\n          4, \\n          0, \\n          0\\n        ], \\n        [\\n          5, \\n          0, \\n          0\\n        ], \\n        [\\n          6, \\n          0, \\n          0\\n        ], \\n        [\\n          7, \\n          0, \\n          0\\n        ], \\n        [\\n          8, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_max_pool2d\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"1\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_max_pool2d\", \\n        \"out_layout\": \"\", \\n        \"layout\": \"NCHW\", \\n        \"hash\": \"c1a2714972f6995c\"\\n      }, \\n      \"inputs\": [\\n        [\\n          9, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p7\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p8\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p9\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_1\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"4\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_1\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"402a583eab6e9d86\"\\n      }, \\n      \"inputs\": [\\n        [\\n          10, \\n          0, \\n          0\\n        ], \\n        [\\n          11, \\n          0, \\n          0\\n        ], \\n        [\\n          12, \\n          0, \\n          0\\n        ], \\n        [\\n          13, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p10\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p11\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p12\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p13\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_1\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"5\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_1\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"afcc3e3243b199b7\"\\n      }, \\n      \"inputs\": [\\n        [\\n          14, \\n          0, \\n          0\\n        ], \\n        [\\n          15, \\n          0, \\n          0\\n        ], \\n        [\\n          16, \\n          0, \\n          0\\n        ], \\n        [\\n          17, \\n          0, \\n          0\\n        ], \\n        [\\n          18, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_max_pool2d_1\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"1\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_max_pool2d_1\", \\n        \"out_layout\": \"\", \\n        \"layout\": \"NCHW\", \\n        \"hash\": \"4ed73dc5cd3319f1\"\\n      }, \\n      \"inputs\": [\\n        [\\n          19, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p14\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p15\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p16\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_2\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"4\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_2\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"4a7612bf260c4695\"\\n      }, \\n      \"inputs\": [\\n        [\\n          20, \\n          0, \\n          0\\n        ], \\n        [\\n          21, \\n          0, \\n          0\\n        ], \\n        [\\n          22, \\n          0, \\n          0\\n        ], \\n        [\\n          23, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p17\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p18\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p19\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p20\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_2\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"5\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_2\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"8cb240c20c9a61e5\"\\n      }, \\n      \"inputs\": [\\n        [\\n          24, \\n          0, \\n          0\\n        ], \\n        [\\n          25, \\n          0, \\n          0\\n        ], \\n        [\\n          26, \\n          0, \\n          0\\n        ], \\n        [\\n          27, \\n          0, \\n          0\\n        ], \\n        [\\n          28, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_max_pool2d_2\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"1\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_max_pool2d_2\", \\n        \"out_layout\": \"\", \\n        \"layout\": \"NCHW\", \\n        \"hash\": \"cb616bafe17574b2\"\\n      }, \\n      \"inputs\": [\\n        [\\n          29, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p21\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p22\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p23\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_3\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"4\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_3\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"28c22030fbad6c26\"\\n      }, \\n      \"inputs\": [\\n        [\\n          30, \\n          0, \\n          0\\n        ], \\n        [\\n          31, \\n          0, \\n          0\\n        ], \\n        [\\n          32, \\n          0, \\n          0\\n        ], \\n        [\\n          33, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p24\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p25\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p26\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p27\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_3\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"5\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_3\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"eef4cb8b0b95c0f6\"\\n      }, \\n      \"inputs\": [\\n        [\\n          34, \\n          0, \\n          0\\n        ], \\n        [\\n          35, \\n          0, \\n          0\\n        ], \\n        [\\n          36, \\n          0, \\n          0\\n        ], \\n        [\\n          37, \\n          0, \\n          0\\n        ], \\n        [\\n          38, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_max_pool2d_3\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"1\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_max_pool2d_3\", \\n        \"out_layout\": \"\", \\n        \"layout\": \"NCHW\", \\n        \"hash\": \"8f41a8f1878e35e5\"\\n      }, \\n      \"inputs\": [\\n        [\\n          39, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p28\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p29\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p30\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_4\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"4\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_4\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"507c2c1b845a6d33\"\\n      }, \\n      \"inputs\": [\\n        [\\n          40, \\n          0, \\n          0\\n        ], \\n        [\\n          41, \\n          0, \\n          0\\n        ], \\n        [\\n          42, \\n          0, \\n          0\\n        ], \\n        [\\n          43, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p31\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p32\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p33\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p34\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_4\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"5\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_4\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"8f709a2d2851a3c9\"\\n      }, \\n      \"inputs\": [\\n        [\\n          44, \\n          0, \\n          0\\n        ], \\n        [\\n          45, \\n          0, \\n          0\\n        ], \\n        [\\n          46, \\n          0, \\n          0\\n        ], \\n        [\\n          47, \\n          0, \\n          0\\n        ], \\n        [\\n          48, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p35\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p36\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p37\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p38\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"5\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"IOHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"2b546a346317a518\"\\n      }, \\n      \"inputs\": [\\n        [\\n          49, \\n          0, \\n          0\\n        ], \\n        [\\n          50, \\n          0, \\n          0\\n        ], \\n        [\\n          51, \\n          0, \\n          0\\n        ], \\n        [\\n          52, \\n          0, \\n          0\\n        ], \\n        [\\n          53, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_concatenate\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"2\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_concatenate\", \\n        \"hash\": \"a0cd721541e9f7e5\"\\n      }, \\n      \"inputs\": [\\n        [\\n          54, \\n          0, \\n          0\\n        ], \\n        [\\n          39, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p39\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p40\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p41\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_5\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"4\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_5\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"02e6e84308b7771f\"\\n      }, \\n      \"inputs\": [\\n        [\\n          55, \\n          0, \\n          0\\n        ], \\n        [\\n          56, \\n          0, \\n          0\\n        ], \\n        [\\n          57, \\n          0, \\n          0\\n        ], \\n        [\\n          58, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p42\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p43\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p44\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p45\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_31\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"5\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_3\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"eef4cb8b0b95c0f6\"\\n      }, \\n      \"inputs\": [\\n        [\\n          59, \\n          0, \\n          0\\n        ], \\n        [\\n          60, \\n          0, \\n          0\\n        ], \\n        [\\n          61, \\n          0, \\n          0\\n        ], \\n        [\\n          62, \\n          0, \\n          0\\n        ], \\n        [\\n          63, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p46\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p47\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p48\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p49\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_1\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"5\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_1\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"IOHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"7c3bb1d0f4348302\"\\n      }, \\n      \"inputs\": [\\n        [\\n          64, \\n          0, \\n          0\\n        ], \\n        [\\n          65, \\n          0, \\n          0\\n        ], \\n        [\\n          66, \\n          0, \\n          0\\n        ], \\n        [\\n          67, \\n          0, \\n          0\\n        ], \\n        [\\n          68, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_concatenate_1\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"2\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_concatenate_1\", \\n        \"hash\": \"3a8a880cc13869bd\"\\n      }, \\n      \"inputs\": [\\n        [\\n          69, \\n          0, \\n          0\\n        ], \\n        [\\n          29, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p50\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p51\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p52\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_6\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"4\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_6\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"7be6ecb9f8d28f4d\"\\n      }, \\n      \"inputs\": [\\n        [\\n          70, \\n          0, \\n          0\\n        ], \\n        [\\n          71, \\n          0, \\n          0\\n        ], \\n        [\\n          72, \\n          0, \\n          0\\n        ], \\n        [\\n          73, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p53\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p54\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p55\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p56\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_21\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"5\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_2\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"8cb240c20c9a61e5\"\\n      }, \\n      \"inputs\": [\\n        [\\n          74, \\n          0, \\n          0\\n        ], \\n        [\\n          75, \\n          0, \\n          0\\n        ], \\n        [\\n          76, \\n          0, \\n          0\\n        ], \\n        [\\n          77, \\n          0, \\n          0\\n        ], \\n        [\\n          78, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p57\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p58\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p59\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p60\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_2\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"5\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_2\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"IOHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"a19d4dacf72f18f3\"\\n      }, \\n      \"inputs\": [\\n        [\\n          79, \\n          0, \\n          0\\n        ], \\n        [\\n          80, \\n          0, \\n          0\\n        ], \\n        [\\n          81, \\n          0, \\n          0\\n        ], \\n        [\\n          82, \\n          0, \\n          0\\n        ], \\n        [\\n          83, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_concatenate_2\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"2\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_concatenate_2\", \\n        \"hash\": \"0d688a8f5e75b65e\"\\n      }, \\n      \"inputs\": [\\n        [\\n          84, \\n          0, \\n          0\\n        ], \\n        [\\n          19, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p61\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p62\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p63\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_7\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"4\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_7\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"f81b264d19b9a6be\"\\n      }, \\n      \"inputs\": [\\n        [\\n          85, \\n          0, \\n          0\\n        ], \\n        [\\n          86, \\n          0, \\n          0\\n        ], \\n        [\\n          87, \\n          0, \\n          0\\n        ], \\n        [\\n          88, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p64\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p65\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p66\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p67\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_11\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"5\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_multiply_add_1\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"afcc3e3243b199b7\"\\n      }, \\n      \"inputs\": [\\n        [\\n          89, \\n          0, \\n          0\\n        ], \\n        [\\n          90, \\n          0, \\n          0\\n        ], \\n        [\\n          91, \\n          0, \\n          0\\n        ], \\n        [\\n          92, \\n          0, \\n          0\\n        ], \\n        [\\n          93, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p68\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p69\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p70\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p71\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_3\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"5\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_3\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"IOHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"e2fccacfae4878e3\"\\n      }, \\n      \"inputs\": [\\n        [\\n          94, \\n          0, \\n          0\\n        ], \\n        [\\n          95, \\n          0, \\n          0\\n        ], \\n        [\\n          96, \\n          0, \\n          0\\n        ], \\n        [\\n          97, \\n          0, \\n          0\\n        ], \\n        [\\n          98, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_concatenate_3\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"2\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_concatenate_3\", \\n        \"hash\": \"88c150325a68c3fc\"\\n      }, \\n      \"inputs\": [\\n        [\\n          99, \\n          0, \\n          0\\n        ], \\n        [\\n          9, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p72\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p73\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p74\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_8\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"4\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_8\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"0258e262c093a80d\"\\n      }, \\n      \"inputs\": [\\n        [\\n          100, \\n          0, \\n          0\\n        ], \\n        [\\n          101, \\n          0, \\n          0\\n        ], \\n        [\\n          102, \\n          0, \\n          0\\n        ], \\n        [\\n          103, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p75\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p76\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p77\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_add\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"4\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_add\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"b83cfcda2b3569ee\"\\n      }, \\n      \"inputs\": [\\n        [\\n          104, \\n          0, \\n          0\\n        ], \\n        [\\n          105, \\n          0, \\n          0\\n        ], \\n        [\\n          106, \\n          0, \\n          0\\n        ], \\n        [\\n          107, \\n          0, \\n          0\\n        ]\\n      ]\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p78\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"null\", \\n      \"name\": \"p79\", \\n      \"inputs\": []\\n    }, \\n    {\\n      \"op\": \"tvm_op\", \\n      \"name\": \"tvmgen_default_fused_nn_conv2d_add_sigmoid\", \\n      \"attrs\": {\\n        \"num_outputs\": \"1\", \\n        \"num_inputs\": \"3\", \\n        \"flatten_data\": \"0\", \\n        \"func_name\": \"tvmgen_default_fused_nn_conv2d_add_sigmoid\", \\n        \"out_layout\": \"\", \\n        \"kernel_layout\": \"OIHW\", \\n        \"data_layout\": \"NCHW\", \\n        \"hash\": \"61cdf1c6242552b1\"\\n      }, \\n      \"inputs\": [\\n        [\\n          108, \\n          0, \\n          0\\n        ], \\n        [\\n          109, \\n          0, \\n          0\\n        ], \\n        [\\n          110, \\n          0, \\n          0\\n        ]\\n      ]\\n    }\\n  ], \\n  \"arg_nodes\": [\\n    0, \\n    1, \\n    2, \\n    3, \\n    5, \\n    6, \\n    7, \\n    8, \\n    11, \\n    12, \\n    13, \\n    15, \\n    16, \\n    17, \\n    18, \\n    21, \\n    22, \\n    23, \\n    25, \\n    26, \\n    27, \\n    28, \\n    31, \\n    32, \\n    33, \\n    35, \\n    36, \\n    37, \\n    38, \\n    41, \\n    42, \\n    43, \\n    45, \\n    46, \\n    47, \\n    48, \\n    50, \\n    51, \\n    52, \\n    53, \\n    56, \\n    57, \\n    58, \\n    60, \\n    61, \\n    62, \\n    63, \\n    65, \\n    66, \\n    67, \\n    68, \\n    71, \\n    72, \\n    73, \\n    75, \\n    76, \\n    77, \\n    78, \\n    80, \\n    81, \\n    82, \\n    83, \\n    86, \\n    87, \\n    88, \\n    90, \\n    91, \\n    92, \\n    93, \\n    95, \\n    96, \\n    97, \\n    98, \\n    101, \\n    102, \\n    103, \\n    105, \\n    106, \\n    107, \\n    109, \\n    110\\n  ], \\n  \"heads\": [\\n    [\\n      111, \\n      0, \\n      0\\n    ]\\n  ], \\n  \"attrs\": {\\n    \"dltype\": [\\n      \"list_str\", \\n      [\\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\", \\n        \"float32\"\\n      ]\\n    ], \\n    \"device_index\": [\\n      \"list_int\", \\n      [\\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2, \\n        2\\n      ]\\n    ], \\n    \"storage_id\": [\\n      \"list_int\", \\n      [\\n        0, \\n        1, \\n        2, \\n        3, \\n        4, \\n        5, \\n        6, \\n        7, \\n        8, \\n        9, \\n        4, \\n        10, \\n        11, \\n        12, \\n        13, \\n        14, \\n        15, \\n        16, \\n        17, \\n        4, \\n        13, \\n        18, \\n        19, \\n        20, \\n        21, \\n        22, \\n        23, \\n        24, \\n        25, \\n        13, \\n        21, \\n        26, \\n        27, \\n        28, \\n        29, \\n        30, \\n        31, \\n        32, \\n        33, \\n        21, \\n        29, \\n        34, \\n        35, \\n        36, \\n        37, \\n        38, \\n        39, \\n        40, \\n        41, \\n        29, \\n        42, \\n        43, \\n        44, \\n        45, \\n        37, \\n        29, \\n        46, \\n        47, \\n        48, \\n        37, \\n        49, \\n        50, \\n        51, \\n        52, \\n        21, \\n        53, \\n        54, \\n        55, \\n        56, \\n        29, \\n        21, \\n        57, \\n        58, \\n        59, \\n        29, \\n        60, \\n        61, \\n        62, \\n        63, \\n        13, \\n        64, \\n        65, \\n        66, \\n        67, \\n        21, \\n        13, \\n        68, \\n        69, \\n        70, \\n        21, \\n        71, \\n        72, \\n        73, \\n        74, \\n        4, \\n        75, \\n        76, \\n        77, \\n        78, \\n        13, \\n        4, \\n        79, \\n        80, \\n        81, \\n        13, \\n        82, \\n        83, \\n        84, \\n        9, \\n        85, \\n        86, \\n        37\\n      ]\\n    ], \\n    \"shape\": [\\n      \"list_shape\", \\n      [\\n        [1, 3, 512, 512], \\n        [16, 3, 3, 3], \\n        [1, 16, 1, 1], \\n        [1, 16, 1, 1], \\n        [1, 16, 512, 512], \\n        [16, 16, 3, 3], \\n        [1, 16, 1, 1], \\n        [1, 16, 1, 1], \\n        [1, 16, 1, 1], \\n        [1, 16, 512, 512], \\n        [1, 16, 256, 256], \\n        [32, 16, 3, 3], \\n        [1, 32, 1, 1], \\n        [1, 32, 1, 1], \\n        [1, 32, 256, 256], \\n        [32, 32, 3, 3], \\n        [1, 32, 1, 1], \\n        [1, 32, 1, 1], \\n        [1, 32, 1, 1], \\n        [1, 32, 256, 256], \\n        [1, 32, 128, 128], \\n        [64, 32, 3, 3], \\n        [1, 64, 1, 1], \\n        [1, 64, 1, 1], \\n        [1, 64, 128, 128], \\n        [64, 64, 3, 3], \\n        [1, 64, 1, 1], \\n        [1, 64, 1, 1], \\n        [1, 64, 1, 1], \\n        [1, 64, 128, 128], \\n        [1, 64, 64, 64], \\n        [128, 64, 3, 3], \\n        [1, 128, 1, 1], \\n        [1, 128, 1, 1], \\n        [1, 128, 64, 64], \\n        [128, 128, 3, 3], \\n        [1, 128, 1, 1], \\n        [1, 128, 1, 1], \\n        [1, 128, 1, 1], \\n        [1, 128, 64, 64], \\n        [1, 128, 32, 32], \\n        [256, 128, 3, 3], \\n        [1, 256, 1, 1], \\n        [1, 256, 1, 1], \\n        [1, 256, 32, 32], \\n        [256, 256, 3, 3], \\n        [1, 256, 1, 1], \\n        [1, 256, 1, 1], \\n        [1, 256, 1, 1], \\n        [1, 256, 32, 32], \\n        [256, 128, 3, 3], \\n        [1, 128, 1, 1], \\n        [1, 128, 1, 1], \\n        [1, 128, 1, 1], \\n        [1, 128, 64, 64], \\n        [1, 256, 64, 64], \\n        [128, 256, 3, 3], \\n        [1, 128, 1, 1], \\n        [1, 128, 1, 1], \\n        [1, 128, 64, 64], \\n        [128, 128, 3, 3], \\n        [1, 128, 1, 1], \\n        [1, 128, 1, 1], \\n        [1, 128, 1, 1], \\n        [1, 128, 64, 64], \\n        [128, 64, 3, 3], \\n        [1, 64, 1, 1], \\n        [1, 64, 1, 1], \\n        [1, 64, 1, 1], \\n        [1, 64, 128, 128], \\n        [1, 128, 128, 128], \\n        [64, 128, 3, 3], \\n        [1, 64, 1, 1], \\n        [1, 64, 1, 1], \\n        [1, 64, 128, 128], \\n        [64, 64, 3, 3], \\n        [1, 64, 1, 1], \\n        [1, 64, 1, 1], \\n        [1, 64, 1, 1], \\n        [1, 64, 128, 128], \\n        [64, 32, 3, 3], \\n        [1, 32, 1, 1], \\n        [1, 32, 1, 1], \\n        [1, 32, 1, 1], \\n        [1, 32, 256, 256], \\n        [1, 64, 256, 256], \\n        [32, 64, 3, 3], \\n        [1, 32, 1, 1], \\n        [1, 32, 1, 1], \\n        [1, 32, 256, 256], \\n        [32, 32, 3, 3], \\n        [1, 32, 1, 1], \\n        [1, 32, 1, 1], \\n        [1, 32, 1, 1], \\n        [1, 32, 256, 256], \\n        [32, 16, 3, 3], \\n        [1, 16, 1, 1], \\n        [1, 16, 1, 1], \\n        [1, 16, 1, 1], \\n        [1, 16, 512, 512], \\n        [1, 32, 512, 512], \\n        [16, 32, 3, 3], \\n        [1, 16, 1, 1], \\n        [1, 16, 1, 1], \\n        [1, 16, 512, 512], \\n        [16, 16, 3, 3], \\n        [1, 16, 1, 1], \\n        [1, 16, 1, 1], \\n        [1, 16, 512, 512], \\n        [1, 16, 3, 3], \\n        [1, 1, 1], \\n        [1, 1, 512, 512]\\n      ]\\n    ]\\n  }, \\n  \"node_row_ptr\": [\\n    0, \\n    1, \\n    2, \\n    3, \\n    4, \\n    5, \\n    6, \\n    7, \\n    8, \\n    9, \\n    10, \\n    11, \\n    12, \\n    13, \\n    14, \\n    15, \\n    16, \\n    17, \\n    18, \\n    19, \\n    20, \\n    21, \\n    22, \\n    23, \\n    24, \\n    25, \\n    26, \\n    27, \\n    28, \\n    29, \\n    30, \\n    31, \\n    32, \\n    33, \\n    34, \\n    35, \\n    36, \\n    37, \\n    38, \\n    39, \\n    40, \\n    41, \\n    42, \\n    43, \\n    44, \\n    45, \\n    46, \\n    47, \\n    48, \\n    49, \\n    50, \\n    51, \\n    52, \\n    53, \\n    54, \\n    55, \\n    56, \\n    57, \\n    58, \\n    59, \\n    60, \\n    61, \\n    62, \\n    63, \\n    64, \\n    65, \\n    66, \\n    67, \\n    68, \\n    69, \\n    70, \\n    71, \\n    72, \\n    73, \\n    74, \\n    75, \\n    76, \\n    77, \\n    78, \\n    79, \\n    80, \\n    81, \\n    82, \\n    83, \\n    84, \\n    85, \\n    86, \\n    87, \\n    88, \\n    89, \\n    90, \\n    91, \\n    92, \\n    93, \\n    94, \\n    95, \\n    96, \\n    97, \\n    98, \\n    99, \\n    100, \\n    101, \\n    102, \\n    103, \\n    104, \\n    105, \\n    106, \\n    107, \\n    108, \\n    109, \\n    110, \\n    111, \\n    112\\n  ]\\n}'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_json_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_json_raw = lib['get_graph_json']()\n",
    "tvm_slicer = TVMSlicer(graph_json_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_json_front_info = tvm_slicer.slice_graph(0, 29)\n",
    "graph_json_back_info = tvm_slicer.slice_graph(29, 111)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build_graph(mod, target=target, target_host=None, params=params, mod_name=\"default\", graph_config=json.dumps(graph_json_front_info))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#[version = \"0.0.5\"]\n",
       "def @main(%input_1: Tensor[(1, 3, 512, 512), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(16), float32], %v_param_4: Tensor[(16), float32], %v_param_5: Tensor[(16), float32], %v_param_6: Tensor[(16), float32], %v_param_7: Tensor[(16, 16, 3, 3), float32], %v_param_8: Tensor[(16), float32], %v_param_9: Tensor[(16), float32], %v_param_10: Tensor[(16), float32], %v_param_11: Tensor[(16), float32], %v_param_12: Tensor[(16), float32], %v_param_13: Tensor[(32, 16, 3, 3), float32], %v_param_14: Tensor[(32), float32], %v_param_15: Tensor[(32), float32], %v_param_16: Tensor[(32), float32], %v_param_17: Tensor[(32), float32], %v_param_18: Tensor[(32), float32], %v_param_19: Tensor[(32, 32, 3, 3), float32], %v_param_20: Tensor[(32), float32], %v_param_21: Tensor[(32), float32], %v_param_22: Tensor[(32), float32], %v_param_23: Tensor[(32), float32], %v_param_24: Tensor[(32), float32], %v_param_25: Tensor[(64, 32, 3, 3), float32], %v_param_26: Tensor[(64), float32], %v_param_27: Tensor[(64), float32], %v_param_28: Tensor[(64), float32], %v_param_29: Tensor[(64), float32], %v_param_30: Tensor[(64), float32], %v_param_31: Tensor[(64, 64, 3, 3), float32], %v_param_32: Tensor[(64), float32], %v_param_33: Tensor[(64), float32], %v_param_34: Tensor[(64), float32], %v_param_35: Tensor[(64), float32], %v_param_36: Tensor[(64), float32], %v_param_37: Tensor[(128, 64, 3, 3), float32], %v_param_38: Tensor[(128), float32], %v_param_39: Tensor[(128), float32], %v_param_40: Tensor[(128), float32], %v_param_41: Tensor[(128), float32], %v_param_42: Tensor[(128), float32], %v_param_43: Tensor[(128, 128, 3, 3), float32], %v_param_44: Tensor[(128), float32], %v_param_45: Tensor[(128), float32], %v_param_46: Tensor[(128), float32], %v_param_47: Tensor[(128), float32], %v_param_48: Tensor[(128), float32], %v_param_49: Tensor[(256, 128, 3, 3), float32], %v_param_50: Tensor[(256), float32], %v_param_51: Tensor[(256), float32], %v_param_52: Tensor[(256), float32], %v_param_53: Tensor[(256), float32], %v_param_54: Tensor[(256), float32], %v_param_55: Tensor[(256, 256, 3, 3), float32], %v_param_56: Tensor[(256), float32], %v_param_57: Tensor[(256), float32], %v_param_58: Tensor[(256), float32], %v_param_59: Tensor[(256), float32], %v_param_60: Tensor[(256), float32], %v_param_61: Tensor[(256, 128, 3, 3), float32], %v_param_62: Tensor[(128), float32], %v_param_63: Tensor[(128), float32], %v_param_64: Tensor[(128), float32], %v_param_65: Tensor[(128), float32], %v_param_66: Tensor[(128), float32], %v_param_67: Tensor[(128, 256, 3, 3), float32], %v_param_68: Tensor[(128), float32], %v_param_69: Tensor[(128), float32], %v_param_70: Tensor[(128), float32], %v_param_71: Tensor[(128), float32], %v_param_72: Tensor[(128), float32], %v_param_73: Tensor[(128, 128, 3, 3), float32], %v_param_74: Tensor[(128), float32], %v_param_75: Tensor[(128), float32], %v_param_76: Tensor[(128), float32], %v_param_77: Tensor[(128), float32], %v_param_78: Tensor[(128), float32], %v_param_79: Tensor[(128, 64, 3, 3), float32], %v_param_80: Tensor[(64), float32], %v_param_81: Tensor[(64), float32], %v_param_82: Tensor[(64), float32], %v_param_83: Tensor[(64), float32], %v_param_84: Tensor[(64), float32], %v_param_85: Tensor[(64, 128, 3, 3), float32], %v_param_86: Tensor[(64), float32], %v_param_87: Tensor[(64), float32], %v_param_88: Tensor[(64), float32], %v_param_89: Tensor[(64), float32], %v_param_90: Tensor[(64), float32], %v_param_91: Tensor[(64, 64, 3, 3), float32], %v_param_92: Tensor[(64), float32], %v_param_93: Tensor[(64), float32], %v_param_94: Tensor[(64), float32], %v_param_95: Tensor[(64), float32], %v_param_96: Tensor[(64), float32], %v_param_97: Tensor[(64, 32, 3, 3), float32], %v_param_98: Tensor[(32), float32], %v_param_99: Tensor[(32), float32], %v_param_100: Tensor[(32), float32], %v_param_101: Tensor[(32), float32], %v_param_102: Tensor[(32), float32], %v_param_103: Tensor[(32, 64, 3, 3), float32], %v_param_104: Tensor[(32), float32], %v_param_105: Tensor[(32), float32], %v_param_106: Tensor[(32), float32], %v_param_107: Tensor[(32), float32], %v_param_108: Tensor[(32), float32], %v_param_109: Tensor[(32, 32, 3, 3), float32], %v_param_110: Tensor[(32), float32], %v_param_111: Tensor[(32), float32], %v_param_112: Tensor[(32), float32], %v_param_113: Tensor[(32), float32], %v_param_114: Tensor[(32), float32], %v_param_115: Tensor[(32, 16, 3, 3), float32], %v_param_116: Tensor[(16), float32], %v_param_117: Tensor[(16), float32], %v_param_118: Tensor[(16), float32], %v_param_119: Tensor[(16), float32], %v_param_120: Tensor[(16), float32], %v_param_121: Tensor[(16, 32, 3, 3), float32], %v_param_122: Tensor[(16), float32], %v_param_123: Tensor[(16), float32], %v_param_124: Tensor[(16), float32], %v_param_125: Tensor[(16), float32], %v_param_126: Tensor[(16), float32], %v_param_127: Tensor[(16, 16, 3, 3), float32], %v_param_128: Tensor[(16), float32], %v_param_129: Tensor[(16), float32], %v_param_130: Tensor[(16), float32], %v_param_131: Tensor[(16), float32], %v_param_132: Tensor[(16), float32], %v_param_133: Tensor[(1, 16, 3, 3), float32], %v_param_134: Tensor[(1), float32]) {\n",
       "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %1 = nn.bias_add(%0, %v_param_2);\n",
       "  %2 = nn.batch_norm(%1, %v_param_3, %v_param_4, %v_param_5, %v_param_6, epsilon=0.001f);\n",
       "  %3 = %2.0;\n",
       "  %4 = nn.leaky_relu(%3, alpha=0.2f);\n",
       "  %5 = nn.conv2d(%4, %v_param_7, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %6 = nn.bias_add(%5, %v_param_8);\n",
       "  %7 = nn.batch_norm(%6, %v_param_9, %v_param_10, %v_param_11, %v_param_12, epsilon=0.001f);\n",
       "  %8 = %7.0;\n",
       "  %9 = nn.max_pool2d(%8, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %10 = nn.conv2d(%9, %v_param_13, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %11 = nn.bias_add(%10, %v_param_14);\n",
       "  %12 = nn.batch_norm(%11, %v_param_15, %v_param_16, %v_param_17, %v_param_18, epsilon=0.001f);\n",
       "  %13 = %12.0;\n",
       "  %14 = nn.leaky_relu(%13, alpha=0.2f);\n",
       "  %15 = nn.conv2d(%14, %v_param_19, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %16 = nn.bias_add(%15, %v_param_20);\n",
       "  %17 = nn.batch_norm(%16, %v_param_21, %v_param_22, %v_param_23, %v_param_24, epsilon=0.001f);\n",
       "  %18 = %17.0;\n",
       "  %19 = nn.max_pool2d(%18, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %20 = nn.conv2d(%19, %v_param_25, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %21 = nn.bias_add(%20, %v_param_26);\n",
       "  %22 = nn.batch_norm(%21, %v_param_27, %v_param_28, %v_param_29, %v_param_30, epsilon=0.001f);\n",
       "  %23 = %22.0;\n",
       "  %24 = nn.leaky_relu(%23, alpha=0.2f);\n",
       "  %25 = nn.conv2d(%24, %v_param_31, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %26 = nn.bias_add(%25, %v_param_32);\n",
       "  %27 = nn.batch_norm(%26, %v_param_33, %v_param_34, %v_param_35, %v_param_36, epsilon=0.001f);\n",
       "  %28 = %27.0;\n",
       "  %29 = nn.max_pool2d(%28, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %30 = nn.conv2d(%29, %v_param_37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
       "  %31 = nn.bias_add(%30, %v_param_38);\n",
       "  %32 = nn.batch_norm(%31, %v_param_39, %v_param_40, %v_param_41, %v_param_42, epsilon=0.001f);\n",
       "  %33 = %32.0;\n",
       "  %34 = nn.leaky_relu(%33, alpha=0.2f);\n",
       "  %35 = nn.conv2d(%34, %v_param_43, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
       "  %36 = nn.bias_add(%35, %v_param_44);\n",
       "  %37 = nn.batch_norm(%36, %v_param_45, %v_param_46, %v_param_47, %v_param_48, epsilon=0.001f);\n",
       "  %38 = %37.0;\n",
       "  %39 = nn.max_pool2d(%38, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %40 = nn.conv2d(%39, %v_param_49, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
       "  %41 = nn.bias_add(%40, %v_param_50);\n",
       "  %42 = nn.batch_norm(%41, %v_param_51, %v_param_52, %v_param_53, %v_param_54, epsilon=0.001f);\n",
       "  %43 = %42.0;\n",
       "  %44 = nn.leaky_relu(%43, alpha=0.2f);\n",
       "  %45 = nn.conv2d(%44, %v_param_55, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
       "  %46 = nn.bias_add(%45, %v_param_56);\n",
       "  %47 = nn.batch_norm(%46, %v_param_57, %v_param_58, %v_param_59, %v_param_60, epsilon=0.001f);\n",
       "  %48 = %47.0;\n",
       "  %49 = nn.conv2d_transpose(%48, %v_param_61, channels=128, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
       "  %50 = nn.bias_add(%49, %v_param_62);\n",
       "  %51 = nn.batch_norm(%50, %v_param_63, %v_param_64, %v_param_65, %v_param_66, epsilon=0.001f);\n",
       "  %52 = %51.0;\n",
       "  %53 = nn.leaky_relu(%52, alpha=0.2f);\n",
       "  %54 = (%53, %38);\n",
       "  %55 = concatenate(%54, axis=1);\n",
       "  %56 = nn.conv2d(%55, %v_param_67, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
       "  %57 = nn.bias_add(%56, %v_param_68);\n",
       "  %58 = nn.batch_norm(%57, %v_param_69, %v_param_70, %v_param_71, %v_param_72, epsilon=0.001f);\n",
       "  %59 = %58.0;\n",
       "  %60 = nn.leaky_relu(%59, alpha=0.2f);\n",
       "  %61 = nn.conv2d(%60, %v_param_73, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
       "  %62 = nn.bias_add(%61, %v_param_74);\n",
       "  %63 = nn.batch_norm(%62, %v_param_75, %v_param_76, %v_param_77, %v_param_78, epsilon=0.001f);\n",
       "  %64 = %63.0;\n",
       "  %65 = nn.conv2d_transpose(%64, %v_param_79, channels=64, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
       "  %66 = nn.bias_add(%65, %v_param_80);\n",
       "  %67 = nn.batch_norm(%66, %v_param_81, %v_param_82, %v_param_83, %v_param_84, epsilon=0.001f);\n",
       "  %68 = %67.0;\n",
       "  %69 = nn.leaky_relu(%68, alpha=0.2f);\n",
       "  %70 = (%69, %28);\n",
       "  %71 = concatenate(%70, axis=1);\n",
       "  %72 = nn.conv2d(%71, %v_param_85, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %73 = nn.bias_add(%72, %v_param_86);\n",
       "  %74 = nn.batch_norm(%73, %v_param_87, %v_param_88, %v_param_89, %v_param_90, epsilon=0.001f);\n",
       "  %75 = %74.0;\n",
       "  %76 = nn.leaky_relu(%75, alpha=0.2f);\n",
       "  %77 = nn.conv2d(%76, %v_param_91, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %78 = nn.bias_add(%77, %v_param_92);\n",
       "  %79 = nn.batch_norm(%78, %v_param_93, %v_param_94, %v_param_95, %v_param_96, epsilon=0.001f);\n",
       "  %80 = %79.0;\n",
       "  %81 = nn.conv2d_transpose(%80, %v_param_97, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
       "  %82 = nn.bias_add(%81, %v_param_98);\n",
       "  %83 = nn.batch_norm(%82, %v_param_99, %v_param_100, %v_param_101, %v_param_102, epsilon=0.001f);\n",
       "  %84 = %83.0;\n",
       "  %85 = nn.leaky_relu(%84, alpha=0.2f);\n",
       "  %86 = (%85, %18);\n",
       "  %87 = concatenate(%86, axis=1);\n",
       "  %88 = nn.conv2d(%87, %v_param_103, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %89 = nn.bias_add(%88, %v_param_104);\n",
       "  %90 = nn.batch_norm(%89, %v_param_105, %v_param_106, %v_param_107, %v_param_108, epsilon=0.001f);\n",
       "  %91 = %90.0;\n",
       "  %92 = nn.leaky_relu(%91, alpha=0.2f);\n",
       "  %93 = nn.conv2d(%92, %v_param_109, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %94 = nn.bias_add(%93, %v_param_110);\n",
       "  %95 = nn.batch_norm(%94, %v_param_111, %v_param_112, %v_param_113, %v_param_114, epsilon=0.001f);\n",
       "  %96 = %95.0;\n",
       "  %97 = nn.conv2d_transpose(%96, %v_param_115, channels=16, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
       "  %98 = nn.bias_add(%97, %v_param_116);\n",
       "  %99 = nn.batch_norm(%98, %v_param_117, %v_param_118, %v_param_119, %v_param_120, epsilon=0.001f);\n",
       "  %100 = %99.0;\n",
       "  %101 = nn.leaky_relu(%100, alpha=0.2f);\n",
       "  %102 = (%101, %8);\n",
       "  %103 = concatenate(%102, axis=1);\n",
       "  %104 = nn.conv2d(%103, %v_param_121, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %105 = nn.bias_add(%104, %v_param_122);\n",
       "  %106 = nn.batch_norm(%105, %v_param_123, %v_param_124, %v_param_125, %v_param_126, epsilon=0.001f);\n",
       "  %107 = %106.0;\n",
       "  %108 = nn.leaky_relu(%107, alpha=0.2f);\n",
       "  %109 = nn.conv2d(%108, %v_param_127, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %110 = nn.bias_add(%109, %v_param_128);\n",
       "  %111 = nn.batch_norm(%110, %v_param_129, %v_param_130, %v_param_131, %v_param_132, epsilon=0.001f);\n",
       "  %112 = %111.0;\n",
       "  %113 = nn.conv2d(%112, %v_param_133, padding=[1, 1, 1, 1], channels=1, kernel_size=[3, 3]);\n",
       "  %114 = nn.bias_add(%113, %v_param_134);\n",
       "  sigmoid(%114)\n",
       "}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib.ir_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
