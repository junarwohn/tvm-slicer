{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 12:44:05.959244: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from faulthandler import disable\n",
    "from unittest import result\n",
    "from SlicingMachine import TVMSlicer\n",
    "import tensorflow as tf\n",
    "import tvm\n",
    "import tvm.relay as relay\n",
    "from tvm.contrib import graph_executor \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pygraphviz as pgv\n",
    "from argparse import ArgumentParser\n",
    "from tvm.relay.build_module import bind_params_by_name\n",
    "from tvm.relay.dataflow_pattern import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetPreProcessCallback(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        super().__init__(rewrite_once=True)\n",
    "\n",
    "        self.var2 = wildcard()\n",
    "        tuple_node = is_tuple([wildcard(), self.var2])\n",
    "        concat_node = is_op('concatenate')(tuple_node)\n",
    "        self.pattern = concat_node\n",
    "        self.match_node = []\n",
    "        self.match_node2 = []\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        var2 = node_map[self.var2][0]\n",
    "        self.match_node.append(var2)\n",
    "        self.match_node2.append(pre)\n",
    "        return post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 12:47:05.211556: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-05 12:47:05.211655: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-07-05 12:47:05.244109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 12:47:05.244464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.665GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-07-05 12:47:05.244482: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-07-05 12:47:05.246252: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-07-05 12:47:05.246292: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-07-05 12:47:05.247029: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-05 12:47:05.247196: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-05 12:47:05.249134: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-05 12:47:05.249588: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-07-05 12:47:05.249624: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-07-05 12:47:05.249705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 12:47:05.250092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 12:47:05.250418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-05 12:47:05.250732: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-05 12:47:05.251102: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-05 12:47:05.251179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 12:47:05.251509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.665GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-07-05 12:47:05.251529: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-07-05 12:47:05.251551: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-07-05 12:47:05.251566: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-07-05 12:47:05.251579: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-05 12:47:05.251592: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-05 12:47:05.251605: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-05 12:47:05.251619: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-07-05 12:47:05.251632: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-07-05 12:47:05.251674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 12:47:05.252015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 12:47:05.252342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-05 12:47:05.252364: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-07-05 12:47:05.682922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-07-05 12:47:05.682949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-07-05 12:47:05.682955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-07-05 12:47:05.683108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 12:47:05.683479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 12:47:05.683811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 12:47:05.684120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9432 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fn (%input_1: Tensor[(1, 3, 512, 512), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(16), float32], %v_param_4: Tensor[(16), float32], %v_param_5: Tensor[(16), float32], %v_param_6: Tensor[(16), float32], %v_param_7: Tensor[(16, 16, 3, 3), float32], %v_param_8: Tensor[(16), float32], %v_param_9: Tensor[(16), float32], %v_param_10: Tensor[(16), float32], %v_param_11: Tensor[(16), float32], %v_param_12: Tensor[(16), float32], %v_param_13: Tensor[(32, 16, 3, 3), float32], %v_param_14: Tensor[(32), float32], %v_param_15: Tensor[(32), float32], %v_param_16: Tensor[(32), float32], %v_param_17: Tensor[(32), float32], %v_param_18: Tensor[(32), float32], %v_param_19: Tensor[(32, 32, 3, 3), float32], %v_param_20: Tensor[(32), float32], %v_param_21: Tensor[(32), float32], %v_param_22: Tensor[(32), float32], %v_param_23: Tensor[(32), float32], %v_param_24: Tensor[(32), float32], %v_param_25: Tensor[(64, 32, 3, 3), float32], %v_param_26: Tensor[(64), float32], %v_param_27: Tensor[(64), float32], %v_param_28: Tensor[(64), float32], %v_param_29: Tensor[(64), float32], %v_param_30: Tensor[(64), float32], %v_param_31: Tensor[(64, 64, 3, 3), float32], %v_param_32: Tensor[(64), float32], %v_param_33: Tensor[(64), float32], %v_param_34: Tensor[(64), float32], %v_param_35: Tensor[(64), float32], %v_param_36: Tensor[(64), float32], %v_param_37: Tensor[(128, 64, 3, 3), float32], %v_param_38: Tensor[(128), float32], %v_param_39: Tensor[(128), float32], %v_param_40: Tensor[(128), float32], %v_param_41: Tensor[(128), float32], %v_param_42: Tensor[(128), float32], %v_param_43: Tensor[(128, 128, 3, 3), float32], %v_param_44: Tensor[(128), float32], %v_param_45: Tensor[(128), float32], %v_param_46: Tensor[(128), float32], %v_param_47: Tensor[(128), float32], %v_param_48: Tensor[(128), float32], %v_param_49: Tensor[(256, 128, 3, 3), float32], %v_param_50: Tensor[(256), float32], %v_param_51: Tensor[(256), float32], %v_param_52: Tensor[(256), float32], %v_param_53: Tensor[(256), float32], %v_param_54: Tensor[(256), float32], %v_param_55: Tensor[(256, 256, 3, 3), float32], %v_param_56: Tensor[(256), float32], %v_param_57: Tensor[(256), float32], %v_param_58: Tensor[(256), float32], %v_param_59: Tensor[(256), float32], %v_param_60: Tensor[(256), float32], %v_param_61: Tensor[(256, 128, 3, 3), float32], %v_param_62: Tensor[(128), float32], %v_param_63: Tensor[(128), float32], %v_param_64: Tensor[(128), float32], %v_param_65: Tensor[(128), float32], %v_param_66: Tensor[(128), float32], %v_param_67: Tensor[(128, 256, 3, 3), float32], %v_param_68: Tensor[(128), float32], %v_param_69: Tensor[(128), float32], %v_param_70: Tensor[(128), float32], %v_param_71: Tensor[(128), float32], %v_param_72: Tensor[(128), float32], %v_param_73: Tensor[(128, 128, 3, 3), float32], %v_param_74: Tensor[(128), float32], %v_param_75: Tensor[(128), float32], %v_param_76: Tensor[(128), float32], %v_param_77: Tensor[(128), float32], %v_param_78: Tensor[(128), float32], %v_param_79: Tensor[(128, 64, 3, 3), float32], %v_param_80: Tensor[(64), float32], %v_param_81: Tensor[(64), float32], %v_param_82: Tensor[(64), float32], %v_param_83: Tensor[(64), float32], %v_param_84: Tensor[(64), float32], %v_param_85: Tensor[(64, 128, 3, 3), float32], %v_param_86: Tensor[(64), float32], %v_param_87: Tensor[(64), float32], %v_param_88: Tensor[(64), float32], %v_param_89: Tensor[(64), float32], %v_param_90: Tensor[(64), float32], %v_param_91: Tensor[(64, 64, 3, 3), float32], %v_param_92: Tensor[(64), float32], %v_param_93: Tensor[(64), float32], %v_param_94: Tensor[(64), float32], %v_param_95: Tensor[(64), float32], %v_param_96: Tensor[(64), float32], %v_param_97: Tensor[(64, 32, 3, 3), float32], %v_param_98: Tensor[(32), float32], %v_param_99: Tensor[(32), float32], %v_param_100: Tensor[(32), float32], %v_param_101: Tensor[(32), float32], %v_param_102: Tensor[(32), float32], %v_param_103: Tensor[(32, 64, 3, 3), float32], %v_param_104: Tensor[(32), float32], %v_param_105: Tensor[(32), float32], %v_param_106: Tensor[(32), float32], %v_param_107: Tensor[(32), float32], %v_param_108: Tensor[(32), float32], %v_param_109: Tensor[(32, 32, 3, 3), float32], %v_param_110: Tensor[(32), float32], %v_param_111: Tensor[(32), float32], %v_param_112: Tensor[(32), float32], %v_param_113: Tensor[(32), float32], %v_param_114: Tensor[(32), float32], %v_param_115: Tensor[(32, 16, 3, 3), float32], %v_param_116: Tensor[(16), float32], %v_param_117: Tensor[(16), float32], %v_param_118: Tensor[(16), float32], %v_param_119: Tensor[(16), float32], %v_param_120: Tensor[(16), float32], %v_param_121: Tensor[(16, 32, 3, 3), float32], %v_param_122: Tensor[(16), float32], %v_param_123: Tensor[(16), float32], %v_param_124: Tensor[(16), float32], %v_param_125: Tensor[(16), float32], %v_param_126: Tensor[(16), float32], %v_param_127: Tensor[(16, 16, 3, 3), float32], %v_param_128: Tensor[(16), float32], %v_param_129: Tensor[(16), float32], %v_param_130: Tensor[(16), float32], %v_param_131: Tensor[(16), float32], %v_param_132: Tensor[(16), float32], %v_param_133: Tensor[(1, 16, 3, 3), float32], %v_param_134: Tensor[(1), float32]) {\n",
       "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %1 = nn.bias_add(%0, %v_param_2);\n",
       "  %2 = nn.batch_norm(%1, %v_param_3, %v_param_4, %v_param_5, %v_param_6, epsilon=0.001f);\n",
       "  %3 = %2.0;\n",
       "  %4 = nn.leaky_relu(%3, alpha=0.2f);\n",
       "  %5 = nn.conv2d(%4, %v_param_7, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %6 = nn.bias_add(%5, %v_param_8);\n",
       "  %7 = nn.batch_norm(%6, %v_param_9, %v_param_10, %v_param_11, %v_param_12, epsilon=0.001f);\n",
       "  %8 = %7.0;\n",
       "  %9 = nn.max_pool2d(%8, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %10 = nn.conv2d(%9, %v_param_13, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %11 = nn.bias_add(%10, %v_param_14);\n",
       "  %12 = nn.batch_norm(%11, %v_param_15, %v_param_16, %v_param_17, %v_param_18, epsilon=0.001f);\n",
       "  %13 = %12.0;\n",
       "  %14 = nn.leaky_relu(%13, alpha=0.2f);\n",
       "  %15 = nn.conv2d(%14, %v_param_19, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %16 = nn.bias_add(%15, %v_param_20);\n",
       "  %17 = nn.batch_norm(%16, %v_param_21, %v_param_22, %v_param_23, %v_param_24, epsilon=0.001f);\n",
       "  %18 = %17.0;\n",
       "  %19 = nn.max_pool2d(%18, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %20 = nn.conv2d(%19, %v_param_25, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %21 = nn.bias_add(%20, %v_param_26);\n",
       "  %22 = nn.batch_norm(%21, %v_param_27, %v_param_28, %v_param_29, %v_param_30, epsilon=0.001f);\n",
       "  %23 = %22.0;\n",
       "  %24 = nn.leaky_relu(%23, alpha=0.2f);\n",
       "  %25 = nn.conv2d(%24, %v_param_31, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %26 = nn.bias_add(%25, %v_param_32);\n",
       "  %27 = nn.batch_norm(%26, %v_param_33, %v_param_34, %v_param_35, %v_param_36, epsilon=0.001f);\n",
       "  %28 = %27.0;\n",
       "  %29 = nn.max_pool2d(%28, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %30 = nn.conv2d(%29, %v_param_37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
       "  %31 = nn.bias_add(%30, %v_param_38);\n",
       "  %32 = nn.batch_norm(%31, %v_param_39, %v_param_40, %v_param_41, %v_param_42, epsilon=0.001f);\n",
       "  %33 = %32.0;\n",
       "  %34 = nn.leaky_relu(%33, alpha=0.2f);\n",
       "  %35 = nn.conv2d(%34, %v_param_43, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
       "  %36 = nn.bias_add(%35, %v_param_44);\n",
       "  %37 = nn.batch_norm(%36, %v_param_45, %v_param_46, %v_param_47, %v_param_48, epsilon=0.001f);\n",
       "  %38 = %37.0;\n",
       "  %39 = nn.max_pool2d(%38, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %40 = nn.conv2d(%39, %v_param_49, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
       "  %41 = nn.bias_add(%40, %v_param_50);\n",
       "  %42 = nn.batch_norm(%41, %v_param_51, %v_param_52, %v_param_53, %v_param_54, epsilon=0.001f);\n",
       "  %43 = %42.0;\n",
       "  %44 = nn.leaky_relu(%43, alpha=0.2f);\n",
       "  %45 = nn.conv2d(%44, %v_param_55, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
       "  %46 = nn.bias_add(%45, %v_param_56);\n",
       "  %47 = nn.batch_norm(%46, %v_param_57, %v_param_58, %v_param_59, %v_param_60, epsilon=0.001f);\n",
       "  %48 = %47.0;\n",
       "  %49 = nn.conv2d_transpose(%48, %v_param_61, channels=128, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
       "  %50 = nn.bias_add(%49, %v_param_62);\n",
       "  %51 = nn.batch_norm(%50, %v_param_63, %v_param_64, %v_param_65, %v_param_66, epsilon=0.001f);\n",
       "  %52 = %51.0;\n",
       "  %53 = nn.leaky_relu(%52, alpha=0.2f);\n",
       "  %54 = (%53, %38);\n",
       "  %55 = concatenate(%54, axis=1);\n",
       "  %56 = nn.conv2d(%55, %v_param_67, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
       "  %57 = nn.bias_add(%56, %v_param_68);\n",
       "  %58 = nn.batch_norm(%57, %v_param_69, %v_param_70, %v_param_71, %v_param_72, epsilon=0.001f);\n",
       "  %59 = %58.0;\n",
       "  %60 = nn.leaky_relu(%59, alpha=0.2f);\n",
       "  %61 = nn.conv2d(%60, %v_param_73, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
       "  %62 = nn.bias_add(%61, %v_param_74);\n",
       "  %63 = nn.batch_norm(%62, %v_param_75, %v_param_76, %v_param_77, %v_param_78, epsilon=0.001f);\n",
       "  %64 = %63.0;\n",
       "  %65 = nn.conv2d_transpose(%64, %v_param_79, channels=64, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
       "  %66 = nn.bias_add(%65, %v_param_80);\n",
       "  %67 = nn.batch_norm(%66, %v_param_81, %v_param_82, %v_param_83, %v_param_84, epsilon=0.001f);\n",
       "  %68 = %67.0;\n",
       "  %69 = nn.leaky_relu(%68, alpha=0.2f);\n",
       "  %70 = (%69, %28);\n",
       "  %71 = concatenate(%70, axis=1);\n",
       "  %72 = nn.conv2d(%71, %v_param_85, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %73 = nn.bias_add(%72, %v_param_86);\n",
       "  %74 = nn.batch_norm(%73, %v_param_87, %v_param_88, %v_param_89, %v_param_90, epsilon=0.001f);\n",
       "  %75 = %74.0;\n",
       "  %76 = nn.leaky_relu(%75, alpha=0.2f);\n",
       "  %77 = nn.conv2d(%76, %v_param_91, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %78 = nn.bias_add(%77, %v_param_92);\n",
       "  %79 = nn.batch_norm(%78, %v_param_93, %v_param_94, %v_param_95, %v_param_96, epsilon=0.001f);\n",
       "  %80 = %79.0;\n",
       "  %81 = nn.conv2d_transpose(%80, %v_param_97, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
       "  %82 = nn.bias_add(%81, %v_param_98);\n",
       "  %83 = nn.batch_norm(%82, %v_param_99, %v_param_100, %v_param_101, %v_param_102, epsilon=0.001f);\n",
       "  %84 = %83.0;\n",
       "  %85 = nn.leaky_relu(%84, alpha=0.2f);\n",
       "  %86 = (%85, %18);\n",
       "  %87 = concatenate(%86, axis=1);\n",
       "  %88 = nn.conv2d(%87, %v_param_103, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %89 = nn.bias_add(%88, %v_param_104);\n",
       "  %90 = nn.batch_norm(%89, %v_param_105, %v_param_106, %v_param_107, %v_param_108, epsilon=0.001f);\n",
       "  %91 = %90.0;\n",
       "  %92 = nn.leaky_relu(%91, alpha=0.2f);\n",
       "  %93 = nn.conv2d(%92, %v_param_109, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %94 = nn.bias_add(%93, %v_param_110);\n",
       "  %95 = nn.batch_norm(%94, %v_param_111, %v_param_112, %v_param_113, %v_param_114, epsilon=0.001f);\n",
       "  %96 = %95.0;\n",
       "  %97 = nn.conv2d_transpose(%96, %v_param_115, channels=16, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
       "  %98 = nn.bias_add(%97, %v_param_116);\n",
       "  %99 = nn.batch_norm(%98, %v_param_117, %v_param_118, %v_param_119, %v_param_120, epsilon=0.001f);\n",
       "  %100 = %99.0;\n",
       "  %101 = nn.leaky_relu(%100, alpha=0.2f);\n",
       "  %102 = (%101, %8);\n",
       "  %103 = concatenate(%102, axis=1);\n",
       "  %104 = nn.conv2d(%103, %v_param_121, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %105 = nn.bias_add(%104, %v_param_122);\n",
       "  %106 = nn.batch_norm(%105, %v_param_123, %v_param_124, %v_param_125, %v_param_126, epsilon=0.001f);\n",
       "  %107 = %106.0;\n",
       "  %108 = nn.leaky_relu(%107, alpha=0.2f);\n",
       "  %109 = nn.conv2d(%108, %v_param_127, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %110 = nn.bias_add(%109, %v_param_128);\n",
       "  %111 = nn.batch_norm(%110, %v_param_129, %v_param_130, %v_param_131, %v_param_132, epsilon=0.001f);\n",
       "  %112 = %111.0;\n",
       "  %113 = nn.conv2d(%112, %v_param_133, padding=[1, 1, 1, 1], channels=1, kernel_size=[3, 3]);\n",
       "  %114 = nn.bias_add(%113, %v_param_134);\n",
       "  sigmoid(%114)\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_file_path = \"./\"\n",
    "\n",
    "np.random.seed(0)\n",
    "img_size = 512\n",
    "input_data = np.random.normal(0,1,(1,img_size,img_size,3)).astype(np.float32)\n",
    "model_keras = tf.keras.models.load_model(current_file_path + '../../tvm-slicer/src/model/{}_{}.h5'.format('unet', img_size))\n",
    "\n",
    "# tvm result\n",
    "input_data = input_data.transpose([0, 3, 1, 2])\n",
    "shape_dict = {\"input_1\": input_data.shape}\n",
    "mod, params = relay.frontend.from_keras(model_keras, shape_dict)\n",
    "target = 'cuda'\n",
    "dev = tvm.cuda()\n",
    "\n",
    "# Inserting quantized layer\n",
    "# upc = UnetPreProcessCallback()\n",
    "# rewrite(upc, mod['main'])\n",
    "\n",
    "# uc = UnetCallback(upc.match_node)\n",
    "# out = rewrite(uc, mod['main'])\n",
    "# out = relay.Function(out.params, relay.Tuple(uc.tmp + [out.body]), out.ret_type, out.type_params, out.attrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetCallback(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, match_node, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        super().__init__(rewrite_once=True)\n",
    "\n",
    "        self.tuple_get_item_node = is_tuple_get_item(wildcard(), 0)\n",
    "        self.pattern_1 = self.tuple_get_item_node\n",
    "\n",
    "        self.pattern = self.pattern_1 \n",
    "        self.match_node = match_node\n",
    "        self.counter = 0\n",
    "        self.tmp = []\n",
    "\n",
    "    def quant(self, node):\n",
    "        cast_to_int8 = relay.cast(\n",
    "            relay.clip(\n",
    "                relay.round(\n",
    "                    relay.multiply(node, relay.const(8.0))\n",
    "                ), \n",
    "                a_min=-127.0, a_max=127.0\n",
    "            ),\n",
    "            dtype=\"int8\"\n",
    "        )\n",
    "        result_node = relay.annotation.stop_fusion(cast_to_int8)\n",
    "        self.tmp.append(result_node)\n",
    "        return result_node\n",
    "\n",
    "    def dequant(self, node):\n",
    "        cast_to_float32 = relay.divide(\n",
    "            relay.cast(node, dtype='float32'), relay.const(8.0)\n",
    "        )\n",
    "        return cast_to_float32\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        if self.pattern_1.match(pre):\n",
    "            if pre in self.match_node:\n",
    "                print(\"pat 1\")\n",
    "                return self.dequant(self.quant(post))\n",
    "        return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UnetCallback2(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, match_node, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        super().__init__(rewrite_once=True)\n",
    "\n",
    "        # self.tuple_get_item_node = is_tuple_get_item(wildcard(), 0)\n",
    "        # self.pattern_1 = self.tuple_get_item_node\n",
    "        self.var2 = wildcard()\n",
    "        tuple_node = is_tuple([wildcard(), self.var2])\n",
    "        concat_node = is_op('concatenate')(tuple_node)\n",
    "        self.pattern = concat_node\n",
    "        # self.pattern = self.pattern_1 \n",
    "        self.match_node = match_node\n",
    "        self.counter = 0\n",
    "        self.tmp = []\n",
    "\n",
    "    def quant(self, node):\n",
    "        cast_to_int8 = relay.cast(\n",
    "            relay.clip(\n",
    "                relay.round(\n",
    "                    relay.multiply(node, relay.const(8.0))\n",
    "                ), \n",
    "                a_min=-127.0, a_max=127.0\n",
    "            ),\n",
    "            dtype=\"int8\"\n",
    "        )\n",
    "        result_node = relay.annotation.stop_fusion(cast_to_int8)\n",
    "        self.tmp.append(result_node)\n",
    "        return result_node\n",
    "\n",
    "    def dequant(self, node):\n",
    "        cast_to_float32 = relay.divide(\n",
    "            relay.cast(node, dtype='float32'), relay.const(8.0)\n",
    "        )\n",
    "        return cast_to_float32\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        if self.pattern.match(pre):\n",
    "            if pre in self.match_node:\n",
    "                print(\"pat 1\")\n",
    "                return self.dequant(self.quant(post))\n",
    "        return post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pat 1\n",
      "pat 1\n",
      "pat 1\n",
      "pat 1\n",
      "pat 1\n",
      "pat 1\n",
      "pat 1\n",
      "pat 1\n"
     ]
    }
   ],
   "source": [
    "upc = UnetPreProcessCallback()\n",
    "rewrite(upc, mod['main'])\n",
    "uc = UnetCallback(upc.match_node)\n",
    "out = rewrite(uc, mod['main'])\n",
    "\n",
    "upc = UnetPreProcessCallback()\n",
    "rewrite(upc, out)\n",
    "uc2 = UnetCallback2(upc.match_node2)\n",
    "out = rewrite(uc2, out)\n",
    "out = relay.Function(out.params, relay.Tuple(uc.tmp + [out.body]), out.ret_type, out.type_params, out.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fn (%input_1: Tensor[(1, 3, 512, 512), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(16), float32], %v_param_4: Tensor[(16), float32], %v_param_5: Tensor[(16), float32], %v_param_6: Tensor[(16), float32], %v_param_7: Tensor[(16, 16, 3, 3), float32], %v_param_8: Tensor[(16), float32], %v_param_9: Tensor[(16), float32], %v_param_10: Tensor[(16), float32], %v_param_11: Tensor[(16), float32], %v_param_12: Tensor[(16), float32], %v_param_13: Tensor[(32, 16, 3, 3), float32], %v_param_14: Tensor[(32), float32], %v_param_15: Tensor[(32), float32], %v_param_16: Tensor[(32), float32], %v_param_17: Tensor[(32), float32], %v_param_18: Tensor[(32), float32], %v_param_19: Tensor[(32, 32, 3, 3), float32], %v_param_20: Tensor[(32), float32], %v_param_21: Tensor[(32), float32], %v_param_22: Tensor[(32), float32], %v_param_23: Tensor[(32), float32], %v_param_24: Tensor[(32), float32], %v_param_25: Tensor[(64, 32, 3, 3), float32], %v_param_26: Tensor[(64), float32], %v_param_27: Tensor[(64), float32], %v_param_28: Tensor[(64), float32], %v_param_29: Tensor[(64), float32], %v_param_30: Tensor[(64), float32], %v_param_31: Tensor[(64, 64, 3, 3), float32], %v_param_32: Tensor[(64), float32], %v_param_33: Tensor[(64), float32], %v_param_34: Tensor[(64), float32], %v_param_35: Tensor[(64), float32], %v_param_36: Tensor[(64), float32], %v_param_37: Tensor[(128, 64, 3, 3), float32], %v_param_38: Tensor[(128), float32], %v_param_39: Tensor[(128), float32], %v_param_40: Tensor[(128), float32], %v_param_41: Tensor[(128), float32], %v_param_42: Tensor[(128), float32], %v_param_43: Tensor[(128, 128, 3, 3), float32], %v_param_44: Tensor[(128), float32], %v_param_45: Tensor[(128), float32], %v_param_46: Tensor[(128), float32], %v_param_47: Tensor[(128), float32], %v_param_48: Tensor[(128), float32], %v_param_49: Tensor[(256, 128, 3, 3), float32], %v_param_50: Tensor[(256), float32], %v_param_51: Tensor[(256), float32], %v_param_52: Tensor[(256), float32], %v_param_53: Tensor[(256), float32], %v_param_54: Tensor[(256), float32], %v_param_55: Tensor[(256, 256, 3, 3), float32], %v_param_56: Tensor[(256), float32], %v_param_57: Tensor[(256), float32], %v_param_58: Tensor[(256), float32], %v_param_59: Tensor[(256), float32], %v_param_60: Tensor[(256), float32], %v_param_61: Tensor[(256, 128, 3, 3), float32], %v_param_62: Tensor[(128), float32], %v_param_63: Tensor[(128), float32], %v_param_64: Tensor[(128), float32], %v_param_65: Tensor[(128), float32], %v_param_66: Tensor[(128), float32], %v_param_67: Tensor[(128, 256, 3, 3), float32], %v_param_68: Tensor[(128), float32], %v_param_69: Tensor[(128), float32], %v_param_70: Tensor[(128), float32], %v_param_71: Tensor[(128), float32], %v_param_72: Tensor[(128), float32], %v_param_73: Tensor[(128, 128, 3, 3), float32], %v_param_74: Tensor[(128), float32], %v_param_75: Tensor[(128), float32], %v_param_76: Tensor[(128), float32], %v_param_77: Tensor[(128), float32], %v_param_78: Tensor[(128), float32], %v_param_79: Tensor[(128, 64, 3, 3), float32], %v_param_80: Tensor[(64), float32], %v_param_81: Tensor[(64), float32], %v_param_82: Tensor[(64), float32], %v_param_83: Tensor[(64), float32], %v_param_84: Tensor[(64), float32], %v_param_85: Tensor[(64, 128, 3, 3), float32], %v_param_86: Tensor[(64), float32], %v_param_87: Tensor[(64), float32], %v_param_88: Tensor[(64), float32], %v_param_89: Tensor[(64), float32], %v_param_90: Tensor[(64), float32], %v_param_91: Tensor[(64, 64, 3, 3), float32], %v_param_92: Tensor[(64), float32], %v_param_93: Tensor[(64), float32], %v_param_94: Tensor[(64), float32], %v_param_95: Tensor[(64), float32], %v_param_96: Tensor[(64), float32], %v_param_97: Tensor[(64, 32, 3, 3), float32], %v_param_98: Tensor[(32), float32], %v_param_99: Tensor[(32), float32], %v_param_100: Tensor[(32), float32], %v_param_101: Tensor[(32), float32], %v_param_102: Tensor[(32), float32], %v_param_103: Tensor[(32, 64, 3, 3), float32], %v_param_104: Tensor[(32), float32], %v_param_105: Tensor[(32), float32], %v_param_106: Tensor[(32), float32], %v_param_107: Tensor[(32), float32], %v_param_108: Tensor[(32), float32], %v_param_109: Tensor[(32, 32, 3, 3), float32], %v_param_110: Tensor[(32), float32], %v_param_111: Tensor[(32), float32], %v_param_112: Tensor[(32), float32], %v_param_113: Tensor[(32), float32], %v_param_114: Tensor[(32), float32], %v_param_115: Tensor[(32, 16, 3, 3), float32], %v_param_116: Tensor[(16), float32], %v_param_117: Tensor[(16), float32], %v_param_118: Tensor[(16), float32], %v_param_119: Tensor[(16), float32], %v_param_120: Tensor[(16), float32], %v_param_121: Tensor[(16, 32, 3, 3), float32], %v_param_122: Tensor[(16), float32], %v_param_123: Tensor[(16), float32], %v_param_124: Tensor[(16), float32], %v_param_125: Tensor[(16), float32], %v_param_126: Tensor[(16), float32], %v_param_127: Tensor[(16, 16, 3, 3), float32], %v_param_128: Tensor[(16), float32], %v_param_129: Tensor[(16), float32], %v_param_130: Tensor[(16), float32], %v_param_131: Tensor[(16), float32], %v_param_132: Tensor[(16), float32], %v_param_133: Tensor[(1, 16, 3, 3), float32], %v_param_134: Tensor[(1), float32]) {\n",
       "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %1 = nn.bias_add(%0, %v_param_2);\n",
       "  %2 = nn.batch_norm(%1, %v_param_3, %v_param_4, %v_param_5, %v_param_6, epsilon=0.001f);\n",
       "  %3 = %2.0;\n",
       "  %4 = nn.leaky_relu(%3, alpha=0.2f);\n",
       "  %5 = nn.conv2d(%4, %v_param_7, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %6 = nn.bias_add(%5, %v_param_8);\n",
       "  %7 = nn.batch_norm(%6, %v_param_9, %v_param_10, %v_param_11, %v_param_12, epsilon=0.001f);\n",
       "  %8 = %7.0;\n",
       "  %9 = multiply(%8, 8f);\n",
       "  %10 = round(%9);\n",
       "  %11 = clip(%10, a_min=-127f, a_max=127f);\n",
       "  %12 = cast(%11, dtype=\"int8\");\n",
       "  %13 = annotation.stop_fusion(%12);\n",
       "  %14 = cast(%13, dtype=\"float32\");\n",
       "  %15 = divide(%14, 8f);\n",
       "  %16 = nn.max_pool2d(%15, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %17 = nn.conv2d(%16, %v_param_13, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %18 = nn.bias_add(%17, %v_param_14);\n",
       "  %19 = nn.batch_norm(%18, %v_param_15, %v_param_16, %v_param_17, %v_param_18, epsilon=0.001f);\n",
       "  %20 = %19.0;\n",
       "  %21 = nn.leaky_relu(%20, alpha=0.2f);\n",
       "  %22 = nn.conv2d(%21, %v_param_19, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %23 = nn.bias_add(%22, %v_param_20);\n",
       "  %24 = nn.batch_norm(%23, %v_param_21, %v_param_22, %v_param_23, %v_param_24, epsilon=0.001f);\n",
       "  %25 = %24.0;\n",
       "  %26 = multiply(%25, 8f);\n",
       "  %27 = round(%26);\n",
       "  %28 = clip(%27, a_min=-127f, a_max=127f);\n",
       "  %29 = cast(%28, dtype=\"int8\");\n",
       "  %30 = annotation.stop_fusion(%29);\n",
       "  %31 = cast(%30, dtype=\"float32\");\n",
       "  %32 = divide(%31, 8f);\n",
       "  %33 = nn.max_pool2d(%32, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %34 = nn.conv2d(%33, %v_param_25, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %35 = nn.bias_add(%34, %v_param_26);\n",
       "  %36 = nn.batch_norm(%35, %v_param_27, %v_param_28, %v_param_29, %v_param_30, epsilon=0.001f);\n",
       "  %37 = %36.0;\n",
       "  %38 = nn.leaky_relu(%37, alpha=0.2f);\n",
       "  %39 = nn.conv2d(%38, %v_param_31, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %40 = nn.bias_add(%39, %v_param_32);\n",
       "  %41 = nn.batch_norm(%40, %v_param_33, %v_param_34, %v_param_35, %v_param_36, epsilon=0.001f);\n",
       "  %42 = %41.0;\n",
       "  %43 = multiply(%42, 8f);\n",
       "  %44 = round(%43);\n",
       "  %45 = clip(%44, a_min=-127f, a_max=127f);\n",
       "  %46 = cast(%45, dtype=\"int8\");\n",
       "  %47 = annotation.stop_fusion(%46);\n",
       "  %48 = cast(%47, dtype=\"float32\");\n",
       "  %49 = divide(%48, 8f);\n",
       "  %50 = nn.max_pool2d(%49, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %51 = nn.conv2d(%50, %v_param_37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
       "  %52 = nn.bias_add(%51, %v_param_38);\n",
       "  %53 = nn.batch_norm(%52, %v_param_39, %v_param_40, %v_param_41, %v_param_42, epsilon=0.001f);\n",
       "  %54 = %53.0;\n",
       "  %55 = nn.leaky_relu(%54, alpha=0.2f);\n",
       "  %56 = nn.conv2d(%55, %v_param_43, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
       "  %57 = nn.bias_add(%56, %v_param_44);\n",
       "  %58 = nn.batch_norm(%57, %v_param_45, %v_param_46, %v_param_47, %v_param_48, epsilon=0.001f);\n",
       "  %59 = %58.0;\n",
       "  %60 = multiply(%59, 8f);\n",
       "  %61 = round(%60);\n",
       "  %62 = clip(%61, a_min=-127f, a_max=127f);\n",
       "  %63 = cast(%62, dtype=\"int8\");\n",
       "  %64 = annotation.stop_fusion(%63);\n",
       "  %65 = cast(%64, dtype=\"float32\");\n",
       "  %66 = divide(%65, 8f);\n",
       "  %67 = nn.max_pool2d(%66, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
       "  %68 = nn.conv2d(%67, %v_param_49, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
       "  %69 = nn.bias_add(%68, %v_param_50);\n",
       "  %70 = nn.batch_norm(%69, %v_param_51, %v_param_52, %v_param_53, %v_param_54, epsilon=0.001f);\n",
       "  %71 = %70.0;\n",
       "  %72 = nn.leaky_relu(%71, alpha=0.2f);\n",
       "  %73 = nn.conv2d(%72, %v_param_55, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
       "  %74 = nn.bias_add(%73, %v_param_56);\n",
       "  %75 = nn.batch_norm(%74, %v_param_57, %v_param_58, %v_param_59, %v_param_60, epsilon=0.001f);\n",
       "  %76 = %75.0;\n",
       "  %77 = nn.conv2d_transpose(%76, %v_param_61, channels=128, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
       "  %78 = nn.bias_add(%77, %v_param_62);\n",
       "  %79 = nn.batch_norm(%78, %v_param_63, %v_param_64, %v_param_65, %v_param_66, epsilon=0.001f);\n",
       "  %80 = %79.0;\n",
       "  %81 = nn.leaky_relu(%80, alpha=0.2f);\n",
       "  %82 = (%81, %66);\n",
       "  %83 = concatenate(%82, axis=1);\n",
       "  %84 = multiply(%83, 8f);\n",
       "  %85 = round(%84);\n",
       "  %86 = clip(%85, a_min=-127f, a_max=127f);\n",
       "  %87 = cast(%86, dtype=\"int8\");\n",
       "  %88 = annotation.stop_fusion(%87);\n",
       "  %89 = cast(%88, dtype=\"float32\");\n",
       "  %90 = divide(%89, 8f);\n",
       "  %91 = nn.conv2d(%90, %v_param_67, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
       "  %92 = nn.bias_add(%91, %v_param_68);\n",
       "  %93 = nn.batch_norm(%92, %v_param_69, %v_param_70, %v_param_71, %v_param_72, epsilon=0.001f);\n",
       "  %94 = %93.0;\n",
       "  %95 = nn.leaky_relu(%94, alpha=0.2f);\n",
       "  %96 = nn.conv2d(%95, %v_param_73, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
       "  %97 = nn.bias_add(%96, %v_param_74);\n",
       "  %98 = nn.batch_norm(%97, %v_param_75, %v_param_76, %v_param_77, %v_param_78, epsilon=0.001f);\n",
       "  %99 = %98.0;\n",
       "  %100 = nn.conv2d_transpose(%99, %v_param_79, channels=64, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
       "  %101 = nn.bias_add(%100, %v_param_80);\n",
       "  %102 = nn.batch_norm(%101, %v_param_81, %v_param_82, %v_param_83, %v_param_84, epsilon=0.001f);\n",
       "  %103 = %102.0;\n",
       "  %104 = nn.leaky_relu(%103, alpha=0.2f);\n",
       "  %105 = (%104, %49);\n",
       "  %106 = concatenate(%105, axis=1);\n",
       "  %107 = multiply(%106, 8f);\n",
       "  %108 = round(%107);\n",
       "  %109 = clip(%108, a_min=-127f, a_max=127f);\n",
       "  %110 = cast(%109, dtype=\"int8\");\n",
       "  %111 = annotation.stop_fusion(%110);\n",
       "  %112 = cast(%111, dtype=\"float32\");\n",
       "  %113 = divide(%112, 8f);\n",
       "  %114 = nn.conv2d(%113, %v_param_85, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %115 = nn.bias_add(%114, %v_param_86);\n",
       "  %116 = nn.batch_norm(%115, %v_param_87, %v_param_88, %v_param_89, %v_param_90, epsilon=0.001f);\n",
       "  %117 = %116.0;\n",
       "  %118 = nn.leaky_relu(%117, alpha=0.2f);\n",
       "  %119 = nn.conv2d(%118, %v_param_91, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
       "  %120 = nn.bias_add(%119, %v_param_92);\n",
       "  %121 = nn.batch_norm(%120, %v_param_93, %v_param_94, %v_param_95, %v_param_96, epsilon=0.001f);\n",
       "  %122 = %121.0;\n",
       "  %123 = nn.conv2d_transpose(%122, %v_param_97, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
       "  %124 = nn.bias_add(%123, %v_param_98);\n",
       "  %125 = nn.batch_norm(%124, %v_param_99, %v_param_100, %v_param_101, %v_param_102, epsilon=0.001f);\n",
       "  %126 = %125.0;\n",
       "  %127 = nn.leaky_relu(%126, alpha=0.2f);\n",
       "  %128 = (%127, %32);\n",
       "  %129 = concatenate(%128, axis=1);\n",
       "  %130 = multiply(%129, 8f);\n",
       "  %131 = round(%130);\n",
       "  %132 = clip(%131, a_min=-127f, a_max=127f);\n",
       "  %133 = cast(%132, dtype=\"int8\");\n",
       "  %134 = annotation.stop_fusion(%133);\n",
       "  %135 = cast(%134, dtype=\"float32\");\n",
       "  %136 = divide(%135, 8f);\n",
       "  %137 = nn.conv2d(%136, %v_param_103, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %138 = nn.bias_add(%137, %v_param_104);\n",
       "  %139 = nn.batch_norm(%138, %v_param_105, %v_param_106, %v_param_107, %v_param_108, epsilon=0.001f);\n",
       "  %140 = %139.0;\n",
       "  %141 = nn.leaky_relu(%140, alpha=0.2f);\n",
       "  %142 = nn.conv2d(%141, %v_param_109, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
       "  %143 = nn.bias_add(%142, %v_param_110);\n",
       "  %144 = nn.batch_norm(%143, %v_param_111, %v_param_112, %v_param_113, %v_param_114, epsilon=0.001f);\n",
       "  %145 = %144.0;\n",
       "  %146 = nn.conv2d_transpose(%145, %v_param_115, channels=16, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
       "  %147 = nn.bias_add(%146, %v_param_116);\n",
       "  %148 = nn.batch_norm(%147, %v_param_117, %v_param_118, %v_param_119, %v_param_120, epsilon=0.001f);\n",
       "  %149 = %148.0;\n",
       "  %150 = nn.leaky_relu(%149, alpha=0.2f);\n",
       "  %151 = (%150, %15);\n",
       "  %152 = concatenate(%151, axis=1);\n",
       "  %153 = multiply(%152, 8f);\n",
       "  %154 = round(%153);\n",
       "  %155 = clip(%154, a_min=-127f, a_max=127f);\n",
       "  %156 = cast(%155, dtype=\"int8\");\n",
       "  %157 = annotation.stop_fusion(%156);\n",
       "  %158 = cast(%157, dtype=\"float32\");\n",
       "  %159 = divide(%158, 8f);\n",
       "  %160 = nn.conv2d(%159, %v_param_121, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %161 = nn.bias_add(%160, %v_param_122);\n",
       "  %162 = nn.batch_norm(%161, %v_param_123, %v_param_124, %v_param_125, %v_param_126, epsilon=0.001f);\n",
       "  %163 = %162.0;\n",
       "  %164 = nn.leaky_relu(%163, alpha=0.2f);\n",
       "  %165 = nn.conv2d(%164, %v_param_127, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
       "  %166 = nn.bias_add(%165, %v_param_128);\n",
       "  %167 = nn.batch_norm(%166, %v_param_129, %v_param_130, %v_param_131, %v_param_132, epsilon=0.001f);\n",
       "  %168 = %167.0;\n",
       "  %169 = nn.conv2d(%168, %v_param_133, padding=[1, 1, 1, 1], channels=1, kernel_size=[3, 3]);\n",
       "  %170 = nn.bias_add(%169, %v_param_134);\n",
       "  %171 = sigmoid(%170);\n",
       "  (%13, %30, %47, %64, %171)\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(out, target, params=params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygraphviz as pgv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph(json_data, file_name=None):\n",
    "    if type(json_data) == str:\n",
    "        json_data = json.loads(json_data)\n",
    "    A = pgv.AGraph(directed=True)\n",
    "    for node_idx, node in enumerate(json_data['nodes']):\n",
    "        for src in node['inputs']:\n",
    "            if 0 == 1:\n",
    "                print(src[0], json_data['attrs']['dltype'][1][src[0]], json_data['attrs']['shape'][1][src[0]])\n",
    "                A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]), node['name'] + '[{}]'.format(node_idx) + '{}'.format(json_data['attrs']['dltype'][1][node_idx]))\n",
    "            else:\n",
    "                A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]), node['name'] + '[{}]'.format(node_idx) + '{}'.format(json_data['attrs']['dltype'][1][node_idx]))\n",
    "    if file_name:\n",
    "        A.draw(file_name + '.png', format='png', prog='dot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_json_raw = lib['get_graph_json']()\n",
    "\n",
    "show_graph(graph_json_raw, \"./test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Initial models\n",
      "model_nodes [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91]\n",
      "complement_nodes [-1]\n",
      "##############################\n",
      "[ 92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119]\n",
      "######################################\n",
      "After input analyze\n",
      "intermediate_nodes []\n",
      "input_dependency defaultdict(<class 'list'>, {})\n",
      "model_nodes [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91]\n",
      "######################################\n",
      "######################################\n",
      "After output analyze\n",
      "output_dependency defaultdict(<class 'list'>, {91: [92, 92], 9: [107]})\n",
      "model_nodes [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91]\n",
      "######################################\n",
      "input_dependency.keys() dict_keys([])\n",
      "model_nodes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]\n",
      "output_dependency defaultdict(<class 'list'>, {91: [92, 92], 9: [107]})\n",
      "output_nodes [9, 91]\n",
      "{'input_1': 0, 'p0': 1, 'p1': 2, 'p2': 3, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu': 4, 'p3': 5, 'p4': 6, 'p5': 7, 'p6': 8, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast': 9, 'tvmgen_default_fused_cast_divide': 10, 'tvmgen_default_fused_nn_max_pool2d': 11, 'p7': 12, 'p8': 13, 'p9': 14, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_1': 15, 'p10': 16, 'p11': 17, 'p12': 18, 'p13': 19, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast_1': 20, 'tvmgen_default_fused_cast_divide_1': 21, 'tvmgen_default_fused_nn_max_pool2d_1': 22, 'p14': 23, 'p15': 24, 'p16': 25, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_2': 26, 'p17': 27, 'p18': 28, 'p19': 29, 'p20': 30, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast_2': 31, 'tvmgen_default_fused_cast_divide_2': 32, 'tvmgen_default_fused_nn_max_pool2d_2': 33, 'p21': 34, 'p22': 35, 'p23': 36, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_3': 37, 'p24': 38, 'p25': 39, 'p26': 40, 'p27': 41, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast_3': 42, 'tvmgen_default_fused_cast_divide_3': 43, 'tvmgen_default_fused_nn_max_pool2d_3': 44, 'p28': 45, 'p29': 46, 'p30': 47, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_4': 48, 'p31': 49, 'p32': 50, 'p33': 51, 'p34': 52, 'tvmgen_default_fused_nn_conv2d_add_multiply_add': 53, 'p35': 54, 'p36': 55, 'p37': 56, 'p38': 57, 'tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu': 58, 'tvmgen_default_fused_concatenate_multiply_round_clip_cast': 59, 'tvmgen_default_fused_cast_divide_4': 60, 'p39': 61, 'p40': 62, 'p41': 63, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_5': 64, 'p42': 65, 'p43': 66, 'p44': 67, 'p45': 68, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_1': 69, 'p46': 70, 'p47': 71, 'p48': 72, 'p49': 73, 'tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_1': 74, 'tvmgen_default_fused_concatenate_multiply_round_clip_cast_1': 75, 'tvmgen_default_fused_cast_divide_5': 76, 'p50': 77, 'p51': 78, 'p52': 79, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_6': 80, 'p53': 81, 'p54': 82, 'p55': 83, 'p56': 84, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_2': 85, 'p57': 86, 'p58': 87, 'p59': 88, 'p60': 89, 'tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_2': 90, 'tvmgen_default_fused_concatenate_multiply_round_clip_cast_2': 91}\n",
      "====================\n",
      "[ 9 91]\n",
      "##############################\n",
      "Initial models\n",
      "model_nodes [ 92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119]\n",
      "complement_nodes [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91]\n",
      "##############################\n",
      "[]\n",
      "######################################\n",
      "After input analyze\n",
      "intermediate_nodes [10]\n",
      "input_dependency defaultdict(<class 'list'>, {91: [92, 92], 9: [10]})\n",
      "model_nodes [ 92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119]\n",
      "######################################\n",
      "######################################\n",
      "After output analyze\n",
      "output_dependency defaultdict(<class 'list'>, {})\n",
      "model_nodes [ 92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119]\n",
      "######################################\n",
      "input_dependency.keys() dict_keys([91, 9])\n",
      "model_nodes [9, 91, 10, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "output_dependency defaultdict(<class 'list'>, {})\n",
      "output_nodes [119]\n",
      "{'input_9': 0, 'input_91': 1, 'tvmgen_default_fused_cast_divide': 2, 'tvmgen_default_fused_cast_divide_6': 3, 'p61': 4, 'p62': 5, 'p63': 6, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_7': 7, 'p64': 8, 'p65': 9, 'p66': 10, 'p67': 11, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_3': 12, 'p68': 13, 'p69': 14, 'p70': 15, 'p71': 16, 'tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_3': 17, 'tvmgen_default_fused_concatenate_multiply_round_clip_cast_3': 18, 'tvmgen_default_fused_cast_divide_7': 19, 'p72': 20, 'p73': 21, 'p74': 22, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_8': 23, 'p75': 24, 'p76': 25, 'p77': 26, 'tvmgen_default_fused_nn_conv2d_add_add': 27, 'p78': 28, 'p79': 29, 'tvmgen_default_fused_nn_conv2d_add_sigmoid': 30}\n",
      "input_9\n",
      "input_91\n",
      "====================\n",
      "[119]\n"
     ]
    }
   ],
   "source": [
    "tvm_slicer = TVMSlicer(graph_json_raw)\n",
    "graph_json_front, _, _ = tvm_slicer.slice_graph(0, 91, is_quantize_sliced=True)\n",
    "graph_json_back, _, _ = tvm_slicer.slice_graph(91 + 1, 119, is_quantize_sliced=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(graph_json_front, \"./front\")\n",
    "show_graph(graph_json_back, \"./back\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'op': 'null',\n",
       "   'name': 'input_9',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '5',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'OIHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': 'c35c41a655f8a1a2'},\n",
       "   'inputs': []},\n",
       "  {'op': 'null',\n",
       "   'name': 'input_91',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '2',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_concatenate_multiply_round_clip_cast_2',\n",
       "    'hash': '9e940466005aa1f3'},\n",
       "   'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_cast_divide',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '1',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_cast_divide',\n",
       "    'hash': '85165d63e18469f5'},\n",
       "   'inputs': [[0, 0, 0]]},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_cast_divide_6',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '1',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_cast_divide_6',\n",
       "    'hash': 'fa27adaace0bfe80'},\n",
       "   'inputs': [[1, 0, 0]]},\n",
       "  {'op': 'null', 'name': 'p61', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p62', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p63', 'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_7',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '4',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_7',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'OIHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': 'f81b264d19b9a6be'},\n",
       "   'inputs': [[3, 0, 0], [4, 0, 0], [5, 0, 0], [6, 0, 0]]},\n",
       "  {'op': 'null', 'name': 'p64', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p65', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p66', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p67', 'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_nn_conv2d_add_multiply_add_3',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '5',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_add_multiply_add_3',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'OIHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': 'afcc3e3243b199b7'},\n",
       "   'inputs': [[7, 0, 0], [8, 0, 0], [9, 0, 0], [10, 0, 0], [11, 0, 0]]},\n",
       "  {'op': 'null', 'name': 'p68', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p69', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p70', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p71', 'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_3',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '5',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_3',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'IOHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': 'e2fccacfae4878e3'},\n",
       "   'inputs': [[12, 0, 0], [13, 0, 0], [14, 0, 0], [15, 0, 0], [16, 0, 0]]},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_concatenate_multiply_round_clip_cast_3',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '2',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_concatenate_multiply_round_clip_cast_3',\n",
       "    'hash': '9c8a637413505916'},\n",
       "   'inputs': [[17, 0, 0], [2, 0, 0]]},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_cast_divide_7',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '1',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_cast_divide_7',\n",
       "    'hash': '186333aa7e215879'},\n",
       "   'inputs': [[18, 0, 0]]},\n",
       "  {'op': 'null', 'name': 'p72', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p73', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p74', 'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_8',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '4',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_8',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'OIHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': '0258e262c093a80d'},\n",
       "   'inputs': [[19, 0, 0], [20, 0, 0], [21, 0, 0], [22, 0, 0]]},\n",
       "  {'op': 'null', 'name': 'p75', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p76', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p77', 'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_nn_conv2d_add_add',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '4',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_add_add',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'OIHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': 'b83cfcda2b3569ee'},\n",
       "   'inputs': [[23, 0, 0], [24, 0, 0], [25, 0, 0], [26, 0, 0]]},\n",
       "  {'op': 'null', 'name': 'p78', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p79', 'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_nn_conv2d_add_sigmoid',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '3',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_add_sigmoid',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'OIHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': '61cdf1c6242552b1'},\n",
       "   'inputs': [[27, 0, 0], [28, 0, 0], [29, 0, 0]]}],\n",
       " 'arg_nodes': [0,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  28,\n",
       "  29],\n",
       " 'heads': [[30, 0, 0]],\n",
       " 'attrs': {'dltype': ['list_str',\n",
       "   ['int8',\n",
       "    'int8',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'int8',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32']],\n",
       "  'device_index': ['list_int',\n",
       "   [2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2]],\n",
       "  'storage_id': ['list_int',\n",
       "   [9,\n",
       "    41,\n",
       "    4,\n",
       "    14,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    23,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    14,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    23,\n",
       "    14,\n",
       "    4,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    23,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    14,\n",
       "    89,\n",
       "    90,\n",
       "    37]],\n",
       "  'shape': ['list_shape',\n",
       "   [[1, 16, 512, 512],\n",
       "    [1, 64, 256, 256],\n",
       "    [1, 16, 512, 512],\n",
       "    [1, 64, 256, 256],\n",
       "    [32, 64, 3, 3],\n",
       "    [1, 32, 1, 1],\n",
       "    [1, 32, 1, 1],\n",
       "    [1, 32, 256, 256],\n",
       "    [32, 32, 3, 3],\n",
       "    [1, 32, 1, 1],\n",
       "    [1, 32, 1, 1],\n",
       "    [1, 32, 1, 1],\n",
       "    [1, 32, 256, 256],\n",
       "    [32, 16, 3, 3],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 512, 512],\n",
       "    [1, 32, 512, 512],\n",
       "    [1, 32, 512, 512],\n",
       "    [16, 32, 3, 3],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 512, 512],\n",
       "    [16, 16, 3, 3],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 512, 512],\n",
       "    [1, 16, 3, 3],\n",
       "    [1, 1, 1],\n",
       "    [1, 1, 512, 512]]]},\n",
       " 'node_row_ptr': [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_json_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
