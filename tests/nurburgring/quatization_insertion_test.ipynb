{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 18:53:22.456795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from faulthandler import disable\n",
    "from unittest import result\n",
    "from SlicingMachine import TVMSlicer\n",
    "import tensorflow as tf\n",
    "import tvm\n",
    "import tvm.relay as relay\n",
    "from tvm.contrib import graph_executor \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pygraphviz as pgv\n",
    "from argparse import ArgumentParser\n",
    "from tvm.relay.build_module import bind_params_by_name\n",
    "from tvm.relay.dataflow_pattern import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 18:53:24.826060: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-07 18:53:24.826154: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-07-07 18:53:24.850173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-07 18:53:24.850519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.665GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-07-07 18:53:24.850537: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-07-07 18:53:24.852284: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-07-07 18:53:24.852321: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-07-07 18:53:24.853050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-07 18:53:24.853213: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-07 18:53:24.855093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-07 18:53:24.855519: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-07-07 18:53:24.855550: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-07-07 18:53:24.855624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-07 18:53:24.856004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-07 18:53:24.856322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-07 18:53:24.856712: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-07 18:53:24.857034: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-07 18:53:24.857160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-07 18:53:24.857644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.665GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-07-07 18:53:24.857676: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-07-07 18:53:24.857705: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-07-07 18:53:24.857717: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-07-07 18:53:24.857728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-07 18:53:24.857739: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-07 18:53:24.857750: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-07 18:53:24.857762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-07-07 18:53:24.857773: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-07-07 18:53:24.857834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-07 18:53:24.858197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-07 18:53:24.858505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-07 18:53:24.858534: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-07-07 18:53:25.275823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-07-07 18:53:25.275847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-07-07 18:53:25.275853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-07-07 18:53:25.276009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-07 18:53:25.276369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-07 18:53:25.276691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-07 18:53:25.276993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8800 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "current_file_path = \"./\"\n",
    "\n",
    "np.random.seed(0)\n",
    "img_size = 512\n",
    "input_data = np.random.normal(0,1,(1,img_size,img_size,3)).astype(np.float32)\n",
    "model_keras = tf.keras.models.load_model(current_file_path + '../../tvm-slicer/src/model/{}_{}.h5'.format('unet', img_size))\n",
    "\n",
    "# tvm result\n",
    "input_data = input_data.transpose([0, 3, 1, 2])\n",
    "shape_dict = {\"input_1\": input_data.shape}\n",
    "mod, params = relay.frontend.from_keras(model_keras, shape_dict)\n",
    "target = 'cuda'\n",
    "dev = tvm.cuda()\n",
    "\n",
    "# Inserting quantized layer\n",
    "# upc = UnetPreProcessCallback()\n",
    "# rewrite(upc, mod['main'])\n",
    "\n",
    "# uc = UnetCallback(upc.match_node)\n",
    "# out = rewrite(uc, mod['main'])\n",
    "# out = relay.Function(out.params, relay.Tuple(uc.tmp + [out.body]), out.ret_type, out.type_params, out.attrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetPreProcessCallback(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        super().__init__(rewrite_once=True)\n",
    "\n",
    "        self.var2 = wildcard()\n",
    "        tuple_node = is_tuple([wildcard(), self.var2])\n",
    "        concat_node = is_op('concatenate')(tuple_node)\n",
    "        self.pattern = concat_node\n",
    "        self.match_node = []\n",
    "        self.match_node2 = []\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        var2 = node_map[self.var2][0]\n",
    "        self.match_node.append(var2)\n",
    "        self.match_node2.append(pre)\n",
    "        return post\n",
    "        \n",
    "class UnetCallback(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, match_node, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        super().__init__(rewrite_once=True)\n",
    "\n",
    "        self.tuple_get_item_node = is_tuple_get_item(wildcard(), 0)\n",
    "        self.pattern_1 = self.tuple_get_item_node\n",
    "\n",
    "        self.pattern = self.pattern_1 \n",
    "        self.match_node = match_node\n",
    "        self.counter = 0\n",
    "        self.tmp = []\n",
    "\n",
    "    def quant(self, node):\n",
    "        cast_to_int8 = relay.cast(\n",
    "            relay.clip(\n",
    "                relay.round(\n",
    "                    relay.multiply(node, relay.const(8.0))\n",
    "                ), \n",
    "                a_min=-127.0, a_max=127.0\n",
    "            ),\n",
    "            dtype=\"int8\"\n",
    "        )\n",
    "        result_node = relay.annotation.stop_fusion(cast_to_int8)\n",
    "        self.tmp.append(result_node)\n",
    "        return result_node\n",
    "\n",
    "    def dequant(self, node):\n",
    "        cast_to_float32 = relay.divide(\n",
    "            relay.cast(node, dtype='float32'), relay.const(8.0)\n",
    "        )\n",
    "        return cast_to_float32\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        if self.pattern_1.match(pre):\n",
    "            if pre in self.match_node:\n",
    "                print(\"pat 1\")\n",
    "                return self.dequant(self.quant(post))\n",
    "        return post\n",
    "\n",
    "class UnetCallback2(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, match_node, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        super().__init__(rewrite_once=True)\n",
    "\n",
    "        # self.tuple_get_item_node = is_tuple_get_item(wildcard(), 0)\n",
    "        # self.pattern_1 = self.tuple_get_item_node\n",
    "        self.var2 = wildcard()\n",
    "        tuple_node = is_tuple([wildcard(), self.var2])\n",
    "        concat_node = is_op('concatenate')(tuple_node)\n",
    "        self.pattern = concat_node\n",
    "        # self.pattern = self.pattern_1 \n",
    "        self.match_node = match_node\n",
    "        self.counter = 0\n",
    "        self.tmp = []\n",
    "\n",
    "    def quant(self, node):\n",
    "        cast_to_int8 = relay.cast(\n",
    "            relay.clip(\n",
    "                relay.round(\n",
    "                    relay.multiply(node, relay.const(8.0))\n",
    "                ), \n",
    "                a_min=-127.0, a_max=127.0\n",
    "            ),\n",
    "            dtype=\"int8\"\n",
    "        )\n",
    "        result_node = relay.annotation.stop_fusion(cast_to_int8)\n",
    "        self.tmp.append(result_node)\n",
    "        return result_node\n",
    "\n",
    "    def dequant(self, node):\n",
    "        cast_to_float32 = relay.divide(\n",
    "            relay.cast(node, dtype='float32'), relay.const(8.0)\n",
    "        )\n",
    "        return cast_to_float32\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        if self.pattern.match(pre):\n",
    "            if pre in self.match_node:\n",
    "                print(\"pat 1\")\n",
    "                return self.dequant(self.quant(post))\n",
    "        return post\n",
    "\n",
    "\n",
    "class UnetMaxPool2dCallback(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        super().__init__(rewrite_once=True)\n",
    "\n",
    "        max_pool2d_node = is_op('nn.max_pool2d')(wildcard())\n",
    "        self.pattern = max_pool2d_node\n",
    "        self.match_node = []\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        self.match_node.append(pre)\n",
    "        return post\n",
    "\n",
    "\n",
    "class UnetCallback3(DFPatternCallback):\n",
    "    # A callback class to rewrite the matched pattern to a batch_norm op.\n",
    "    def __init__(self, match_node, require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        super().__init__(rewrite_once=True)\n",
    "\n",
    "        # self.tuple_get_item_node = is_tuple_get_item(wildcard(), 0)\n",
    "        # self.pattern_1 = self.tuple_get_item_node\n",
    "        max_pool2d_node = is_op('nn.max_pool2d')(wildcard())\n",
    "        self.pattern = max_pool2d_node\n",
    "        self.match_node = match_node\n",
    "        self.counter = 0\n",
    "        self.tmp = []\n",
    "\n",
    "    def quant(self, node):\n",
    "        cast_to_int8 = relay.cast(\n",
    "            relay.clip(\n",
    "                relay.round(\n",
    "                    relay.multiply(node, relay.const(8.0))\n",
    "                ), \n",
    "                a_min=-127.0, a_max=127.0\n",
    "            ),\n",
    "            dtype=\"int8\"\n",
    "        )\n",
    "        result_node = relay.annotation.stop_fusion(cast_to_int8)\n",
    "        self.tmp.append(result_node)\n",
    "        return result_node\n",
    "\n",
    "    def dequant(self, node):\n",
    "        cast_to_float32 = relay.divide(\n",
    "            relay.cast(node, dtype='float32'), relay.const(8.0)\n",
    "        )\n",
    "        return cast_to_float32\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        print(\"match pool2d\")\n",
    "\n",
    "        if self.pattern.match(pre):\n",
    "            if pre in self.match_node:\n",
    "                print(\"pat 1\")\n",
    "                return self.dequant(self.quant(post))\n",
    "        return post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pat 1\n",
      "pat 1\n",
      "pat 1\n",
      "pat 1\n",
      "pat 1\n",
      "pat 1\n",
      "pat 1\n",
      "pat 1\n",
      "4\n",
      "match pool2d\n",
      "pat 1\n",
      "match pool2d\n",
      "pat 1\n",
      "match pool2d\n",
      "pat 1\n",
      "match pool2d\n",
      "pat 1\n",
      "fn (%input_1: Tensor[(1, 3, 512, 512), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(16), float32], %v_param_4: Tensor[(16), float32], %v_param_5: Tensor[(16), float32], %v_param_6: Tensor[(16), float32], %v_param_7: Tensor[(16, 16, 3, 3), float32], %v_param_8: Tensor[(16), float32], %v_param_9: Tensor[(16), float32], %v_param_10: Tensor[(16), float32], %v_param_11: Tensor[(16), float32], %v_param_12: Tensor[(16), float32], %v_param_13: Tensor[(32, 16, 3, 3), float32], %v_param_14: Tensor[(32), float32], %v_param_15: Tensor[(32), float32], %v_param_16: Tensor[(32), float32], %v_param_17: Tensor[(32), float32], %v_param_18: Tensor[(32), float32], %v_param_19: Tensor[(32, 32, 3, 3), float32], %v_param_20: Tensor[(32), float32], %v_param_21: Tensor[(32), float32], %v_param_22: Tensor[(32), float32], %v_param_23: Tensor[(32), float32], %v_param_24: Tensor[(32), float32], %v_param_25: Tensor[(64, 32, 3, 3), float32], %v_param_26: Tensor[(64), float32], %v_param_27: Tensor[(64), float32], %v_param_28: Tensor[(64), float32], %v_param_29: Tensor[(64), float32], %v_param_30: Tensor[(64), float32], %v_param_31: Tensor[(64, 64, 3, 3), float32], %v_param_32: Tensor[(64), float32], %v_param_33: Tensor[(64), float32], %v_param_34: Tensor[(64), float32], %v_param_35: Tensor[(64), float32], %v_param_36: Tensor[(64), float32], %v_param_37: Tensor[(128, 64, 3, 3), float32], %v_param_38: Tensor[(128), float32], %v_param_39: Tensor[(128), float32], %v_param_40: Tensor[(128), float32], %v_param_41: Tensor[(128), float32], %v_param_42: Tensor[(128), float32], %v_param_43: Tensor[(128, 128, 3, 3), float32], %v_param_44: Tensor[(128), float32], %v_param_45: Tensor[(128), float32], %v_param_46: Tensor[(128), float32], %v_param_47: Tensor[(128), float32], %v_param_48: Tensor[(128), float32], %v_param_49: Tensor[(256, 128, 3, 3), float32], %v_param_50: Tensor[(256), float32], %v_param_51: Tensor[(256), float32], %v_param_52: Tensor[(256), float32], %v_param_53: Tensor[(256), float32], %v_param_54: Tensor[(256), float32], %v_param_55: Tensor[(256, 256, 3, 3), float32], %v_param_56: Tensor[(256), float32], %v_param_57: Tensor[(256), float32], %v_param_58: Tensor[(256), float32], %v_param_59: Tensor[(256), float32], %v_param_60: Tensor[(256), float32], %v_param_61: Tensor[(256, 128, 3, 3), float32], %v_param_62: Tensor[(128), float32], %v_param_63: Tensor[(128), float32], %v_param_64: Tensor[(128), float32], %v_param_65: Tensor[(128), float32], %v_param_66: Tensor[(128), float32], %v_param_67: Tensor[(128, 256, 3, 3), float32], %v_param_68: Tensor[(128), float32], %v_param_69: Tensor[(128), float32], %v_param_70: Tensor[(128), float32], %v_param_71: Tensor[(128), float32], %v_param_72: Tensor[(128), float32], %v_param_73: Tensor[(128, 128, 3, 3), float32], %v_param_74: Tensor[(128), float32], %v_param_75: Tensor[(128), float32], %v_param_76: Tensor[(128), float32], %v_param_77: Tensor[(128), float32], %v_param_78: Tensor[(128), float32], %v_param_79: Tensor[(128, 64, 3, 3), float32], %v_param_80: Tensor[(64), float32], %v_param_81: Tensor[(64), float32], %v_param_82: Tensor[(64), float32], %v_param_83: Tensor[(64), float32], %v_param_84: Tensor[(64), float32], %v_param_85: Tensor[(64, 128, 3, 3), float32], %v_param_86: Tensor[(64), float32], %v_param_87: Tensor[(64), float32], %v_param_88: Tensor[(64), float32], %v_param_89: Tensor[(64), float32], %v_param_90: Tensor[(64), float32], %v_param_91: Tensor[(64, 64, 3, 3), float32], %v_param_92: Tensor[(64), float32], %v_param_93: Tensor[(64), float32], %v_param_94: Tensor[(64), float32], %v_param_95: Tensor[(64), float32], %v_param_96: Tensor[(64), float32], %v_param_97: Tensor[(64, 32, 3, 3), float32], %v_param_98: Tensor[(32), float32], %v_param_99: Tensor[(32), float32], %v_param_100: Tensor[(32), float32], %v_param_101: Tensor[(32), float32], %v_param_102: Tensor[(32), float32], %v_param_103: Tensor[(32, 64, 3, 3), float32], %v_param_104: Tensor[(32), float32], %v_param_105: Tensor[(32), float32], %v_param_106: Tensor[(32), float32], %v_param_107: Tensor[(32), float32], %v_param_108: Tensor[(32), float32], %v_param_109: Tensor[(32, 32, 3, 3), float32], %v_param_110: Tensor[(32), float32], %v_param_111: Tensor[(32), float32], %v_param_112: Tensor[(32), float32], %v_param_113: Tensor[(32), float32], %v_param_114: Tensor[(32), float32], %v_param_115: Tensor[(32, 16, 3, 3), float32], %v_param_116: Tensor[(16), float32], %v_param_117: Tensor[(16), float32], %v_param_118: Tensor[(16), float32], %v_param_119: Tensor[(16), float32], %v_param_120: Tensor[(16), float32], %v_param_121: Tensor[(16, 32, 3, 3), float32], %v_param_122: Tensor[(16), float32], %v_param_123: Tensor[(16), float32], %v_param_124: Tensor[(16), float32], %v_param_125: Tensor[(16), float32], %v_param_126: Tensor[(16), float32], %v_param_127: Tensor[(16, 16, 3, 3), float32], %v_param_128: Tensor[(16), float32], %v_param_129: Tensor[(16), float32], %v_param_130: Tensor[(16), float32], %v_param_131: Tensor[(16), float32], %v_param_132: Tensor[(16), float32], %v_param_133: Tensor[(1, 16, 3, 3), float32], %v_param_134: Tensor[(1), float32]) {\n",
      "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %1 = nn.bias_add(%0, %v_param_2);\n",
      "  %2 = nn.batch_norm(%1, %v_param_3, %v_param_4, %v_param_5, %v_param_6, epsilon=0.001f);\n",
      "  %3 = %2.0;\n",
      "  %4 = nn.leaky_relu(%3, alpha=0.2f);\n",
      "  %5 = nn.conv2d(%4, %v_param_7, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %6 = nn.bias_add(%5, %v_param_8);\n",
      "  %7 = nn.batch_norm(%6, %v_param_9, %v_param_10, %v_param_11, %v_param_12, epsilon=0.001f);\n",
      "  %8 = %7.0;\n",
      "  %9 = multiply(%8, 8f);\n",
      "  %10 = round(%9);\n",
      "  %11 = clip(%10, a_min=-127f, a_max=127f);\n",
      "  %12 = cast(%11, dtype=\"int8\");\n",
      "  %13 = annotation.stop_fusion(%12);\n",
      "  %14 = cast(%13, dtype=\"float32\");\n",
      "  %15 = divide(%14, 8f);\n",
      "  %16 = nn.max_pool2d(%15, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %17 = multiply(%16, 8f);\n",
      "  %18 = round(%17);\n",
      "  %19 = clip(%18, a_min=-127f, a_max=127f);\n",
      "  %20 = cast(%19, dtype=\"int8\");\n",
      "  %21 = annotation.stop_fusion(%20);\n",
      "  %22 = cast(%21, dtype=\"float32\");\n",
      "  %23 = divide(%22, 8f);\n",
      "  %24 = nn.conv2d(%23, %v_param_13, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %25 = nn.bias_add(%24, %v_param_14);\n",
      "  %26 = nn.batch_norm(%25, %v_param_15, %v_param_16, %v_param_17, %v_param_18, epsilon=0.001f);\n",
      "  %27 = %26.0;\n",
      "  %28 = nn.leaky_relu(%27, alpha=0.2f);\n",
      "  %29 = nn.conv2d(%28, %v_param_19, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %30 = nn.bias_add(%29, %v_param_20);\n",
      "  %31 = nn.batch_norm(%30, %v_param_21, %v_param_22, %v_param_23, %v_param_24, epsilon=0.001f);\n",
      "  %32 = %31.0;\n",
      "  %33 = multiply(%32, 8f);\n",
      "  %34 = round(%33);\n",
      "  %35 = clip(%34, a_min=-127f, a_max=127f);\n",
      "  %36 = cast(%35, dtype=\"int8\");\n",
      "  %37 = annotation.stop_fusion(%36);\n",
      "  %38 = cast(%37, dtype=\"float32\");\n",
      "  %39 = divide(%38, 8f);\n",
      "  %40 = nn.max_pool2d(%39, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %41 = multiply(%40, 8f);\n",
      "  %42 = round(%41);\n",
      "  %43 = clip(%42, a_min=-127f, a_max=127f);\n",
      "  %44 = cast(%43, dtype=\"int8\");\n",
      "  %45 = annotation.stop_fusion(%44);\n",
      "  %46 = cast(%45, dtype=\"float32\");\n",
      "  %47 = divide(%46, 8f);\n",
      "  %48 = nn.conv2d(%47, %v_param_25, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %49 = nn.bias_add(%48, %v_param_26);\n",
      "  %50 = nn.batch_norm(%49, %v_param_27, %v_param_28, %v_param_29, %v_param_30, epsilon=0.001f);\n",
      "  %51 = %50.0;\n",
      "  %52 = nn.leaky_relu(%51, alpha=0.2f);\n",
      "  %53 = nn.conv2d(%52, %v_param_31, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %54 = nn.bias_add(%53, %v_param_32);\n",
      "  %55 = nn.batch_norm(%54, %v_param_33, %v_param_34, %v_param_35, %v_param_36, epsilon=0.001f);\n",
      "  %56 = %55.0;\n",
      "  %57 = multiply(%56, 8f);\n",
      "  %58 = round(%57);\n",
      "  %59 = clip(%58, a_min=-127f, a_max=127f);\n",
      "  %60 = cast(%59, dtype=\"int8\");\n",
      "  %61 = annotation.stop_fusion(%60);\n",
      "  %62 = cast(%61, dtype=\"float32\");\n",
      "  %63 = divide(%62, 8f);\n",
      "  %64 = nn.max_pool2d(%63, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %65 = multiply(%64, 8f);\n",
      "  %66 = round(%65);\n",
      "  %67 = clip(%66, a_min=-127f, a_max=127f);\n",
      "  %68 = cast(%67, dtype=\"int8\");\n",
      "  %69 = annotation.stop_fusion(%68);\n",
      "  %70 = cast(%69, dtype=\"float32\");\n",
      "  %71 = divide(%70, 8f);\n",
      "  %72 = nn.conv2d(%71, %v_param_37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %73 = nn.bias_add(%72, %v_param_38);\n",
      "  %74 = nn.batch_norm(%73, %v_param_39, %v_param_40, %v_param_41, %v_param_42, epsilon=0.001f);\n",
      "  %75 = %74.0;\n",
      "  %76 = nn.leaky_relu(%75, alpha=0.2f);\n",
      "  %77 = nn.conv2d(%76, %v_param_43, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %78 = nn.bias_add(%77, %v_param_44);\n",
      "  %79 = nn.batch_norm(%78, %v_param_45, %v_param_46, %v_param_47, %v_param_48, epsilon=0.001f);\n",
      "  %80 = %79.0;\n",
      "  %81 = multiply(%80, 8f);\n",
      "  %82 = round(%81);\n",
      "  %83 = clip(%82, a_min=-127f, a_max=127f);\n",
      "  %84 = cast(%83, dtype=\"int8\");\n",
      "  %85 = annotation.stop_fusion(%84);\n",
      "  %86 = cast(%85, dtype=\"float32\");\n",
      "  %87 = divide(%86, 8f);\n",
      "  %88 = nn.max_pool2d(%87, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %89 = multiply(%88, 8f);\n",
      "  %90 = round(%89);\n",
      "  %91 = clip(%90, a_min=-127f, a_max=127f);\n",
      "  %92 = cast(%91, dtype=\"int8\");\n",
      "  %93 = annotation.stop_fusion(%92);\n",
      "  %94 = cast(%93, dtype=\"float32\");\n",
      "  %95 = divide(%94, 8f);\n",
      "  %96 = nn.conv2d(%95, %v_param_49, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %97 = nn.bias_add(%96, %v_param_50);\n",
      "  %98 = nn.batch_norm(%97, %v_param_51, %v_param_52, %v_param_53, %v_param_54, epsilon=0.001f);\n",
      "  %99 = %98.0;\n",
      "  %100 = nn.leaky_relu(%99, alpha=0.2f);\n",
      "  %101 = nn.conv2d(%100, %v_param_55, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %102 = nn.bias_add(%101, %v_param_56);\n",
      "  %103 = nn.batch_norm(%102, %v_param_57, %v_param_58, %v_param_59, %v_param_60, epsilon=0.001f);\n",
      "  %104 = %103.0;\n",
      "  %105 = nn.conv2d_transpose(%104, %v_param_61, channels=128, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
      "  %106 = nn.bias_add(%105, %v_param_62);\n",
      "  %107 = nn.batch_norm(%106, %v_param_63, %v_param_64, %v_param_65, %v_param_66, epsilon=0.001f);\n",
      "  %108 = %107.0;\n",
      "  %109 = nn.leaky_relu(%108, alpha=0.2f);\n",
      "  %110 = (%109, %87);\n",
      "  %111 = concatenate(%110, axis=1);\n",
      "  %112 = multiply(%111, 8f);\n",
      "  %113 = round(%112);\n",
      "  %114 = clip(%113, a_min=-127f, a_max=127f);\n",
      "  %115 = cast(%114, dtype=\"int8\");\n",
      "  %116 = annotation.stop_fusion(%115);\n",
      "  %117 = cast(%116, dtype=\"float32\");\n",
      "  %118 = divide(%117, 8f);\n",
      "  %119 = nn.conv2d(%118, %v_param_67, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %120 = nn.bias_add(%119, %v_param_68);\n",
      "  %121 = nn.batch_norm(%120, %v_param_69, %v_param_70, %v_param_71, %v_param_72, epsilon=0.001f);\n",
      "  %122 = %121.0;\n",
      "  %123 = nn.leaky_relu(%122, alpha=0.2f);\n",
      "  %124 = nn.conv2d(%123, %v_param_73, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %125 = nn.bias_add(%124, %v_param_74);\n",
      "  %126 = nn.batch_norm(%125, %v_param_75, %v_param_76, %v_param_77, %v_param_78, epsilon=0.001f);\n",
      "  %127 = %126.0;\n",
      "  %128 = nn.conv2d_transpose(%127, %v_param_79, channels=64, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
      "  %129 = nn.bias_add(%128, %v_param_80);\n",
      "  %130 = nn.batch_norm(%129, %v_param_81, %v_param_82, %v_param_83, %v_param_84, epsilon=0.001f);\n",
      "  %131 = %130.0;\n",
      "  %132 = nn.leaky_relu(%131, alpha=0.2f);\n",
      "  %133 = (%132, %63);\n",
      "  %134 = concatenate(%133, axis=1);\n",
      "  %135 = multiply(%134, 8f);\n",
      "  %136 = round(%135);\n",
      "  %137 = clip(%136, a_min=-127f, a_max=127f);\n",
      "  %138 = cast(%137, dtype=\"int8\");\n",
      "  %139 = annotation.stop_fusion(%138);\n",
      "  %140 = cast(%139, dtype=\"float32\");\n",
      "  %141 = divide(%140, 8f);\n",
      "  %142 = nn.conv2d(%141, %v_param_85, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %143 = nn.bias_add(%142, %v_param_86);\n",
      "  %144 = nn.batch_norm(%143, %v_param_87, %v_param_88, %v_param_89, %v_param_90, epsilon=0.001f);\n",
      "  %145 = %144.0;\n",
      "  %146 = nn.leaky_relu(%145, alpha=0.2f);\n",
      "  %147 = nn.conv2d(%146, %v_param_91, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %148 = nn.bias_add(%147, %v_param_92);\n",
      "  %149 = nn.batch_norm(%148, %v_param_93, %v_param_94, %v_param_95, %v_param_96, epsilon=0.001f);\n",
      "  %150 = %149.0;\n",
      "  %151 = nn.conv2d_transpose(%150, %v_param_97, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
      "  %152 = nn.bias_add(%151, %v_param_98);\n",
      "  %153 = nn.batch_norm(%152, %v_param_99, %v_param_100, %v_param_101, %v_param_102, epsilon=0.001f);\n",
      "  %154 = %153.0;\n",
      "  %155 = nn.leaky_relu(%154, alpha=0.2f);\n",
      "  %156 = (%155, %39);\n",
      "  %157 = concatenate(%156, axis=1);\n",
      "  %158 = multiply(%157, 8f);\n",
      "  %159 = round(%158);\n",
      "  %160 = clip(%159, a_min=-127f, a_max=127f);\n",
      "  %161 = cast(%160, dtype=\"int8\");\n",
      "  %162 = annotation.stop_fusion(%161);\n",
      "  %163 = cast(%162, dtype=\"float32\");\n",
      "  %164 = divide(%163, 8f);\n",
      "  %165 = nn.conv2d(%164, %v_param_103, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %166 = nn.bias_add(%165, %v_param_104);\n",
      "  %167 = nn.batch_norm(%166, %v_param_105, %v_param_106, %v_param_107, %v_param_108, epsilon=0.001f);\n",
      "  %168 = %167.0;\n",
      "  %169 = nn.leaky_relu(%168, alpha=0.2f);\n",
      "  %170 = nn.conv2d(%169, %v_param_109, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %171 = nn.bias_add(%170, %v_param_110);\n",
      "  %172 = nn.batch_norm(%171, %v_param_111, %v_param_112, %v_param_113, %v_param_114, epsilon=0.001f);\n",
      "  %173 = %172.0;\n",
      "  %174 = nn.conv2d_transpose(%173, %v_param_115, channels=16, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
      "  %175 = nn.bias_add(%174, %v_param_116);\n",
      "  %176 = nn.batch_norm(%175, %v_param_117, %v_param_118, %v_param_119, %v_param_120, epsilon=0.001f);\n",
      "  %177 = %176.0;\n",
      "  %178 = nn.leaky_relu(%177, alpha=0.2f);\n",
      "  %179 = (%178, %15);\n",
      "  %180 = concatenate(%179, axis=1);\n",
      "  %181 = multiply(%180, 8f);\n",
      "  %182 = round(%181);\n",
      "  %183 = clip(%182, a_min=-127f, a_max=127f);\n",
      "  %184 = cast(%183, dtype=\"int8\");\n",
      "  %185 = annotation.stop_fusion(%184);\n",
      "  %186 = cast(%185, dtype=\"float32\");\n",
      "  %187 = divide(%186, 8f);\n",
      "  %188 = nn.conv2d(%187, %v_param_121, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %189 = nn.bias_add(%188, %v_param_122);\n",
      "  %190 = nn.batch_norm(%189, %v_param_123, %v_param_124, %v_param_125, %v_param_126, epsilon=0.001f);\n",
      "  %191 = %190.0;\n",
      "  %192 = nn.leaky_relu(%191, alpha=0.2f);\n",
      "  %193 = nn.conv2d(%192, %v_param_127, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %194 = nn.bias_add(%193, %v_param_128);\n",
      "  %195 = nn.batch_norm(%194, %v_param_129, %v_param_130, %v_param_131, %v_param_132, epsilon=0.001f);\n",
      "  %196 = %195.0;\n",
      "  %197 = nn.conv2d(%196, %v_param_133, padding=[1, 1, 1, 1], channels=1, kernel_size=[3, 3]);\n",
      "  %198 = nn.bias_add(%197, %v_param_134);\n",
      "  sigmoid(%198)\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "upc = UnetPreProcessCallback()\n",
    "rewrite(upc, mod['main'])\n",
    "uc = UnetCallback(upc.match_node)\n",
    "out = rewrite(uc, mod['main'])\n",
    "\n",
    "upc = UnetPreProcessCallback()\n",
    "rewrite(upc, out)\n",
    "uc2 = UnetCallback2(upc.match_node2)\n",
    "out = rewrite(uc2, out)\n",
    "\n",
    "upc = UnetMaxPool2dCallback()\n",
    "rewrite(upc, out)\n",
    "print(len(upc.match_node))\n",
    "uc2 = UnetCallback3(upc.match_node)\n",
    "out = rewrite(uc2, out)\n",
    "print(out)\n",
    "# out = relay.Function(out.params, relay.Tuple(uc.tmp + [out.body]), out.ret_type, out.type_params, out.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(upc.match_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(out, target, params=params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygraphviz as pgv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph(json_data, file_name=None):\n",
    "    if type(json_data) == str:\n",
    "        json_data = json.loads(json_data)\n",
    "    A = pgv.AGraph(directed=True)\n",
    "    for node_idx, node in enumerate(json_data['nodes']):\n",
    "        for src in node['inputs']:\n",
    "            if 1 == 1:\n",
    "                src_size = 1\n",
    "                for i in json_data['attrs']['shape'][1][src[0]]:\n",
    "                    src_size = src_size * i\n",
    "                \n",
    "                dst_size = 1\n",
    "                for i in json_data['attrs']['shape'][1][node_idx]:\n",
    "                    dst_size = dst_size * i\n",
    "                print(src[0], json_data['nodes'][src[0]]['name'], src_size)\n",
    "\n",
    "                A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]) + \"[{}]\".format(src_size), node['name'] + '[{}]'.format(node_idx) + '{}'.format(json_data['attrs']['dltype'][1][node_idx]) + \"[{}]\".format(dst_size))\n",
    "            else:\n",
    "                A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]), node['name'] + '[{}]'.format(node_idx) + '{}'.format(json_data['attrs']['dltype'][1][node_idx]))\n",
    "    if file_name:\n",
    "        A.draw(file_name + '.png', format='png', prog='dot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1 786432\n",
      "1 p0 432\n",
      "2 p1 16\n",
      "3 p2 16\n",
      "4 tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu 4194304\n",
      "5 p3 2304\n",
      "6 p4 16\n",
      "7 p5 16\n",
      "8 p6 16\n",
      "9 tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast 4194304\n",
      "10 tvmgen_default_fused_cast_divide 4194304\n",
      "11 tvmgen_default_fused_nn_max_pool2d_multiply_round_clip_cast 1048576\n",
      "12 tvmgen_default_fused_cast_divide_1 1048576\n",
      "13 p7 4608\n",
      "14 p8 32\n",
      "15 p9 32\n",
      "16 tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_1 2097152\n",
      "17 p10 9216\n",
      "18 p11 32\n",
      "19 p12 32\n",
      "20 p13 32\n",
      "21 tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast_1 2097152\n",
      "22 tvmgen_default_fused_cast_divide_2 2097152\n",
      "23 tvmgen_default_fused_nn_max_pool2d_multiply_round_clip_cast_1 524288\n",
      "24 tvmgen_default_fused_cast_divide_3 524288\n",
      "25 p14 18432\n",
      "26 p15 64\n",
      "27 p16 64\n",
      "28 tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_2 1048576\n",
      "29 p17 36864\n",
      "30 p18 64\n",
      "31 p19 64\n",
      "32 p20 64\n",
      "33 tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast_2 1048576\n",
      "34 tvmgen_default_fused_cast_divide_4 1048576\n",
      "35 tvmgen_default_fused_nn_max_pool2d_multiply_round_clip_cast_2 262144\n",
      "36 tvmgen_default_fused_cast_divide_5 262144\n",
      "37 p21 73728\n",
      "38 p22 128\n",
      "39 p23 128\n",
      "40 tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_3 524288\n",
      "41 p24 147456\n",
      "42 p25 128\n",
      "43 p26 128\n",
      "44 p27 128\n",
      "45 tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast_3 524288\n",
      "46 tvmgen_default_fused_cast_divide_6 524288\n",
      "47 tvmgen_default_fused_nn_max_pool2d_multiply_round_clip_cast_3 131072\n",
      "48 tvmgen_default_fused_cast_divide_7 131072\n",
      "49 p28 294912\n",
      "50 p29 256\n",
      "51 p30 256\n",
      "52 tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_4 262144\n",
      "53 p31 589824\n",
      "54 p32 256\n",
      "55 p33 256\n",
      "56 p34 256\n",
      "57 tvmgen_default_fused_nn_conv2d_add_multiply_add 262144\n",
      "58 p35 294912\n",
      "59 p36 128\n",
      "60 p37 128\n",
      "61 p38 128\n",
      "62 tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu 524288\n",
      "46 tvmgen_default_fused_cast_divide_6 524288\n",
      "63 tvmgen_default_fused_concatenate_multiply_round_clip_cast 1048576\n",
      "64 tvmgen_default_fused_cast_divide_8 1048576\n",
      "65 p39 294912\n",
      "66 p40 128\n",
      "67 p41 128\n",
      "68 tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_5 524288\n",
      "69 p42 147456\n",
      "70 p43 128\n",
      "71 p44 128\n",
      "72 p45 128\n",
      "73 tvmgen_default_fused_nn_conv2d_add_multiply_add_1 524288\n",
      "74 p46 73728\n",
      "75 p47 64\n",
      "76 p48 64\n",
      "77 p49 64\n",
      "78 tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_1 1048576\n",
      "34 tvmgen_default_fused_cast_divide_4 1048576\n",
      "79 tvmgen_default_fused_concatenate_multiply_round_clip_cast_1 2097152\n",
      "80 tvmgen_default_fused_cast_divide_9 2097152\n",
      "81 p50 73728\n",
      "82 p51 64\n",
      "83 p52 64\n",
      "84 tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_6 1048576\n",
      "85 p53 36864\n",
      "86 p54 64\n",
      "87 p55 64\n",
      "88 p56 64\n",
      "89 tvmgen_default_fused_nn_conv2d_add_multiply_add_2 1048576\n",
      "90 p57 18432\n",
      "91 p58 32\n",
      "92 p59 32\n",
      "93 p60 32\n",
      "94 tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_2 2097152\n",
      "22 tvmgen_default_fused_cast_divide_2 2097152\n",
      "95 tvmgen_default_fused_concatenate_multiply_round_clip_cast_2 4194304\n",
      "96 tvmgen_default_fused_cast_divide_10 4194304\n",
      "97 p61 18432\n",
      "98 p62 32\n",
      "99 p63 32\n",
      "100 tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_7 2097152\n",
      "101 p64 9216\n",
      "102 p65 32\n",
      "103 p66 32\n",
      "104 p67 32\n",
      "105 tvmgen_default_fused_nn_conv2d_add_multiply_add_3 2097152\n",
      "106 p68 4608\n",
      "107 p69 16\n",
      "108 p70 16\n",
      "109 p71 16\n",
      "110 tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_3 4194304\n",
      "10 tvmgen_default_fused_cast_divide 4194304\n",
      "111 tvmgen_default_fused_concatenate_multiply_round_clip_cast_3 8388608\n",
      "112 tvmgen_default_fused_cast_divide_11 8388608\n",
      "113 p72 4608\n",
      "114 p73 16\n",
      "115 p74 16\n",
      "116 tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_8 4194304\n",
      "117 p75 2304\n",
      "118 p76 16\n",
      "119 p77 16\n",
      "120 tvmgen_default_fused_nn_conv2d_add_add 4194304\n",
      "121 p78 144\n",
      "122 p79 1\n"
     ]
    }
   ],
   "source": [
    "graph_json_raw = lib['get_graph_json']()\n",
    "\n",
    "show_graph(graph_json_raw, \"./test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./src/graph/{}_{}_{}_{}_{}-{}.json\".format(\"unet\", \"cuda\", 512, 3, 0, 123), \"w\") as json_file:\n",
    "    json_file.write(json.dumps(json.loads(graph_json_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Initial models\n",
      "model_nodes [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91]\n",
      "complement_nodes [-1]\n",
      "##############################\n",
      "[ 92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119]\n",
      "######################################\n",
      "After input analyze\n",
      "intermediate_nodes []\n",
      "input_dependency defaultdict(<class 'list'>, {})\n",
      "model_nodes [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91]\n",
      "######################################\n",
      "######################################\n",
      "After output analyze\n",
      "output_dependency defaultdict(<class 'list'>, {91: [92, 92], 9: [107]})\n",
      "model_nodes [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91]\n",
      "######################################\n",
      "input_dependency.keys() dict_keys([])\n",
      "model_nodes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]\n",
      "output_dependency defaultdict(<class 'list'>, {91: [92, 92], 9: [107]})\n",
      "output_nodes [9, 91]\n",
      "{'input_1': 0, 'p0': 1, 'p1': 2, 'p2': 3, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu': 4, 'p3': 5, 'p4': 6, 'p5': 7, 'p6': 8, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast': 9, 'tvmgen_default_fused_cast_divide': 10, 'tvmgen_default_fused_nn_max_pool2d': 11, 'p7': 12, 'p8': 13, 'p9': 14, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_1': 15, 'p10': 16, 'p11': 17, 'p12': 18, 'p13': 19, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast_1': 20, 'tvmgen_default_fused_cast_divide_1': 21, 'tvmgen_default_fused_nn_max_pool2d_1': 22, 'p14': 23, 'p15': 24, 'p16': 25, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_2': 26, 'p17': 27, 'p18': 28, 'p19': 29, 'p20': 30, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast_2': 31, 'tvmgen_default_fused_cast_divide_2': 32, 'tvmgen_default_fused_nn_max_pool2d_2': 33, 'p21': 34, 'p22': 35, 'p23': 36, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_3': 37, 'p24': 38, 'p25': 39, 'p26': 40, 'p27': 41, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast_3': 42, 'tvmgen_default_fused_cast_divide_3': 43, 'tvmgen_default_fused_nn_max_pool2d_3': 44, 'p28': 45, 'p29': 46, 'p30': 47, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_4': 48, 'p31': 49, 'p32': 50, 'p33': 51, 'p34': 52, 'tvmgen_default_fused_nn_conv2d_add_multiply_add': 53, 'p35': 54, 'p36': 55, 'p37': 56, 'p38': 57, 'tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu': 58, 'tvmgen_default_fused_concatenate_multiply_round_clip_cast': 59, 'tvmgen_default_fused_cast_divide_4': 60, 'p39': 61, 'p40': 62, 'p41': 63, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_5': 64, 'p42': 65, 'p43': 66, 'p44': 67, 'p45': 68, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_1': 69, 'p46': 70, 'p47': 71, 'p48': 72, 'p49': 73, 'tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_1': 74, 'tvmgen_default_fused_concatenate_multiply_round_clip_cast_1': 75, 'tvmgen_default_fused_cast_divide_5': 76, 'p50': 77, 'p51': 78, 'p52': 79, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_6': 80, 'p53': 81, 'p54': 82, 'p55': 83, 'p56': 84, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_2': 85, 'p57': 86, 'p58': 87, 'p59': 88, 'p60': 89, 'tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_2': 90, 'tvmgen_default_fused_concatenate_multiply_round_clip_cast_2': 91}\n",
      "====================\n",
      "[ 9 91]\n",
      "##############################\n",
      "Initial models\n",
      "model_nodes [ 92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119]\n",
      "complement_nodes [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91]\n",
      "##############################\n",
      "[]\n",
      "######################################\n",
      "After input analyze\n",
      "intermediate_nodes [10]\n",
      "input_dependency defaultdict(<class 'list'>, {91: [92, 92], 9: [10]})\n",
      "model_nodes [ 92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119]\n",
      "######################################\n",
      "######################################\n",
      "After output analyze\n",
      "output_dependency defaultdict(<class 'list'>, {})\n",
      "model_nodes [ 92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119]\n",
      "######################################\n",
      "input_dependency.keys() dict_keys([91, 9])\n",
      "model_nodes [9, 91, 10, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "output_dependency defaultdict(<class 'list'>, {})\n",
      "output_nodes [119]\n",
      "{'input_9': 0, 'input_91': 1, 'tvmgen_default_fused_cast_divide': 2, 'tvmgen_default_fused_cast_divide_6': 3, 'p61': 4, 'p62': 5, 'p63': 6, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_7': 7, 'p64': 8, 'p65': 9, 'p66': 10, 'p67': 11, 'tvmgen_default_fused_nn_conv2d_add_multiply_add_3': 12, 'p68': 13, 'p69': 14, 'p70': 15, 'p71': 16, 'tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_3': 17, 'tvmgen_default_fused_concatenate_multiply_round_clip_cast_3': 18, 'tvmgen_default_fused_cast_divide_7': 19, 'p72': 20, 'p73': 21, 'p74': 22, 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_8': 23, 'p75': 24, 'p76': 25, 'p77': 26, 'tvmgen_default_fused_nn_conv2d_add_add': 27, 'p78': 28, 'p79': 29, 'tvmgen_default_fused_nn_conv2d_add_sigmoid': 30}\n",
      "input_9\n",
      "input_91\n",
      "====================\n",
      "[119]\n"
     ]
    }
   ],
   "source": [
    "tvm_slicer = TVMSlicer(graph_json_raw)\n",
    "graph_json_front, _, _ = tvm_slicer.slice_graph(0, 91, is_quantize_sliced=True)\n",
    "graph_json_back, _, _ = tvm_slicer.slice_graph(91 + 1, 119, is_quantize_sliced=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(graph_json_front, \"./front\")\n",
    "show_graph(graph_json_back, \"./back\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'op': 'null',\n",
       "   'name': 'input_9',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '5',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_add_multiply_add_multiply_round_clip_cast',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'OIHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': 'c35c41a655f8a1a2'},\n",
       "   'inputs': []},\n",
       "  {'op': 'null',\n",
       "   'name': 'input_91',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '2',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_concatenate_multiply_round_clip_cast_2',\n",
       "    'hash': '9e940466005aa1f3'},\n",
       "   'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_cast_divide',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '1',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_cast_divide',\n",
       "    'hash': '85165d63e18469f5'},\n",
       "   'inputs': [[0, 0, 0]]},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_cast_divide_6',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '1',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_cast_divide_6',\n",
       "    'hash': 'fa27adaace0bfe80'},\n",
       "   'inputs': [[1, 0, 0]]},\n",
       "  {'op': 'null', 'name': 'p61', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p62', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p63', 'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_7',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '4',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_7',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'OIHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': 'f81b264d19b9a6be'},\n",
       "   'inputs': [[3, 0, 0], [4, 0, 0], [5, 0, 0], [6, 0, 0]]},\n",
       "  {'op': 'null', 'name': 'p64', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p65', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p66', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p67', 'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_nn_conv2d_add_multiply_add_3',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '5',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_add_multiply_add_3',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'OIHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': 'afcc3e3243b199b7'},\n",
       "   'inputs': [[7, 0, 0], [8, 0, 0], [9, 0, 0], [10, 0, 0], [11, 0, 0]]},\n",
       "  {'op': 'null', 'name': 'p68', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p69', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p70', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p71', 'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_3',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '5',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_transpose_add_multiply_add_nn_leaky_relu_3',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'IOHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': 'e2fccacfae4878e3'},\n",
       "   'inputs': [[12, 0, 0], [13, 0, 0], [14, 0, 0], [15, 0, 0], [16, 0, 0]]},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_concatenate_multiply_round_clip_cast_3',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '2',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_concatenate_multiply_round_clip_cast_3',\n",
       "    'hash': '9c8a637413505916'},\n",
       "   'inputs': [[17, 0, 0], [2, 0, 0]]},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_cast_divide_7',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '1',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_cast_divide_7',\n",
       "    'hash': '186333aa7e215879'},\n",
       "   'inputs': [[18, 0, 0]]},\n",
       "  {'op': 'null', 'name': 'p72', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p73', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p74', 'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_8',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '4',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_add_add_nn_leaky_relu_8',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'OIHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': '0258e262c093a80d'},\n",
       "   'inputs': [[19, 0, 0], [20, 0, 0], [21, 0, 0], [22, 0, 0]]},\n",
       "  {'op': 'null', 'name': 'p75', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p76', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p77', 'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_nn_conv2d_add_add',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '4',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_add_add',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'OIHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': 'b83cfcda2b3569ee'},\n",
       "   'inputs': [[23, 0, 0], [24, 0, 0], [25, 0, 0], [26, 0, 0]]},\n",
       "  {'op': 'null', 'name': 'p78', 'inputs': []},\n",
       "  {'op': 'null', 'name': 'p79', 'inputs': []},\n",
       "  {'op': 'tvm_op',\n",
       "   'name': 'tvmgen_default_fused_nn_conv2d_add_sigmoid',\n",
       "   'attrs': {'num_outputs': '1',\n",
       "    'num_inputs': '3',\n",
       "    'flatten_data': '0',\n",
       "    'func_name': 'tvmgen_default_fused_nn_conv2d_add_sigmoid',\n",
       "    'out_layout': '',\n",
       "    'kernel_layout': 'OIHW',\n",
       "    'data_layout': 'NCHW',\n",
       "    'hash': '61cdf1c6242552b1'},\n",
       "   'inputs': [[27, 0, 0], [28, 0, 0], [29, 0, 0]]}],\n",
       " 'arg_nodes': [0,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  28,\n",
       "  29],\n",
       " 'heads': [[30, 0, 0]],\n",
       " 'attrs': {'dltype': ['list_str',\n",
       "   ['int8',\n",
       "    'int8',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'int8',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32',\n",
       "    'float32']],\n",
       "  'device_index': ['list_int',\n",
       "   [2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2,\n",
       "    2]],\n",
       "  'storage_id': ['list_int',\n",
       "   [9,\n",
       "    41,\n",
       "    4,\n",
       "    14,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    23,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    14,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    23,\n",
       "    14,\n",
       "    4,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    23,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    14,\n",
       "    89,\n",
       "    90,\n",
       "    37]],\n",
       "  'shape': ['list_shape',\n",
       "   [[1, 16, 512, 512],\n",
       "    [1, 64, 256, 256],\n",
       "    [1, 16, 512, 512],\n",
       "    [1, 64, 256, 256],\n",
       "    [32, 64, 3, 3],\n",
       "    [1, 32, 1, 1],\n",
       "    [1, 32, 1, 1],\n",
       "    [1, 32, 256, 256],\n",
       "    [32, 32, 3, 3],\n",
       "    [1, 32, 1, 1],\n",
       "    [1, 32, 1, 1],\n",
       "    [1, 32, 1, 1],\n",
       "    [1, 32, 256, 256],\n",
       "    [32, 16, 3, 3],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 512, 512],\n",
       "    [1, 32, 512, 512],\n",
       "    [1, 32, 512, 512],\n",
       "    [16, 32, 3, 3],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 512, 512],\n",
       "    [16, 16, 3, 3],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 1, 1],\n",
       "    [1, 16, 512, 512],\n",
       "    [1, 16, 3, 3],\n",
       "    [1, 1, 1],\n",
       "    [1, 1, 512, 512]]]},\n",
       " 'node_row_ptr': [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_json_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
