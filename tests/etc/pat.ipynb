{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-07 22:06:28.472760: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tvm.relay import testing\n",
    "import tvm\n",
    "from tvm import te\n",
    "from tvm.contrib import graph_executor\n",
    "import tvm.testing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tvm.relay as relay\n",
    "from tvm import relay\n",
    "from tvm.relay.build_module import bind_params_by_name\n",
    "from tvm.relay.dataflow_pattern import *\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pygraphviz as pgv\n",
    "PATH_MODEL = './model/'\n",
    "def show_graph(json_data, file_name=None):\n",
    "    if type(json_data) == str:\n",
    "        json_data = json.loads(json_data)\n",
    "    A = pgv.AGraph(directed=True)\n",
    "    for node_idx, node in enumerate(json_data['nodes']):\n",
    "        for src in node['inputs']:\n",
    "            A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]), node['name'] + '[{}]'.format(node_idx) + '{}'.format(json_data['attrs']['dltype'][1][node_idx]))\n",
    "            #A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(shape_size(json_data['attrs']['shape'][1][src[0]])) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]), node['name'] + '[{}]'.format(node_idx) + '{}'.format(shape_size(json_data['attrs']['shape'][1][node_idx])) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]))\n",
    "    if file_name:\n",
    "        A.draw(file_name + '.png', format='png', prog='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-07 22:06:45.365440: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-07 22:06:45.365505: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-05-07 22:06:45.393177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-07 22:06:45.393459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.665GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-05-07 22:06:45.393473: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-05-07 22:06:45.395170: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-05-07 22:06:45.395202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-05-07 22:06:45.395919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-07 22:06:45.396079: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-07 22:06:45.397893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-07 22:06:45.398313: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-05-07 22:06:45.398341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-05-07 22:06:45.398392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-07 22:06:45.398772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-07 22:06:45.399094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-05-07 22:06:45.399397: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-07 22:06:45.399744: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-07 22:06:45.399841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-07 22:06:45.400186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.665GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-05-07 22:06:45.400202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-05-07 22:06:45.400217: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-05-07 22:06:45.400229: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-05-07 22:06:45.400241: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-07 22:06:45.400252: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-07 22:06:45.400264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-07 22:06:45.400277: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-05-07 22:06:45.400285: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-05-07 22:06:45.400315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-07 22:06:45.400668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-07 22:06:45.400981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-05-07 22:06:45.805798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-05-07 22:06:45.805825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-05-07 22:06:45.805829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-05-07 22:06:45.805927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-07 22:06:45.806249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-07 22:06:45.806508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-07 22:06:45.806746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9680 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "model_keras = tf.keras.models.load_model('../../tvm-slicer/src/model/{}_{}.h5'.format(\"unet\", 512))\n",
    "\n",
    "input_data = np.random.normal(0,1,(1,512,512,3)).astype(np.float32)\n",
    "input_data = input_data.transpose([0, 3, 1, 2])\n",
    "shape_dict = {\"input_1\": input_data.shape}\n",
    "mod, params = relay.frontend.from_keras(model_keras, shape_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%input_1: Tensor[(1, 3, 512, 512), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(16), float32], %v_param_4: Tensor[(16), float32], %v_param_5: Tensor[(16), float32], %v_param_6: Tensor[(16), float32], %v_param_7: Tensor[(16, 16, 3, 3), float32], %v_param_8: Tensor[(16), float32], %v_param_9: Tensor[(16), float32], %v_param_10: Tensor[(16), float32], %v_param_11: Tensor[(16), float32], %v_param_12: Tensor[(16), float32], %v_param_13: Tensor[(32, 16, 3, 3), float32], %v_param_14: Tensor[(32), float32], %v_param_15: Tensor[(32), float32], %v_param_16: Tensor[(32), float32], %v_param_17: Tensor[(32), float32], %v_param_18: Tensor[(32), float32], %v_param_19: Tensor[(32, 32, 3, 3), float32], %v_param_20: Tensor[(32), float32], %v_param_21: Tensor[(32), float32], %v_param_22: Tensor[(32), float32], %v_param_23: Tensor[(32), float32], %v_param_24: Tensor[(32), float32], %v_param_25: Tensor[(64, 32, 3, 3), float32], %v_param_26: Tensor[(64), float32], %v_param_27: Tensor[(64), float32], %v_param_28: Tensor[(64), float32], %v_param_29: Tensor[(64), float32], %v_param_30: Tensor[(64), float32], %v_param_31: Tensor[(64, 64, 3, 3), float32], %v_param_32: Tensor[(64), float32], %v_param_33: Tensor[(64), float32], %v_param_34: Tensor[(64), float32], %v_param_35: Tensor[(64), float32], %v_param_36: Tensor[(64), float32], %v_param_37: Tensor[(128, 64, 3, 3), float32], %v_param_38: Tensor[(128), float32], %v_param_39: Tensor[(128), float32], %v_param_40: Tensor[(128), float32], %v_param_41: Tensor[(128), float32], %v_param_42: Tensor[(128), float32], %v_param_43: Tensor[(128, 128, 3, 3), float32], %v_param_44: Tensor[(128), float32], %v_param_45: Tensor[(128), float32], %v_param_46: Tensor[(128), float32], %v_param_47: Tensor[(128), float32], %v_param_48: Tensor[(128), float32], %v_param_49: Tensor[(256, 128, 3, 3), float32], %v_param_50: Tensor[(256), float32], %v_param_51: Tensor[(256), float32], %v_param_52: Tensor[(256), float32], %v_param_53: Tensor[(256), float32], %v_param_54: Tensor[(256), float32], %v_param_55: Tensor[(256, 256, 3, 3), float32], %v_param_56: Tensor[(256), float32], %v_param_57: Tensor[(256), float32], %v_param_58: Tensor[(256), float32], %v_param_59: Tensor[(256), float32], %v_param_60: Tensor[(256), float32], %v_param_61: Tensor[(256, 128, 3, 3), float32], %v_param_62: Tensor[(128), float32], %v_param_63: Tensor[(128), float32], %v_param_64: Tensor[(128), float32], %v_param_65: Tensor[(128), float32], %v_param_66: Tensor[(128), float32], %v_param_67: Tensor[(128, 256, 3, 3), float32], %v_param_68: Tensor[(128), float32], %v_param_69: Tensor[(128), float32], %v_param_70: Tensor[(128), float32], %v_param_71: Tensor[(128), float32], %v_param_72: Tensor[(128), float32], %v_param_73: Tensor[(128, 128, 3, 3), float32], %v_param_74: Tensor[(128), float32], %v_param_75: Tensor[(128), float32], %v_param_76: Tensor[(128), float32], %v_param_77: Tensor[(128), float32], %v_param_78: Tensor[(128), float32], %v_param_79: Tensor[(128, 64, 3, 3), float32], %v_param_80: Tensor[(64), float32], %v_param_81: Tensor[(64), float32], %v_param_82: Tensor[(64), float32], %v_param_83: Tensor[(64), float32], %v_param_84: Tensor[(64), float32], %v_param_85: Tensor[(64, 128, 3, 3), float32], %v_param_86: Tensor[(64), float32], %v_param_87: Tensor[(64), float32], %v_param_88: Tensor[(64), float32], %v_param_89: Tensor[(64), float32], %v_param_90: Tensor[(64), float32], %v_param_91: Tensor[(64, 64, 3, 3), float32], %v_param_92: Tensor[(64), float32], %v_param_93: Tensor[(64), float32], %v_param_94: Tensor[(64), float32], %v_param_95: Tensor[(64), float32], %v_param_96: Tensor[(64), float32], %v_param_97: Tensor[(64, 32, 3, 3), float32], %v_param_98: Tensor[(32), float32], %v_param_99: Tensor[(32), float32], %v_param_100: Tensor[(32), float32], %v_param_101: Tensor[(32), float32], %v_param_102: Tensor[(32), float32], %v_param_103: Tensor[(32, 64, 3, 3), float32], %v_param_104: Tensor[(32), float32], %v_param_105: Tensor[(32), float32], %v_param_106: Tensor[(32), float32], %v_param_107: Tensor[(32), float32], %v_param_108: Tensor[(32), float32], %v_param_109: Tensor[(32, 32, 3, 3), float32], %v_param_110: Tensor[(32), float32], %v_param_111: Tensor[(32), float32], %v_param_112: Tensor[(32), float32], %v_param_113: Tensor[(32), float32], %v_param_114: Tensor[(32), float32], %v_param_115: Tensor[(32, 16, 3, 3), float32], %v_param_116: Tensor[(16), float32], %v_param_117: Tensor[(16), float32], %v_param_118: Tensor[(16), float32], %v_param_119: Tensor[(16), float32], %v_param_120: Tensor[(16), float32], %v_param_121: Tensor[(16, 32, 3, 3), float32], %v_param_122: Tensor[(16), float32], %v_param_123: Tensor[(16), float32], %v_param_124: Tensor[(16), float32], %v_param_125: Tensor[(16), float32], %v_param_126: Tensor[(16), float32], %v_param_127: Tensor[(16, 16, 3, 3), float32], %v_param_128: Tensor[(16), float32], %v_param_129: Tensor[(16), float32], %v_param_130: Tensor[(16), float32], %v_param_131: Tensor[(16), float32], %v_param_132: Tensor[(16), float32], %v_param_133: Tensor[(1, 16, 3, 3), float32], %v_param_134: Tensor[(1), float32]) {\n",
      "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %1 = nn.bias_add(%0, %v_param_2);\n",
      "  %2 = nn.batch_norm(%1, %v_param_3, %v_param_4, %v_param_5, %v_param_6, epsilon=0.001f);\n",
      "  %3 = %2.0;\n",
      "  %4 = nn.leaky_relu(%3, alpha=0.2f);\n",
      "  %5 = nn.conv2d(%4, %v_param_7, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %6 = nn.bias_add(%5, %v_param_8);\n",
      "  %7 = nn.batch_norm(%6, %v_param_9, %v_param_10, %v_param_11, %v_param_12, epsilon=0.001f);\n",
      "  %8 = %7.0;\n",
      "  %9 = nn.max_pool2d(%8, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %10 = nn.conv2d(%9, %v_param_13, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %11 = nn.bias_add(%10, %v_param_14);\n",
      "  %12 = nn.batch_norm(%11, %v_param_15, %v_param_16, %v_param_17, %v_param_18, epsilon=0.001f);\n",
      "  %13 = %12.0;\n",
      "  %14 = nn.leaky_relu(%13, alpha=0.2f);\n",
      "  %15 = nn.conv2d(%14, %v_param_19, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %16 = nn.bias_add(%15, %v_param_20);\n",
      "  %17 = nn.batch_norm(%16, %v_param_21, %v_param_22, %v_param_23, %v_param_24, epsilon=0.001f);\n",
      "  %18 = %17.0;\n",
      "  %19 = nn.max_pool2d(%18, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %20 = nn.conv2d(%19, %v_param_25, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %21 = nn.bias_add(%20, %v_param_26);\n",
      "  %22 = nn.batch_norm(%21, %v_param_27, %v_param_28, %v_param_29, %v_param_30, epsilon=0.001f);\n",
      "  %23 = %22.0;\n",
      "  %24 = nn.leaky_relu(%23, alpha=0.2f);\n",
      "  %25 = nn.conv2d(%24, %v_param_31, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %26 = nn.bias_add(%25, %v_param_32);\n",
      "  %27 = nn.batch_norm(%26, %v_param_33, %v_param_34, %v_param_35, %v_param_36, epsilon=0.001f);\n",
      "  %28 = %27.0;\n",
      "  %29 = nn.max_pool2d(%28, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %30 = nn.conv2d(%29, %v_param_37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %31 = nn.bias_add(%30, %v_param_38);\n",
      "  %32 = nn.batch_norm(%31, %v_param_39, %v_param_40, %v_param_41, %v_param_42, epsilon=0.001f);\n",
      "  %33 = %32.0;\n",
      "  %34 = nn.leaky_relu(%33, alpha=0.2f);\n",
      "  %35 = nn.conv2d(%34, %v_param_43, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %36 = nn.bias_add(%35, %v_param_44);\n",
      "  %37 = nn.batch_norm(%36, %v_param_45, %v_param_46, %v_param_47, %v_param_48, epsilon=0.001f);\n",
      "  %38 = %37.0;\n",
      "  %39 = nn.max_pool2d(%38, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
      "  %40 = nn.conv2d(%39, %v_param_49, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %41 = nn.bias_add(%40, %v_param_50);\n",
      "  %42 = nn.batch_norm(%41, %v_param_51, %v_param_52, %v_param_53, %v_param_54, epsilon=0.001f);\n",
      "  %43 = %42.0;\n",
      "  %44 = nn.leaky_relu(%43, alpha=0.2f);\n",
      "  %45 = nn.conv2d(%44, %v_param_55, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
      "  %46 = nn.bias_add(%45, %v_param_56);\n",
      "  %47 = nn.batch_norm(%46, %v_param_57, %v_param_58, %v_param_59, %v_param_60, epsilon=0.001f);\n",
      "  %48 = %47.0;\n",
      "  %49 = nn.conv2d_transpose(%48, %v_param_61, channels=128, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
      "  %50 = nn.bias_add(%49, %v_param_62);\n",
      "  %51 = nn.batch_norm(%50, %v_param_63, %v_param_64, %v_param_65, %v_param_66, epsilon=0.001f);\n",
      "  %52 = %51.0;\n",
      "  %53 = nn.leaky_relu(%52, alpha=0.2f);\n",
      "  %54 = (%53, %38);\n",
      "  %55 = concatenate(%54, axis=1);\n",
      "  %56 = nn.conv2d(%55, %v_param_67, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %57 = nn.bias_add(%56, %v_param_68);\n",
      "  %58 = nn.batch_norm(%57, %v_param_69, %v_param_70, %v_param_71, %v_param_72, epsilon=0.001f);\n",
      "  %59 = %58.0;\n",
      "  %60 = nn.leaky_relu(%59, alpha=0.2f);\n",
      "  %61 = nn.conv2d(%60, %v_param_73, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
      "  %62 = nn.bias_add(%61, %v_param_74);\n",
      "  %63 = nn.batch_norm(%62, %v_param_75, %v_param_76, %v_param_77, %v_param_78, epsilon=0.001f);\n",
      "  %64 = %63.0;\n",
      "  %65 = nn.conv2d_transpose(%64, %v_param_79, channels=64, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
      "  %66 = nn.bias_add(%65, %v_param_80);\n",
      "  %67 = nn.batch_norm(%66, %v_param_81, %v_param_82, %v_param_83, %v_param_84, epsilon=0.001f);\n",
      "  %68 = %67.0;\n",
      "  %69 = nn.leaky_relu(%68, alpha=0.2f);\n",
      "  %70 = (%69, %28);\n",
      "  %71 = concatenate(%70, axis=1);\n",
      "  %72 = nn.conv2d(%71, %v_param_85, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %73 = nn.bias_add(%72, %v_param_86);\n",
      "  %74 = nn.batch_norm(%73, %v_param_87, %v_param_88, %v_param_89, %v_param_90, epsilon=0.001f);\n",
      "  %75 = %74.0;\n",
      "  %76 = nn.leaky_relu(%75, alpha=0.2f);\n",
      "  %77 = nn.conv2d(%76, %v_param_91, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
      "  %78 = nn.bias_add(%77, %v_param_92);\n",
      "  %79 = nn.batch_norm(%78, %v_param_93, %v_param_94, %v_param_95, %v_param_96, epsilon=0.001f);\n",
      "  %80 = %79.0;\n",
      "  %81 = nn.conv2d_transpose(%80, %v_param_97, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
      "  %82 = nn.bias_add(%81, %v_param_98);\n",
      "  %83 = nn.batch_norm(%82, %v_param_99, %v_param_100, %v_param_101, %v_param_102, epsilon=0.001f);\n",
      "  %84 = %83.0;\n",
      "  %85 = nn.leaky_relu(%84, alpha=0.2f);\n",
      "  %86 = (%85, %18);\n",
      "  %87 = concatenate(%86, axis=1);\n",
      "  %88 = nn.conv2d(%87, %v_param_103, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %89 = nn.bias_add(%88, %v_param_104);\n",
      "  %90 = nn.batch_norm(%89, %v_param_105, %v_param_106, %v_param_107, %v_param_108, epsilon=0.001f);\n",
      "  %91 = %90.0;\n",
      "  %92 = nn.leaky_relu(%91, alpha=0.2f);\n",
      "  %93 = nn.conv2d(%92, %v_param_109, padding=[1, 1, 1, 1], channels=32, kernel_size=[3, 3]);\n",
      "  %94 = nn.bias_add(%93, %v_param_110);\n",
      "  %95 = nn.batch_norm(%94, %v_param_111, %v_param_112, %v_param_113, %v_param_114, epsilon=0.001f);\n",
      "  %96 = %95.0;\n",
      "  %97 = nn.conv2d_transpose(%96, %v_param_115, channels=16, kernel_size=[3, 3], strides=[2, 2], padding=[0, 0, 1, 1], kernel_layout=\"IOHW\");\n",
      "  %98 = nn.bias_add(%97, %v_param_116);\n",
      "  %99 = nn.batch_norm(%98, %v_param_117, %v_param_118, %v_param_119, %v_param_120, epsilon=0.001f);\n",
      "  %100 = %99.0;\n",
      "  %101 = nn.leaky_relu(%100, alpha=0.2f);\n",
      "  %102 = (%101, %8);\n",
      "  %103 = concatenate(%102, axis=1);\n",
      "  %104 = nn.conv2d(%103, %v_param_121, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %105 = nn.bias_add(%104, %v_param_122);\n",
      "  %106 = nn.batch_norm(%105, %v_param_123, %v_param_124, %v_param_125, %v_param_126, epsilon=0.001f);\n",
      "  %107 = %106.0;\n",
      "  %108 = nn.leaky_relu(%107, alpha=0.2f);\n",
      "  %109 = nn.conv2d(%108, %v_param_127, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %110 = nn.bias_add(%109, %v_param_128);\n",
      "  %111 = nn.batch_norm(%110, %v_param_129, %v_param_130, %v_param_131, %v_param_132, epsilon=0.001f);\n",
      "  %112 = %111.0;\n",
      "  %113 = nn.conv2d(%112, %v_param_133, padding=[1, 1, 1, 1], channels=1, kernel_size=[3, 3]);\n",
      "  %114 = nn.bias_add(%113, %v_param_134);\n",
      "  sigmoid(%114)\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_conv2d = is_op(\"nn.conv2d\")(wildcard(), wildcard())\n",
    "is_max_pool2d = is_op(\"nn.max_pool2d\")(wildcard(), wildcard())\n",
    "is_batch_norm = is_op(\"nn.batch_norm\")(wildcard(), wildcard(), wildcard(), wildcard(), wildcard())\n",
    "is_leaky_relu = is_op(\"nn.leaky_relu\")(wildcard())\n",
    "is_sigmoid = is_op(\"sigmoid\")(wildcard())\n",
    "# var2 = wildcard()\n",
    "# tuple_node = is_tuple([wildcard(), var2])\n",
    "is_concat = is_op('concatenate')(wildcard())\n",
    "is_bias_add = is_op(\"nn.bias_add\")(wildcard(), wildcard())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "var2 = wildcard()\n",
    "tuple_node = is_tuple([wildcard(), var2])\n",
    "concat_node = is_op('concatenate')(tuple_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamond = dominates(is_conv2d, wildcard(), is_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = relay.var(\"input\")\n",
    "weight = relay.var(\"weight\")\n",
    "bias = relay.var(\"bias\")\n",
    "conv2d = relay.op.nn.conv2d(inp, weight)\n",
    "relu = relay.op.nn.relu(conv2d)\n",
    "leaky_relu = relay.op.nn.leaky_relu(relu, alpha=0.2)\n",
    "badd = relay.op.nn.bias_add(leaky_relu, bias)\n",
    "relu = relay.op.nn.relu(badd)\n",
    "leaky_relu = relay.op.nn.leaky_relu(relu, alpha=0.2)\n",
    "badd = relay.op.nn.bias_add(leaky_relu, bias)\n",
    "relu = relay.op.nn.relu(badd)\n",
    "leaky_relu = relay.op.nn.leaky_relu(relu, alpha=0.2)\n",
    "badd = relay.op.nn.bias_add(leaky_relu, bias)\n",
    "relu = relay.op.nn.relu(badd)\n",
    "relu = relay.op.nn.relu(relu)\n",
    "out = relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CallNode(Op(nn.relu), [CallNode(Op(nn.relu), [CallNode(Op(nn.leaky_relu), [CallNode(Op(nn.relu), [CallNode(Op(nn.leaky_relu), [CallNode(Op(nn.relu), [CallNode(Op(nn.leaky_relu), [CallNode(Op(nn.relu), [CallNode(Op(nn.conv2d), [Var(input), Var(weight)], relay.attrs.Conv2DAttrs(0x1c148a78), [])], (nullptr), [])], relay.attrs.LeakyReluAttrs(0x4a3a2e8), [])], (nullptr), [])], relay.attrs.LeakyReluAttrs(0x1b7c67d8), [])], (nullptr), [])], relay.attrs.LeakyReluAttrs(0x643d508), [])], (nullptr), [])], (nullptr), [])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input;\n",
      "free_var %weight;\n",
      "%0 = nn.conv2d(%input, %weight, padding=[0, 0, 0, 0]);\n",
      "%1 = nn.relu(%0);\n",
      "%2 = nn.leaky_relu(%1, alpha=0.2f);\n",
      "free_var %bias;\n",
      "%3 = nn.bias_add(%2, %bias);\n",
      "%4 = nn.relu(%3);\n",
      "%5 = nn.leaky_relu(%4, alpha=0.2f);\n",
      "%6 = nn.bias_add(%5, %bias);\n",
      "%7 = nn.relu(%6);\n",
      "%8 = nn.leaky_relu(%7, alpha=0.2f);\n",
      "%9 = nn.bias_add(%8, %bias);\n",
      "%10 = nn.relu(%9);\n",
      "nn.relu(%10)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_conv2d = is_op(\"nn.conv2d\")(wildcard(), wildcard())\n",
    "# is_unary_elemwise = (wildcard().has_attr({\"TOpPattern\": K_ELEMWISE}))(wildcard())\n",
    "# reduction = is_op(\"add\")(wildcard(), wildcard())\n",
    "is_leaky_relu = is_op(\"nn.leaky_relu\")(wildcard())\n",
    "is_relu = is_op(\"nn.relu\")(wildcard())\n",
    "is_bias_add = is_op(\"nn.bias_add\")(wildcard(), wildcard())\n",
    "# diamond = dominates(is_conv2d, wildcard(), reduction)\n",
    "diamond = dominates(is_conv2d, wildcard(), is_leaky_relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamond.match(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input;\n",
      "free_var %weight;\n",
      "%0 = nn.conv2d(%input, %weight, padding=[0, 0, 0, 0]);\n",
      "%1 = nn.relu(%0);\n",
      "%2 = nn.leaky_relu(%1, alpha=0.2f);\n",
      "free_var %bias;\n",
      "%3 = nn.bias_add(%2, %bias);\n",
      "%4 = nn.relu(%3);\n",
      "%5 = nn.leaky_relu(%4, alpha=0.2f);\n",
      "%6 = nn.bias_add(%5, %bias);\n",
      "%7 = nn.relu(%6);\n",
      "%8 = nn.leaky_relu(%7, alpha=0.2f);\n",
      "%9 = nn.bias_add(%8, %bias);\n",
      "%10 = nn.relu(%9);\n",
      "nn.relu(%10)\n",
      "===================\n",
      "free_var %input;\n",
      "free_var %weight;\n",
      "%2 = fn (%FunctionVar_2_0, %FunctionVar_2_1, PartitionedFromPattern=\"nn.conv2d_nn.relu_nn.leaky_relu_\") {\n",
      "  %0 = nn.conv2d(%FunctionVar_2_0, %FunctionVar_2_1, padding=[0, 0, 0, 0]);\n",
      "  %1 = nn.relu(%0);\n",
      "  nn.leaky_relu(%1, alpha=0.2f)\n",
      "};\n",
      "%3 = %2(%input, %weight);\n",
      "free_var %bias;\n",
      "%4 = nn.bias_add(%3, %bias);\n",
      "%5 = nn.relu(%4);\n",
      "%6 = nn.leaky_relu(%5, alpha=0.2f);\n",
      "%7 = nn.bias_add(%6, %bias);\n",
      "%8 = nn.relu(%7);\n",
      "%9 = nn.leaky_relu(%8, alpha=0.2f);\n",
      "%10 = nn.bias_add(%9, %bias);\n",
      "%11 = nn.relu(%10);\n",
      "nn.relu(%11)\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "print(\"===================\")\n",
    "print(diamond.partition(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = relay.var('input')\n",
    "weight = relay.var('weight')\n",
    "conv2d = relay.op.nn.conv2d(inp, weight)\n",
    "relu = relay.op.nn.relu(conv2d)\n",
    "leaky_relu = relay.op.nn.leaky_relu(conv2d, alpha=0)\n",
    "out = relu - leaky_relu\n",
    "conv2d = relay.op.nn.conv2d(out, weight)\n",
    "relu = relay.op.nn.relu(conv2d)\n",
    "leaky_relu = relay.op.nn.leaky_relu(conv2d, alpha=0)\n",
    "out = relu + leaky_relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input;\n",
      "free_var %weight;\n",
      "%0 = nn.conv2d(%input, %weight, padding=[0, 0, 0, 0]);\n",
      "%1 = nn.relu(%0);\n",
      "%2 = nn.leaky_relu(%0, alpha=0f);\n",
      "%3 = subtract(%1, %2);\n",
      "%4 = nn.conv2d(%3, %weight, padding=[0, 0, 0, 0]);\n",
      "%5 = nn.relu(%4);\n",
      "%6 = nn.leaky_relu(%4, alpha=0f);\n",
      "add(%5, %6)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_conv2d = is_op('nn.conv2d')(is_var(), is_var())\n",
    "reduction = is_op('add')(wildcard(), wildcard())\n",
    "# diamond = dominates(is_conv2d,  is_op('subtract')(wildcard(), wildcard()), reduction)\n",
    "diamond = dominates(is_conv2d, wildcard(), reduction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamond.match(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %input;\n",
      "%7 = fn (%FunctionVar_0_0, PartitionedFromPattern=\"nn.conv2d_nn.relu_nn.leaky_relu_subtract_nn.conv2d_nn.relu_nn.leaky_relu_add_\") {\n",
      "  free_var %weight;\n",
      "  %0 = nn.conv2d(%FunctionVar_0_0, %weight, padding=[0, 0, 0, 0]);\n",
      "  %1 = nn.relu(%0);\n",
      "  %2 = nn.leaky_relu(%0, alpha=0f);\n",
      "  %3 = subtract(%1, %2);\n",
      "  %4 = nn.conv2d(%3, %weight, padding=[0, 0, 0, 0]);\n",
      "  %5 = nn.relu(%4);\n",
      "  %6 = nn.leaky_relu(%4, alpha=0f);\n",
      "  add(%5, %6)\n",
      "};\n",
      "%7(%input)\n"
     ]
    }
   ],
   "source": [
    "print(diamond.partition(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
