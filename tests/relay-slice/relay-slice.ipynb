{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jd/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = \"true\"\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.relay.dataflow_pattern import rewrite\n",
    "from tvm.relay.dataflow_pattern import *\n",
    "import numpy as np\n",
    "from test_model import Model\n",
    "from tvm.relay import transform\n",
    "from tvm.relay.testing import run_opt_pass\n",
    "from tvm.relay import transform, build_module\n",
    "from tvm.relay import testing\n",
    "import tvm.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network():\n",
    "    img_size = 8\n",
    "    out_channels = 16\n",
    "    batch_size = 1\n",
    "    data = relay.var(\"data\", relay.TensorType((batch_size, 3, img_size, img_size), \"float16\"))\n",
    "    dense_weight = relay.var(\n",
    "        \"dweight\", relay.TensorType((batch_size, 16 * img_size * img_size), \"float16\")\n",
    "    )\n",
    "    weight = relay.var(\"weight\")\n",
    "    second_weight = relay.var(\"second_weight\")\n",
    "    bn_gamma = relay.var(\"bn_gamma\")\n",
    "    bn_beta = relay.var(\"bn_beta\")\n",
    "    bn_mmean = relay.var(\"bn_mean\")\n",
    "    bn_mvar = relay.var(\"bn_var\")\n",
    "    simple_net = relay.nn.conv2d(\n",
    "        data=data, weight=weight, kernel_size=(3, 3), channels=out_channels, padding=(1, 1)\n",
    "    )\n",
    "    simple_net = relay.nn.batch_norm(simple_net, bn_gamma, bn_beta, bn_mmean, bn_mvar)[0]\n",
    "    simple_net = relay.nn.relu(simple_net)\n",
    "    simple_net = relay.nn.batch_flatten(simple_net)\n",
    "    simple_net = relay.nn.dense(simple_net, dense_weight)\n",
    "    simple_net = relay.Function(relay.analysis.free_vars(simple_net), simple_net)\n",
    "    data_shape = (batch_size, 3, img_size, img_size)\n",
    "    net, params = testing.create_workload(simple_net)\n",
    "    return net, params, data_shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%data: Tensor[(1, 3, 8, 8), float16] /* ty=Tensor[(1, 3, 8, 8), float16] */, %weight: Tensor[(16, 3, 3, 3), float16] /* ty=Tensor[(16, 3, 3, 3), float16] */, %bn_gamma: Tensor[(16), float16] /* ty=Tensor[(16), float16] */, %bn_beta: Tensor[(16), float16] /* ty=Tensor[(16), float16] */, %bn_mean: Tensor[(16), float16] /* ty=Tensor[(16), float16] */, %bn_var: Tensor[(16), float16] /* ty=Tensor[(16), float16] */, %dweight: Tensor[(1, 1024), float16] /* ty=Tensor[(1, 1024), float16] */) -> Tensor[(2, 1), float16] {\n",
      "  %0 = nn.conv2d(%data, %weight, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "  %1 = nn.conv2d(%data, %weight, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "  %2 = (%0, %1) /* ty=(Tensor[(1, 16, 8, 8), float16], Tensor[(1, 16, 8, 8), float16]) */;\n",
      "  %3 = concatenate(%2) /* ty=Tensor[(2, 16, 8, 8), float16] */;\n",
      "  %4 = nn.batch_norm(%3, %bn_gamma, %bn_beta, %bn_mean, %bn_var) /* ty=(Tensor[(2, 16, 8, 8), float16], Tensor[(16), float16], Tensor[(16), float16]) */;\n",
      "  %5 = %4.0 /* ty=Tensor[(2, 16, 8, 8), float16] */;\n",
      "  %6 = nn.relu(%5) /* ty=Tensor[(2, 16, 8, 8), float16] */;\n",
      "  %7 = nn.batch_flatten(%6) /* ty=Tensor[(2, 1024), float16] */;\n",
      "  nn.dense(%7, %dweight, units=None) /* ty=Tensor[(2, 1), float16] */\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_skip_net():\n",
    "    img_size = 8\n",
    "    out_channels = 16\n",
    "    batch_size = 1\n",
    "    data = relay.var(\"data\", relay.TensorType((batch_size, 3, img_size, img_size), \"float16\"))\n",
    "    dense_weight = relay.var(\n",
    "        \"dweight\", relay.TensorType((batch_size, 16 * img_size * img_size), \"float16\")\n",
    "    )\n",
    "    weight = relay.var(\"weight\")\n",
    "    second_weight = relay.var(\"second_weight\")\n",
    "    bn_gamma = relay.var(\"bn_gamma\")\n",
    "    bn_beta = relay.var(\"bn_beta\")\n",
    "    bn_mmean = relay.var(\"bn_mean\")\n",
    "    bn_mvar = relay.var(\"bn_var\")\n",
    "    x0 = relay.nn.conv2d(\n",
    "        data=data, weight=weight, kernel_size=(3, 3), channels=out_channels, padding=(1, 1)\n",
    "    )\n",
    "    x1 = relay.nn.conv2d(\n",
    "        data=data, weight=weight, kernel_size=(3, 3), channels=out_channels, padding=(1, 1)\n",
    "    )\n",
    "    simple_net = relay.op.concatenate(relay.Tuple([x0, x1]), axis=0)\n",
    "    simple_net = relay.nn.batch_norm(simple_net, bn_gamma, bn_beta, bn_mmean, bn_mvar)[0]\n",
    "    simple_net = relay.nn.relu(simple_net)\n",
    "    simple_net = relay.nn.batch_flatten(simple_net)\n",
    "    simple_net = relay.nn.dense(simple_net, dense_weight)\n",
    "    simple_net = relay.Function(relay.analysis.free_vars(simple_net), simple_net)\n",
    "    data_shape = (batch_size, 3, img_size, img_size)\n",
    "    net, params = testing.create_workload(simple_net)\n",
    "    return net, params, data_shape\n",
    "\n",
    "print(get_skip_net()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = get_skip_net()[0]['main'].body\n",
    "body = run_opt_pass(body, transform.ToANormalForm())\n",
    "body = run_opt_pass(body, transform.InferType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LetNode(Var(x_51), CallNode(Op(nn.conv2d), [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], relay.attrs.Conv2DAttrs(0x6d1fb98), [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)]), LetNode(Var(x_52), CallNode(Op(nn.conv2d), [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], relay.attrs.Conv2DAttrs(0x6c03a68), [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)]), LetNode(Var(x_53), Tuple([Var(x_51), Var(x_52)]), LetNode(Var(x_54), CallNode(Op(concatenate), [Var(x_53)], relay.attrs.ConcatenateAttrs(0x6cbe958), [TupleTypeNode([TensorType([1, 16, 8, 8], float16), TensorType([1, 16, 8, 8], float16)])]), LetNode(Var(x_55), CallNode(Op(nn.batch_norm), [Var(x_54), Var(bn_gamma, ty=TensorType([16], float16)), Var(bn_beta, ty=TensorType([16], float16)), Var(bn_mean, ty=TensorType([16], float16)), Var(bn_var, ty=TensorType([16], float16))], relay.attrs.BatchNormAttrs(0x6bfd428), [TensorType([2, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16)]), LetNode(Var(x_56), TupleGetItemNode(Var(x_55), 0), LetNode(Var(x_57), CallNode(Op(nn.relu), [Var(x_56)], (nullptr), [TensorType([2, 16, 8, 8], float16)]), LetNode(Var(x_58), CallNode(Op(nn.batch_flatten), [Var(x_57)], (nullptr), [TensorType([2, 16, 8, 8], float16)]), LetNode(Var(x_59), CallNode(Op(nn.dense), [Var(x_58), Var(dweight, ty=TensorType([1, 1024], float16))], relay.attrs.DenseAttrs(0x6bf3428), [TensorType([2, 1024], float16), TensorType([1, 1024], float16)]), Var(x_59))))))))))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LetNode(\n",
    "    Var(x_51), \n",
    "    CallNode(Op(nn.conv2d), [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], relay.attrs.Conv2DAttrs(0x6d1fb98), [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)]), \n",
    "    LetNode(\n",
    "        Var(x_52), \n",
    "        CallNode(Op(nn.conv2d), [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], relay.attrs.Conv2DAttrs(0x6c03a68), [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)]), \n",
    "        LetNode(\n",
    "            Var(x_53), \n",
    "            Tuple([Var(x_51), Var(x_52)]), \n",
    "            LetNode(\n",
    "                Var(x_54), \n",
    "                CallNode(Op(concatenate), [Var(x_53)], relay.attrs.ConcatenateAttrs(0x6cbe958), [TupleTypeNode([TensorType([1, 16, 8, 8], float16), TensorType([1, 16, 8, 8], float16)])]), \n",
    "                LetNode(\n",
    "                    Var(x_55), \n",
    "                    CallNode(Op(nn.batch_norm), [Var(x_54), Var(bn_gamma, ty=TensorType([16], float16)), Var(bn_beta, ty=TensorType([16], float16)), Var(bn_mean, ty=TensorType([16], float16)), Var(bn_var, ty=TensorType([16], float16))], relay.attrs.BatchNormAttrs(0x6bfd428), [TensorType([2, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16)]), \n",
    "                    LetNode(Var(x_56), TupleGetItemNode(Var(x_55), 0), LetNode(Var(x_57), CallNode(Op(nn.relu), [Var(x_56)], (nullptr), [TensorType([2, 16, 8, 8], float16)]), LetNode(Var(x_58), CallNode(Op(nn.batch_flatten), [Var(x_57)], (nullptr), [TensorType([2, 16, 8, 8], float16)]), LetNode(Var(x_59), CallNode(Op(nn.dense), [Var(x_58), Var(dweight, ty=TensorType([1, 1024], float16))], relay.attrs.DenseAttrs(0x6bf3428), [TensorType([2, 1024], float16), TensorType([1, 1024], float16)]), Var(x_59))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LetNode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m LetNode(\n\u001b[1;32m      2\u001b[0m     Var(x_42, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), \n\u001b[1;32m      3\u001b[0m     CallNode(Op(nn\u001b[39m.\u001b[39mconv2d), [Var(data, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), Var(weight, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m16\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m], float16))], relay\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mConv2DAttrs(\u001b[39m0x6c7b508\u001b[39m), [TensorType([\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m], float16)]\n\u001b[1;32m      4\u001b[0m     ), \n\u001b[1;32m      5\u001b[0m     LetNode(\n\u001b[1;32m      6\u001b[0m         Var(x_43, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), \n\u001b[1;32m      7\u001b[0m         CallNode(Op(nn\u001b[39m.\u001b[39mconv2d), [Var(data, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), Var(weight, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m16\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m], float16))], relay\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mConv2DAttrs(\u001b[39m0x6c2a268\u001b[39m), [TensorType([\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m], float16)]\n\u001b[1;32m      8\u001b[0m         ), \n\u001b[1;32m      9\u001b[0m         LetNode(\n\u001b[1;32m     10\u001b[0m             Var(x_44, ty\u001b[39m=\u001b[39mTupleTypeNode([TensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16), TensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)])), \n\u001b[1;32m     11\u001b[0m             Tuple([Var(x_42, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), Var(x_43, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16))]\n\u001b[1;32m     12\u001b[0m             ), \n\u001b[1;32m     13\u001b[0m             LetNode(\n\u001b[1;32m     14\u001b[0m                 Var(x_45, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m2\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), \n\u001b[1;32m     15\u001b[0m                 CallNode(Op(concatenate), [Var(x_44, ty\u001b[39m=\u001b[39mTupleTypeNode([TensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16), TensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)]))], relay\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mConcatenateAttrs(\u001b[39m0x6c938b8\u001b[39m), [TupleTypeNode([TensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16), TensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)])]\n\u001b[1;32m     16\u001b[0m                 ), \n\u001b[1;32m     17\u001b[0m                 LetNode(\n\u001b[1;32m     18\u001b[0m                     Var(x_46, ty\u001b[39m=\u001b[39mTupleTypeNode([TensorType([\u001b[39m2\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16)])), \n\u001b[1;32m     19\u001b[0m                     CallNode(Op(nn\u001b[39m.\u001b[39mbatch_norm), [Var(x_45, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m2\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), Var(bn_gamma, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m16\u001b[39m], float16)), Var(bn_beta, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m16\u001b[39m], float16)), Var(bn_mean, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m16\u001b[39m], float16)), Var(bn_var, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m16\u001b[39m], float16))], relay\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mBatchNormAttrs(\u001b[39m0x6cb11b8\u001b[39m), [TensorType([\u001b[39m2\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16)]\n\u001b[1;32m     20\u001b[0m                     ), \n\u001b[1;32m     21\u001b[0m                     LetNode(Var(x_47, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m2\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), TupleGetItemNode(Var(x_46, ty\u001b[39m=\u001b[39mTupleTypeNode([TensorType([\u001b[39m2\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16)])), \u001b[39m0\u001b[39m), LetNode(Var(x_48, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m2\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), CallNode(Op(nn\u001b[39m.\u001b[39mrelu), [Var(x_47, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m2\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16))], (nullptr), [TensorType([\u001b[39m2\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)]), LetNode(Var(x_49, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m2\u001b[39m, \u001b[39m1024\u001b[39m], float16)), CallNode(Op(nn\u001b[39m.\u001b[39mbatch_flatten), [Var(x_48, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m2\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16))], (nullptr), [TensorType([\u001b[39m2\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)]), LetNode(Var(x_50, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m], float16)), CallNode(Op(nn\u001b[39m.\u001b[39mdense), [Var(x_49, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m2\u001b[39m, \u001b[39m1024\u001b[39m], float16)), Var(dweight, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m], float16))], relay\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mDenseAttrs(\u001b[39m0x6b79f78\u001b[39m), [TensorType([\u001b[39m2\u001b[39m, \u001b[39m1024\u001b[39m], float16), TensorType([\u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m], float16)]), Var(x_50, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m], float16)))))))))))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LetNode' is not defined"
     ]
    }
   ],
   "source": [
    "LetNode(\n",
    "    Var(x_42, ty=TensorType([1, 16, 8, 8], float16)), \n",
    "    CallNode(Op(nn.conv2d), [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], relay.attrs.Conv2DAttrs(0x6c7b508), [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)]\n",
    "    ), \n",
    "    LetNode(\n",
    "        Var(x_43, ty=TensorType([1, 16, 8, 8], float16)), \n",
    "        CallNode(Op(nn.conv2d), [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], relay.attrs.Conv2DAttrs(0x6c2a268), [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)]\n",
    "        ), \n",
    "        LetNode(\n",
    "            Var(x_44, ty=TupleTypeNode([TensorType([1, 16, 8, 8], float16), TensorType([1, 16, 8, 8], float16)])), \n",
    "            Tuple([Var(x_42, ty=TensorType([1, 16, 8, 8], float16)), Var(x_43, ty=TensorType([1, 16, 8, 8], float16))]\n",
    "            ), \n",
    "            LetNode(\n",
    "                Var(x_45, ty=TensorType([2, 16, 8, 8], float16)), \n",
    "                CallNode(Op(concatenate), [Var(x_44, ty=TupleTypeNode([TensorType([1, 16, 8, 8], float16), TensorType([1, 16, 8, 8], float16)]))], relay.attrs.ConcatenateAttrs(0x6c938b8), [TupleTypeNode([TensorType([1, 16, 8, 8], float16), TensorType([1, 16, 8, 8], float16)])]\n",
    "                ), \n",
    "                LetNode(\n",
    "                    Var(x_46, ty=TupleTypeNode([TensorType([2, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16)])), \n",
    "                    CallNode(Op(nn.batch_norm), [Var(x_45, ty=TensorType([2, 16, 8, 8], float16)), Var(bn_gamma, ty=TensorType([16], float16)), Var(bn_beta, ty=TensorType([16], float16)), Var(bn_mean, ty=TensorType([16], float16)), Var(bn_var, ty=TensorType([16], float16))], relay.attrs.BatchNormAttrs(0x6cb11b8), [TensorType([2, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16)]\n",
    "                    ), \n",
    "                    LetNode(Var(x_47, ty=TensorType([2, 16, 8, 8], float16)), TupleGetItemNode(Var(x_46, ty=TupleTypeNode([TensorType([2, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16)])), 0), LetNode(Var(x_48, ty=TensorType([2, 16, 8, 8], float16)), CallNode(Op(nn.relu), [Var(x_47, ty=TensorType([2, 16, 8, 8], float16))], (nullptr), [TensorType([2, 16, 8, 8], float16)]), LetNode(Var(x_49, ty=TensorType([2, 1024], float16)), CallNode(Op(nn.batch_flatten), [Var(x_48, ty=TensorType([2, 16, 8, 8], float16))], (nullptr), [TensorType([2, 16, 8, 8], float16)]), LetNode(Var(x_50, ty=TensorType([2, 1], float16)), CallNode(Op(nn.dense), [Var(x_49, ty=TensorType([2, 1024], float16)), Var(dweight, ty=TensorType([1, 1024], float16))], relay.attrs.DenseAttrs(0x6b79f78), [TensorType([2, 1024], float16), TensorType([1, 1024], float16)]), Var(x_50, ty=TensorType([2, 1], float16)))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[TupleGetItemNode(\n",
    "    CallNode(Op(nn.batch_norm), \n",
    "        [CallNode(Op(concatenate), \n",
    "        [Tuple([CallNode(Op(nn.conv2d), \n",
    "        [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], \n",
    "        relay.attrs.Conv2DAttrs(0x6d34998), [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)]), \n",
    "        CallNode(Op(nn.conv2d), [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], \n",
    "        relay.attrs.Conv2DAttrs(0x6bfeed8), [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)])])], \n",
    "        relay.attrs.ConcatenateAttrs(0x6c1cbe8), \n",
    "        [TupleTypeNode([TensorType([1, 16, 8, 8], float16), \n",
    "        TensorType([1, 16, 8, 8], float16)])]), Var(bn_gamma, ty=TensorType([16], float16)), Var(bn_beta, ty=TensorType([16], float16)), Var(bn_mean, ty=TensorType([16], float16)), Var(bn_var, ty=TensorType([16], float16))], relay.attrs.BatchNormAttrs(0x6c05b58), [TensorType([2, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16)]), 0)], (nullptr), [TensorType([2, 16, 8, 8], float16)])], (nullptr), [TensorType([2, 16, 8, 8], float16)]), Var(dweight, ty=TensorType([1, 1024], float16))], relay.attrs.DenseAttrs(0x6c6c018), [TensorType([2, 1024], float16), TensorType([1, 1024], float16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, params, data_shape = get_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CallNode(Op(nn.dense), [CallNode(Op(nn.batch_flatten), [CallNode(Op(nn.relu), [TupleGetItemNode(CallNode(Op(nn.batch_norm), [CallNode(Op(nn.conv2d), [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], relay.attrs.Conv2DAttrs(0x65c2968), [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)]), Var(bn_gamma, ty=TensorType([16], float16)), Var(bn_beta, ty=TensorType([16], float16)), Var(bn_mean, ty=TensorType([16], float16)), Var(bn_var, ty=TensorType([16], float16))], relay.attrs.BatchNormAttrs(0x331cbf8), [TensorType([1, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16)]), 0)], (nullptr), [TensorType([1, 16, 8, 8], float16)])], (nullptr), [TensorType([1, 16, 8, 8], float16)]), Var(dweight, ty=TensorType([1, 1024], float16))], relay.attrs.DenseAttrs(0x331d478), [TensorType([1, 1024], float16), TensorType([1, 1024], float16)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net['main'].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CallNode(Op(nn.dense), [CallNode(Op(nn.batch_flatten), [CallNode(Op(nn.relu), [TupleGetItemNode(CallNode(Op(nn.batch_norm), [CallNode(Op(nn.conv2d), [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], relay.attrs.Conv2DAttrs(0x65c2968), [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)]), Var(bn_gamma, ty=TensorType([16], float16)), Var(bn_beta, ty=TensorType([16], float16)), Var(bn_mean, ty=TensorType([16], float16)), Var(\n",
    "    bn_var, ty=TensorType([16], float16))], relay.attrs.BatchNormAttrs(0x331cbf8), [TensorType([1, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16)]), 0)], (nullptr), [TensorType([1, 16, 8, 8], float16)])], (nullptr), [TensorType([1, 16, 8, 8], float16)]), Var(dweight, ty=TensorType([1, 1024], float16))], relay.attrs.DenseAttrs(0x331d478), [TensorType([1, 1024], float16), TensorType([1, 1024], float16)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets explore\n",
    "body = net['main'].body\n",
    "body = run_opt_pass(body, transform.ToANormalForm())\n",
    "body = run_opt_pass(body, transform.InferType())\n",
    "cur = body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %data: Tensor[(1, 3, 8, 8), float16] /* ty=Tensor[(1, 3, 8, 8), float16] */;\n",
      "free_var %weight: Tensor[(16, 3, 3, 3), float16] /* ty=Tensor[(16, 3, 3, 3), float16] */;\n",
      "let %x_0: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */ = nn.conv2d(%data, %weight, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "free_var %bn_gamma: Tensor[(16), float16] /* ty=Tensor[(16), float16] */;\n",
      "free_var %bn_beta: Tensor[(16), float16] /* ty=Tensor[(16), float16] */;\n",
      "free_var %bn_mean: Tensor[(16), float16] /* ty=Tensor[(16), float16] */;\n",
      "free_var %bn_var: Tensor[(16), float16] /* ty=Tensor[(16), float16] */;\n",
      "let %x_1: (Tensor[(1, 16, 8, 8), float16], Tensor[(16), float16], Tensor[(16), float16]) /* ty=(Tensor[(1, 16, 8, 8), float16], Tensor[(16), float16], Tensor[(16), float16]) */ = nn.batch_norm(%x_0, %bn_gamma, %bn_beta, %bn_mean, %bn_var) /* ty=(Tensor[(1, 16, 8, 8), float16], Tensor[(16), float16], Tensor[(16), float16]) */;\n",
      "let %x_2: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */ = %x_1.0 /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "let %x_3: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */ = nn.relu(%x_2) /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "let %x_4: Tensor[(1, 1024), float16] /* ty=Tensor[(1, 1024), float16] */ = nn.batch_flatten(%x_3) /* ty=Tensor[(1, 1024), float16] */;\n",
      "free_var %dweight: Tensor[(1, 1024), float16] /* ty=Tensor[(1, 1024), float16] */;\n",
      "let %x_5: Tensor[(1, 1), float16] /* ty=Tensor[(1, 1), float16] */ = nn.dense(%x_4, %dweight, units=None) /* ty=Tensor[(1, 1), float16] */;\n",
      "%x_5\n"
     ]
    }
   ],
   "source": [
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %data: Tensor[(1, 3, 8, 8), float16] /* ty=Tensor[(1, 3, 8, 8), float16] */;\n",
      "free_var %weight: Tensor[(16, 3, 3, 3), float16] /* ty=Tensor[(16, 3, 3, 3), float16] */;\n",
      "nn.conv2d(%data, %weight, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 8, 8), float16] */\n",
      "<class 'tvm.relay.expr.Call'>\n"
     ]
    }
   ],
   "source": [
    "pre = cur\n",
    "cur = cur.value\n",
    "print(cur)\n",
    "print(type(cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.relay.expr.Let"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LetNode(Var(x_0, ty=TensorType([1, 16, 8, 8], float16)), CallNode(Op(nn.conv2d), [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], relay.attrs.Conv2DAttrs(0x65c2968), [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)]), LetNode(Var(x_1, ty=TupleTypeNode([TensorType([1, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16)])), CallNode(Op(nn.batch_norm), [Var(x_0, ty=TensorType([1, 16, 8, 8], float16)), Var(bn_gamma, ty=TensorType([16], float16)), Var(bn_beta, ty=TensorType([16], float16)), Var(bn_mean, ty=TensorType([16], float16)), Var(bn_var, ty=TensorType([16], float16))], relay.attrs.BatchNormAttrs(0x331cbf8), [TensorType([1, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16)]), LetNode(Var(x_2, ty=TensorType([1, 16, 8, 8], float16)), TupleGetItemNode(Var(x_1, ty=TupleTypeNode([TensorType([1, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16)])), 0), LetNode(Var(x_3, ty=TensorType([1, 16, 8, 8], float16)), CallNode(Op(nn.relu), [Var(x_2, ty=TensorType([1, 16, 8, 8], float16))], (nullptr), [TensorType([1, 16, 8, 8], float16)]), LetNode(Var(x_4, ty=TensorType([1, 1024], float16)), CallNode(Op(nn.batch_flatten), [Var(x_3, ty=TensorType([1, 16, 8, 8], float16))], (nullptr), [TensorType([1, 16, 8, 8], float16)]), LetNode(Var(x_5, ty=TensorType([1, 1], float16)), CallNode(Op(nn.dense), [Var(x_4, ty=TensorType([1, 1024], float16)), Var(dweight, ty=TensorType([1, 1024], float16))], relay.attrs.DenseAttrs(0x331d478), [TensorType([1, 1024], float16), TensorType([1, 1024], float16)]), Var(x_5, ty=TensorType([1, 1], float16))))))))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Var(x_0, ty=TensorType([1, 16, 8, 8], float16))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CallNode(Op(nn.conv2d), [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], relay.attrs.Conv2DAttrs(0x65c2968), [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LetNode(Var(x_1, ty=TupleTypeNode([TensorType([1, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16)])), CallNode(Op(nn.batch_norm), [Var(x_0, ty=TensorType([1, 16, 8, 8], float16)), Var(bn_gamma, ty=TensorType([16], float16)), Var(bn_beta, ty=TensorType([16], float16)), Var(bn_mean, ty=TensorType([16], float16)), Var(bn_var, ty=TensorType([16], float16))], relay.attrs.BatchNormAttrs(0x331cbf8), [TensorType([1, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16)]), LetNode(Var(x_2, ty=TensorType([1, 16, 8, 8], float16)), TupleGetItemNode(Var(x_1, ty=TupleTypeNode([TensorType([1, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16)])), 0), LetNode(Var(x_3, ty=TensorType([1, 16, 8, 8], float16)), CallNode(Op(nn.relu), [Var(x_2, ty=TensorType([1, 16, 8, 8], float16))], (nullptr), [TensorType([1, 16, 8, 8], float16)]), LetNode(Var(x_4, ty=TensorType([1, 1024], float16)), CallNode(Op(nn.batch_flatten), [Var(x_3, ty=TensorType([1, 16, 8, 8], float16))], (nullptr), [TensorType([1, 16, 8, 8], float16)]), LetNode(Var(x_5, ty=TensorType([1, 1], float16)), CallNode(Op(nn.dense), [Var(x_4, ty=TensorType([1, 1024], float16)), Var(dweight, ty=TensorType([1, 1024], float16))], relay.attrs.DenseAttrs(0x331d478), [TensorType([1, 1024], float16), TensorType([1, 1024], float16)]), Var(x_5, ty=TensorType([1, 1], float16)))))))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LetNode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m LetNode(\n\u001b[1;32m      2\u001b[0m     Var(x_0, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), \n\u001b[1;32m      3\u001b[0m     CallNode(\n\u001b[1;32m      4\u001b[0m         Op(nn\u001b[39m.\u001b[39mconv2d), \n\u001b[1;32m      5\u001b[0m         [Var(data, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), Var(weight, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m16\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m], float16))], \n\u001b[1;32m      6\u001b[0m         relay\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mConv2DAttrs(\u001b[39m0x65c2968\u001b[39m), \n\u001b[1;32m      7\u001b[0m         [TensorType([\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m], float16)]\n\u001b[1;32m      8\u001b[0m     ), \n\u001b[1;32m      9\u001b[0m     LetNode(\n\u001b[1;32m     10\u001b[0m         Var(x_1, ty\u001b[39m=\u001b[39mTupleTypeNode([TensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16)])), \n\u001b[1;32m     11\u001b[0m         CallNode(\n\u001b[1;32m     12\u001b[0m             Op(nn\u001b[39m.\u001b[39mbatch_norm), \n\u001b[1;32m     13\u001b[0m             [\n\u001b[1;32m     14\u001b[0m                 Var(x_0, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), \n\u001b[1;32m     15\u001b[0m                 Var(bn_gamma, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m16\u001b[39m], float16)), \n\u001b[1;32m     16\u001b[0m                 Var(bn_beta, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m16\u001b[39m], float16)), \n\u001b[1;32m     17\u001b[0m                 Var(bn_mean, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m16\u001b[39m], float16)), Var(bn_var, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m16\u001b[39m], float16))\n\u001b[1;32m     18\u001b[0m             ], \n\u001b[1;32m     19\u001b[0m             relay\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mBatchNormAttrs(\u001b[39m0x331cbf8\u001b[39m), \n\u001b[1;32m     20\u001b[0m             [TensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16)]\n\u001b[1;32m     21\u001b[0m         ), \n\u001b[1;32m     22\u001b[0m         LetNode(\n\u001b[1;32m     23\u001b[0m             Var(x_2, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), \n\u001b[1;32m     24\u001b[0m             TupleGetItemNode(Var(x_1, ty\u001b[39m=\u001b[39mTupleTypeNode([TensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16), TensorType([\u001b[39m16\u001b[39m], float16)])), \u001b[39m0\u001b[39m), \n\u001b[1;32m     25\u001b[0m             LetNode(\n\u001b[1;32m     26\u001b[0m                 Var(x_3, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)), \n\u001b[1;32m     27\u001b[0m                 CallNode(\n\u001b[1;32m     28\u001b[0m                     Op(nn\u001b[39m.\u001b[39mrelu), \n\u001b[1;32m     29\u001b[0m                     [Var(x_2, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16))], \n\u001b[1;32m     30\u001b[0m                     (nullptr), \n\u001b[1;32m     31\u001b[0m                     [TensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)]\n\u001b[1;32m     32\u001b[0m                 ), \n\u001b[1;32m     33\u001b[0m                 LetNode(Var(x_4, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m], float16)), CallNode(Op(nn\u001b[39m.\u001b[39mbatch_flatten), [Var(x_3, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16))], (nullptr), [TensorType([\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m], float16)]), LetNode(Var(x_5, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], float16)), CallNode(Op(nn\u001b[39m.\u001b[39mdense), [Var(x_4, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m], float16)), Var(dweight, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m], float16))], relay\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mDenseAttrs(\u001b[39m0x331d478\u001b[39m), [TensorType([\u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m], float16), TensorType([\u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m], float16)]), Var(x_5, ty\u001b[39m=\u001b[39mTensorType([\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], float16))))))))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LetNode' is not defined"
     ]
    }
   ],
   "source": [
    "LetNode(\n",
    "    Var(x_0, ty=TensorType([1, 16, 8, 8], float16)), \n",
    "    CallNode(\n",
    "        Op(nn.conv2d), \n",
    "        [Var(data, ty=TensorType([1, 3, 8, 8], float16)), Var(weight, ty=TensorType([16, 3, 3, 3], float16))], \n",
    "        relay.attrs.Conv2DAttrs(0x65c2968), \n",
    "        [TensorType([1, 3, 8, 8], float16), TensorType([16, 3, 3, 3], float16)]\n",
    "    ), \n",
    "    LetNode(\n",
    "        Var(x_1, ty=TupleTypeNode([TensorType([1, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16)])), \n",
    "        CallNode(\n",
    "            Op(nn.batch_norm), \n",
    "            [\n",
    "                Var(x_0, ty=TensorType([1, 16, 8, 8], float16)), \n",
    "                Var(bn_gamma, ty=TensorType([16], float16)), \n",
    "                Var(bn_beta, ty=TensorType([16], float16)), \n",
    "                Var(bn_mean, ty=TensorType([16], float16)), Var(bn_var, ty=TensorType([16], float16))\n",
    "            ], \n",
    "            relay.attrs.BatchNormAttrs(0x331cbf8), \n",
    "            [TensorType([1, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16), TensorType([16], float16)]\n",
    "        ), \n",
    "        LetNode(\n",
    "            Var(x_2, ty=TensorType([1, 16, 8, 8], float16)), \n",
    "            TupleGetItemNode(Var(x_1, ty=TupleTypeNode([TensorType([1, 16, 8, 8], float16), TensorType([16], float16), TensorType([16], float16)])), 0), \n",
    "            LetNode(\n",
    "                Var(x_3, ty=TensorType([1, 16, 8, 8], float16)), \n",
    "                CallNode(\n",
    "                    Op(nn.relu), \n",
    "                    [Var(x_2, ty=TensorType([1, 16, 8, 8], float16))], \n",
    "                    (nullptr), \n",
    "                    [TensorType([1, 16, 8, 8], float16)]\n",
    "                ), \n",
    "                LetNode(\n",
    "                    Var(x_4, ty=TensorType([1, 1024], float16)), \n",
    "                    CallNode(\n",
    "                        Op(nn.batch_flatten), \n",
    "                        [Var(x_3, ty=TensorType([1, 16, 8, 8], float16))], \n",
    "                        (nullptr), \n",
    "                        [TensorType([1, 16, 8, 8], float16)]\n",
    "                    ), \n",
    "                    LetNode(\n",
    "                        Var(x_5, ty=TensorType([1, 1], float16)), \n",
    "                        CallNode(\n",
    "                            Op(nn.dense), \n",
    "                            [Var(x_4, ty=TensorType([1, 1024], float16)), Var(dweight, ty=TensorType([1, 1024], float16))], \n",
    "                            relay.attrs.DenseAttrs(0x331d478), [TensorType([1, 1024], float16), TensorType([1, 1024], float16)]\n",
    "                        ), \n",
    "                        Var(x_5, ty=TensorType([1, 1], float16))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_split(expr, split_conf, params=None):\n",
    "    \"\"\"Splitting the graph into a list of subgraphs\"\"\"\n",
    "\n",
    "    def get_dep_var(sub_var_dep):\n",
    "        return [var for var in sub_var_dep[len(sub_var_dep) - 1][\"ref_nodes\"]]\n",
    "\n",
    "    def parse_dependency(value, snode_dep, new_input_idx):\n",
    "        new_args = []\n",
    "        need_update = False\n",
    "        for var in value.args:\n",
    "            is_free_var = False\n",
    "            for dep in snode_dep[:-1]:\n",
    "                if var in dep[\"nodes\"]:\n",
    "                    # Mark the previous subgraph node as a dependency.\n",
    "                    dep[\"nodes\"][var] += 1\n",
    "                    dep[\"ref_nodes\"][var] = dep[\"nodes\"][var]\n",
    "                    # The var of this call is a free_var\n",
    "                    is_free_var = True\n",
    "            # if the var of this call is a free_var, recreate it and give it a fixed input name.\n",
    "            if is_free_var:\n",
    "                need_update = True\n",
    "                new_args.append(relay.var(f\"data_n_{new_input_idx}\", var.checked_type))\n",
    "                new_input_idx += 1\n",
    "            else:\n",
    "                new_args.append(var)\n",
    "        # if the 'tvm.relay.expr.Call' has a free_var, recreate it with new name as 'data_n_*'.\n",
    "        if need_update:\n",
    "            value = tvm.relay.expr.Call(\n",
    "                value.op, new_args, value.attrs, value.type_args, value.span\n",
    "            )\n",
    "        return value, snode_dep, new_input_idx\n",
    "\n",
    "    def merge_constant_expr(constant_expr, expr):\n",
    "        # merge constant express with a express\n",
    "        if not isinstance(constant_expr.body, tvm.relay.expr.Let):\n",
    "            return tvm.relay.expr.Let(constant_expr.var, constant_expr.value, expr)\n",
    "\n",
    "        return tvm.relay.expr.Let(\n",
    "            constant_expr.var, constant_expr.value, merge_constant_expr(constant_expr.body, expr)\n",
    "        )\n",
    "\n",
    "    def _recursion(anf, pipeline_mods, split_conf, constant_expr):\n",
    "        # Enumurate all operators of compute graph, then split the compute graph into a group of\n",
    "        # subgraph.\n",
    "        nonlocal operator_index_map\n",
    "        nonlocal new_input_idx\n",
    "        nonlocal snode_dep\n",
    "        # Get last element in snode_dep : current node's dependency\n",
    "        cur_node_dep = snode_dep[len(snode_dep) - 1]\n",
    "        # If function -> decouple\n",
    "        if isinstance(anf, tvm.relay.Function):\n",
    "            return tvm.relay.Function(\n",
    "                anf.params,\n",
    "                _recursion(anf.body, pipeline_mods, split_conf, constant_expr),\n",
    "                anf.ret_type,\n",
    "                anf.type_params,\n",
    "                anf.attrs,\n",
    "            )\n",
    "        # Function of Let\n",
    "        if isinstance(anf, tvm.relay.expr.Let):\n",
    "            value = anf.value\n",
    "            # record the constant expr to make sure all sugraphs can find correct constant.\n",
    "            if isinstance(value, tvm.relay.expr.Constant):\n",
    "                # cosntant_expr is initally None\n",
    "                if not constant_expr:\n",
    "                    constant_expr = tvm.relay.expr.Let(anf.var, value, anf.var)\n",
    "                else:\n",
    "                    constant_expr = tvm.relay.expr.Let(anf.var, value, constant_expr)\n",
    "            if isinstance(value, tvm.relay.expr.Call):\n",
    "                new_args = []\n",
    "                # build current var list\n",
    "                cur_node_dep[\"nodes\"][anf.var] = 0\n",
    "                # Get the dependency information of the nodes.\n",
    "                value, snode_dep, new_input_idx = parse_dependency(value, snode_dep, new_input_idx)\n",
    "                if isinstance(value.op, tvm.ir.Op):\n",
    "                    if value.op.name in operator_index_map:\n",
    "                        operator_index_map[value.op.name] += 1\n",
    "                    else:\n",
    "                        operator_index_map[value.op.name] = 0\n",
    "                    split_operator_name = split_conf[0][\"op_name\"] if split_conf else \"\"\n",
    "                    split_operator_index = split_conf[0][\"op_index\"] if split_conf else \"\"\n",
    "                    # if a operator name and repeating count in the network match with the values\n",
    "                    # of the 'split configuration', then this place is where we should do the\n",
    "                    # graph splitting.\n",
    "                    if (\n",
    "                        split_conf\n",
    "                        and split_operator_name in operator_index_map\n",
    "                        and operator_index_map[split_operator_name] >= split_operator_index\n",
    "                    ):\n",
    "                        # Do graph splitting.\n",
    "                        split_conf.pop(0)\n",
    "                        snode_dep.append({\"nodes\": {}, \"ref_nodes\": {}})\n",
    "                        ann = _recursion(\n",
    "                            anf.body,\n",
    "                            pipeline_mods,\n",
    "                            split_conf,\n",
    "                            constant_expr,\n",
    "                        )\n",
    "                        snode_dep.pop()\n",
    "                        dep_vars = get_dep_var(snode_dep)\n",
    "                        # When the nodes of the current subgraph are the depedency node of another\n",
    "                        # subgraph, we need to set them as the output of current subgraph.\n",
    "                        body = relay.Tuple(dep_vars) if len(dep_vars) > 1 else anf.var\n",
    "                        # when the operator of current subgraph uses previous subgraph constant\n",
    "                        # as the argument of a \"relay.expr.call\", such constant may become a free\n",
    "                        # varaible if the constant does not exist in the current subgraph.\n",
    "                        # merge the previous constant with current subgraph to avoid such issue.\n",
    "                        if constant_expr:\n",
    "                            ann = merge_constant_expr(constant_expr, ann)\n",
    "                        ann = run_opt_pass(ann, transform.ToGraphNormalForm())\n",
    "                        mod = tvm.IRModule.from_expr(ann)\n",
    "                        pipeline_mods.insert(0, mod)\n",
    "                        # Return the last node of the current subgraph.\n",
    "                        return tvm.relay.expr.Let(anf.var, value, body)\n",
    "            return tvm.relay.expr.Let(\n",
    "                anf.var,\n",
    "                value,\n",
    "                _recursion(anf.body, pipeline_mods, split_conf, constant_expr),\n",
    "            )\n",
    "        # Or End\n",
    "        else:\n",
    "            return anf\n",
    "\n",
    "    snode_dep = [{\"nodes\": {}, \"ref_nodes\": {}}]\n",
    "    pipeline_mods = []\n",
    "    operator_index_map = {}\n",
    "    # Used to tracking new input which caused by graph splitting.\n",
    "    new_input_idx = 0\n",
    "    constant_expr = None\n",
    "    subgraph_split_conf = split_conf.copy()\n",
    "    # Binding the parameters.\n",
    "    if params:\n",
    "        expr = build_module.bind_params_by_name(expr, params)\n",
    "    anf = run_opt_pass(expr, transform.ToANormalForm())\n",
    "    anf = run_opt_pass(anf, transform.InferType())\n",
    "    ann = _recursion(\n",
    "        anf,\n",
    "        pipeline_mods,\n",
    "        subgraph_split_conf,\n",
    "        constant_expr,\n",
    "    )\n",
    "    ann = run_opt_pass(ann.body, transform.ToGraphNormalForm())\n",
    "    mod = tvm.IRModule.from_expr(ann)\n",
    "    pipeline_mods.insert(0, mod)\n",
    "    return pipeline_mods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%data: Tensor[(1, 3, 8, 8), float16] /* ty=Tensor[(1, 3, 8, 8), float16] */, %weight: Tensor[(16, 3, 3, 3), float16] /* ty=Tensor[(16, 3, 3, 3), float16] */, %bn_gamma: Tensor[(16), float16] /* ty=Tensor[(16), float16] */, %bn_beta: Tensor[(16), float16] /* ty=Tensor[(16), float16] */, %bn_mean: Tensor[(16), float16] /* ty=Tensor[(16), float16] */, %bn_var: Tensor[(16), float16] /* ty=Tensor[(16), float16] */, %dweight: Tensor[(1, 1024), float16] /* ty=Tensor[(1, 1024), float16] */) -> Tensor[(1, 1), float16] {\n",
      "  %0 = nn.conv2d(%data, %weight, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "  %1 = nn.batch_norm(%0, %bn_gamma, %bn_beta, %bn_mean, %bn_var) /* ty=(Tensor[(1, 16, 8, 8), float16], Tensor[(16), float16], Tensor[(16), float16]) */;\n",
      "  %2 = %1.0 /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "  %3 = nn.relu(%2) /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "  %4 = nn.batch_flatten(%3) /* ty=Tensor[(1, 1024), float16] */;\n",
      "  nn.dense(%4, %dweight, units=None) /* ty=Tensor[(1, 1), float16] */\n",
      "} /* ty=fn (Tensor[(1, 3, 8, 8), float16], Tensor[(16, 3, 3, 3), float16], Tensor[(16), float16], Tensor[(16), float16], Tensor[(16), float16], Tensor[(16), float16], Tensor[(1, 1024), float16]) -> Tensor[(1, 1), float16] */\n"
     ]
    }
   ],
   "source": [
    "split_config = [{\"op_name\": \"nn.relu\", \"op_index\": 0}]\n",
    "print(net['main'])\n",
    "subgraphs = graph_split(net[\"main\"], split_config, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "netnet = run_opt_pass(net[\"main\"], transform.ToANormalForm())\n",
    "netnet = run_opt_pass(netnet, transform.InferType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%data: Tensor[(1, 3, 8, 8), float16] /* ty=Tensor[(1, 3, 8, 8), float16] */, %weight: Tensor[(16, 3, 3, 3), float16] /* ty=Tensor[(16, 3, 3, 3), float16] */, %bn_gamma: Tensor[(16), float16] /* ty=Tensor[(16), float16] */, %bn_beta: Tensor[(16), float16] /* ty=Tensor[(16), float16] */, %bn_mean: Tensor[(16), float16] /* ty=Tensor[(16), float16] */, %bn_var: Tensor[(16), float16] /* ty=Tensor[(16), float16] */, %dweight: Tensor[(1, 1024), float16] /* ty=Tensor[(1, 1024), float16] */) -> Tensor[(1, 1), float16] {\n",
      "  let %x_12: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */ = nn.conv2d(%data, %weight, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "  let %x_13: (Tensor[(1, 16, 8, 8), float16], Tensor[(16), float16], Tensor[(16), float16]) /* ty=(Tensor[(1, 16, 8, 8), float16], Tensor[(16), float16], Tensor[(16), float16]) */ = nn.batch_norm(%x_12, %bn_gamma, %bn_beta, %bn_mean, %bn_var) /* ty=(Tensor[(1, 16, 8, 8), float16], Tensor[(16), float16], Tensor[(16), float16]) */;\n",
      "  let %x_14: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */ = %x_13.0 /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "  let %x_15: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */ = nn.relu(%x_14) /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "  let %x_16: Tensor[(1, 1024), float16] /* ty=Tensor[(1, 1024), float16] */ = nn.batch_flatten(%x_15) /* ty=Tensor[(1, 1024), float16] */;\n",
      "  let %x_17: Tensor[(1, 1), float16] /* ty=Tensor[(1, 1), float16] */ = nn.dense(%x_16, %dweight, units=None) /* ty=Tensor[(1, 1), float16] */;\n",
      "  %x_17\n",
      "} /* ty=fn (Tensor[(1, 3, 8, 8), float16], Tensor[(16, 3, 3, 3), float16], Tensor[(16), float16], Tensor[(16), float16], Tensor[(16), float16], Tensor[(16), float16], Tensor[(1, 1024), float16]) -> Tensor[(1, 1), float16] */\n"
     ]
    }
   ],
   "source": [
    "print(netnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %data: Tensor[(1, 3, 8, 8), float16] /* ty=Tensor[(1, 3, 8, 8), float16] */;\n",
      "free_var %weight: Tensor[(16, 3, 3, 3), float16] /* ty=Tensor[(16, 3, 3, 3), float16] */;\n",
      "let %x_12: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */ = nn.conv2d(%data, %weight, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "free_var %bn_gamma: Tensor[(16), float16] /* ty=Tensor[(16), float16] */;\n",
      "free_var %bn_beta: Tensor[(16), float16] /* ty=Tensor[(16), float16] */;\n",
      "free_var %bn_mean: Tensor[(16), float16] /* ty=Tensor[(16), float16] */;\n",
      "free_var %bn_var: Tensor[(16), float16] /* ty=Tensor[(16), float16] */;\n",
      "let %x_13: (Tensor[(1, 16, 8, 8), float16], Tensor[(16), float16], Tensor[(16), float16]) /* ty=(Tensor[(1, 16, 8, 8), float16], Tensor[(16), float16], Tensor[(16), float16]) */ = nn.batch_norm(%x_12, %bn_gamma, %bn_beta, %bn_mean, %bn_var) /* ty=(Tensor[(1, 16, 8, 8), float16], Tensor[(16), float16], Tensor[(16), float16]) */;\n",
      "let %x_14: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */ = %x_13.0 /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "let %x_15: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */ = nn.relu(%x_14) /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "let %x_16: Tensor[(1, 1024), float16] /* ty=Tensor[(1, 1024), float16] */ = nn.batch_flatten(%x_15) /* ty=Tensor[(1, 1024), float16] */;\n",
      "free_var %dweight: Tensor[(1, 1024), float16] /* ty=Tensor[(1, 1024), float16] */;\n",
      "let %x_17: Tensor[(1, 1), float16] /* ty=Tensor[(1, 1), float16] */ = nn.dense(%x_16, %dweight, units=None) /* ty=Tensor[(1, 1), float16] */;\n",
      "%x_17\n"
     ]
    }
   ],
   "source": [
    "print(netnet.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free_var %data: Tensor[(1, 3, 8, 8), float16] /* ty=Tensor[(1, 3, 8, 8), float16] */;\n",
      "free_var %weight: Tensor[(16, 3, 3, 3), float16] /* ty=Tensor[(16, 3, 3, 3), float16] */;\n",
      "nn.conv2d(%data, %weight, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 8, 8), float16] */\n"
     ]
    }
   ],
   "source": [
    "print(netnet.body.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tvm.ir.container.Array'>\n",
      "free_var %data: Tensor[(1, 3, 8, 8), float16] /* ty=Tensor[(1, 3, 8, 8), float16] */;\n",
      "%data\n"
     ]
    }
   ],
   "source": [
    "print(type(netnet.body.value.args))\n",
    "print(netnet.body.value.args[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%data: Tensor[(1, 3, 8, 8), float16] /* ty=Tensor[(1, 3, 8, 8), float16] */) {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float16] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "  %1 = nn.batch_norm(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float16] */, meta[relay.Constant][2] /* ty=Tensor[(16), float16] */, meta[relay.Constant][3] /* ty=Tensor[(16), float16] */, meta[relay.Constant][4] /* ty=Tensor[(16), float16] */) /* ty=(Tensor[(1, 16, 8, 8), float16], Tensor[(16), float16], Tensor[(16), float16]) */;\n",
      "  %2 = %1.0 /* ty=Tensor[(1, 16, 8, 8), float16] */;\n",
      "  nn.relu(%2) /* ty=Tensor[(1, 16, 8, 8), float16] */\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subgraphs[0]['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fn (%data_n_0: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */) {\n",
       "  %0 = nn.batch_flatten(%data_n_0) /* ty=Tensor[(1, 1024), float16] */;\n",
       "  nn.dense(%0, meta[relay.Constant][0] /* ty=Tensor[(1, 1024), float16] */, units=None) /* ty=Tensor[(1, 1), float16] */\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraphs[1]['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras = Model(3, 1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'cuda'\n",
    "dev = tvm.cuda()\n",
    "img_size = 256\n",
    "input_data = np.random.normal(0,1,(1,img_size,img_size,3)).astype(np.float32)\n",
    "# tvm result\n",
    "input_data = input_data.transpose([0, 3, 1, 2])\n",
    "\n",
    "shape_dict = {\"input_1\": input_data.shape}\n",
    "mod, params = relay.frontend.from_keras(model_keras, shape_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fn (%input_1: Tensor[(1, 3, 256, 256), float32], %v_param_1: Tensor[(16, 3, 3, 3), float32], %v_param_2: Tensor[(16), float32], %v_param_3: Tensor[(16), float32], %v_param_4: Tensor[(16), float32], %v_param_5: Tensor[(16), float32], %v_param_6: Tensor[(16), float32], %v_param_7: Tensor[(16, 16, 3, 3), float32], %v_param_8: Tensor[(16), float32], %v_param_9: Tensor[(16), float32], %v_param_10: Tensor[(16), float32], %v_param_11: Tensor[(16), float32], %v_param_12: Tensor[(16), float32]) {\n",
       "  %0 = nn.conv2d(%input_1, %v_param_1, padding=[1i64, 1i64, 1i64, 1i64], channels=16, kernel_size=[3, 3]);\n",
       "  %1 = nn.bias_add(%0, %v_param_2);\n",
       "  %2 = nn.batch_norm(%1, %v_param_3, %v_param_4, %v_param_5, %v_param_6, epsilon=0.001f);\n",
       "  %3 = %2.0;\n",
       "  %4 = nn.leaky_relu(%3, alpha=0.2f);\n",
       "  %5 = nn.conv2d(%4, %v_param_7, padding=[1i64, 1i64, 1i64, 1i64], channels=16, kernel_size=[3, 3]);\n",
       "  %6 = nn.bias_add(%5, %v_param_8);\n",
       "  %7 = nn.batch_norm(%6, %v_param_9, %v_param_10, %v_param_11, %v_param_12, epsilon=0.001f);\n",
       "  %7.0\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.relay.op.annotation import compiler_begin, compiler_end\n",
    "\n",
    "def test_extern_ccompiler_single_op():\n",
    "    @transform.function_pass(opt_level=0)\n",
    "    class MyAnnotator:\n",
    "        def transform_function(self, func, mod, dev):\n",
    "            class Annotator(tvm.relay.ExprMutator):\n",
    "                def visit_call(self, call):\n",
    "                    new_args = []\n",
    "                    for arg in call.args:\n",
    "                        ann = compiler_begin(self.visit(arg), \"ccompiler\")\n",
    "                        new_args.append(ann)\n",
    "                    new_call = relay.Call(call.op, new_args)\n",
    "                    return compiler_end(new_call, \"ccompiler\")\n",
    "\n",
    "            return Annotator().visit(func)\n",
    "\n",
    "    x = relay.var(\"x\", shape=(8, 8))\n",
    "    y = relay.var(\"y\", shape=(8, 8))\n",
    "    z = x + y\n",
    "    z = z + relay.const(9.0)\n",
    "    z = z + relay.const(9.0)\n",
    "    f = relay.Function([x, y], z)\n",
    "    x_data = np.random.rand(8, 8).astype(\"float32\")\n",
    "    y_data = np.random.rand(8, 8).astype(\"float32\")\n",
    "    mod = tvm.IRModule()\n",
    "    mod[\"main\"] = f\n",
    "    mod = MyAnnotator()(mod)\n",
    "    mod = transform.PartitionGraph()(mod)\n",
    "\n",
    "    # print(mod)\n",
    "    # print(\"---------------\")\n",
    "    # print({\"x\": x_data, \"y\": y_data}, (8, 8), x_data + y_data)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = test_extern_ccompiler_single_op()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.ir.module.IRModule"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = tvm.IRModule.from_expr(mm['tvmgen_default_ccompiler_main_0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#[version = \"0.0.5\"]\n",
       "def @tvmgen_default_ccompiler_main_0(%ccompiler_0_i0: Tensor[(8, 8), float32] /* ty=Tensor[(8, 8), float32] */, %ccompiler_0_i1: Tensor[(8, 8), float32] /* ty=Tensor[(8, 8), float32] */, Inline=1, Compiler=\"ccompiler\", global_symbol=\"tvmgen_default_ccompiler_main_0\", Primitive=1) -> Tensor[(8, 8), float32] {\n",
       "  add(%ccompiler_0_i0, %ccompiler_0_i1) /* ty=Tensor[(8, 8), float32] */\n",
       "}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  7: TVMFuncCall\n  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const\n  5: tvm::relay::backend::RelayBuildModule::Build(tvm::IRModule, tvm::runtime::Array<tvm::Target, void> const&, tvm::Target const&, tvm::relay::Executor const&, tvm::relay::Runtime const&, tvm::WorkspaceMemoryPools const&, tvm::ConstantMemoryPools const&, tvm::runtime::String)\n  4: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)\n  3: tvm::relay::backend::RelayBuildModule::OptimizeImpl(tvm::IRModule)\n  2: tvm::relay::backend::BindParamsInModule(tvm::IRModule, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::NDArray, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, tvm::runtime::NDArray> > > const&)\n  1: tvm::IRModuleNode::Lookup(tvm::runtime::String const&) const\n  0: tvm::IRModuleNode::GetGlobalVar(tvm::runtime::String const&) const\n  File \"/home/jd/workspace/tvm-v0.9.0/src/ir/module.cc\", line 144\nValueError: Cannot find global var \"main\" in the Module\ncandidates are: [\"tvmgen_default_ccompiler_main_0\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [94], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m tvm\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mPassContext(opt_level\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     lib \u001b[39m=\u001b[39m relay\u001b[39m.\u001b[39;49mbuild(mod, target, params\u001b[39m=\u001b[39;49mparams)\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/relay/build_module.py:438\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(ir_mod, target, target_host, executor, runtime, workspace_memory_pools, constant_memory_pools, params, mod_name)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39mwith\u001b[39;00m tophub_context:\n\u001b[1;32m    437\u001b[0m     bld_mod \u001b[39m=\u001b[39m BuildModule()\n\u001b[0;32m--> 438\u001b[0m     graph_json, runtime_mod, params \u001b[39m=\u001b[39m bld_mod\u001b[39m.\u001b[39;49mbuild(\n\u001b[1;32m    439\u001b[0m         mod\u001b[39m=\u001b[39;49mir_mod,\n\u001b[1;32m    440\u001b[0m         target\u001b[39m=\u001b[39;49mraw_targets,\n\u001b[1;32m    441\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    442\u001b[0m         executor\u001b[39m=\u001b[39;49mexecutor,\n\u001b[1;32m    443\u001b[0m         runtime\u001b[39m=\u001b[39;49mruntime,\n\u001b[1;32m    444\u001b[0m         workspace_memory_pools\u001b[39m=\u001b[39;49mworkspace_memory_pools,\n\u001b[1;32m    445\u001b[0m         constant_memory_pools\u001b[39m=\u001b[39;49mconstant_memory_pools,\n\u001b[1;32m    446\u001b[0m         mod_name\u001b[39m=\u001b[39;49mmod_name,\n\u001b[1;32m    447\u001b[0m     )\n\u001b[1;32m    448\u001b[0m     func_metadata \u001b[39m=\u001b[39m bld_mod\u001b[39m.\u001b[39mget_function_metadata()\n\u001b[1;32m    449\u001b[0m     devices \u001b[39m=\u001b[39m bld_mod\u001b[39m.\u001b[39mget_devices()\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/relay/build_module.py:161\u001b[0m, in \u001b[0;36mBuildModule.build\u001b[0;34m(self, mod, target, target_host, executor, runtime, workspace_memory_pools, constant_memory_pools, params, mod_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m autotvm\u001b[39m.\u001b[39mGLOBAL_SCOPE\u001b[39m.\u001b[39msilent \u001b[39m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m     is_auto_scheduler_enabled() \u001b[39mor\u001b[39;00m is_meta_schedule_enabled() \u001b[39mor\u001b[39;00m old_autotvm_silent\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m mod_name \u001b[39m=\u001b[39m mangle_module_name(mod_name)\n\u001b[0;32m--> 161\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build(\n\u001b[1;32m    162\u001b[0m     mod,\n\u001b[1;32m    163\u001b[0m     target,\n\u001b[1;32m    164\u001b[0m     target_host,\n\u001b[1;32m    165\u001b[0m     executor,\n\u001b[1;32m    166\u001b[0m     runtime,\n\u001b[1;32m    167\u001b[0m     workspace_memory_pools,\n\u001b[1;32m    168\u001b[0m     constant_memory_pools,\n\u001b[1;32m    169\u001b[0m     mod_name,\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    171\u001b[0m autotvm\u001b[39m.\u001b[39mGLOBAL_SCOPE\u001b[39m.\u001b[39msilent \u001b[39m=\u001b[39m old_autotvm_silent\n\u001b[1;32m    173\u001b[0m \u001b[39m# Get artifacts\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    225\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    227\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    228\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    236\u001b[0m ):\n\u001b[0;32m--> 237\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    238\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    239\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  7: TVMFuncCall\n  6: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const\n  5: tvm::relay::backend::RelayBuildModule::Build(tvm::IRModule, tvm::runtime::Array<tvm::Target, void> const&, tvm::Target const&, tvm::relay::Executor const&, tvm::relay::Runtime const&, tvm::WorkspaceMemoryPools const&, tvm::ConstantMemoryPools const&, tvm::runtime::String)\n  4: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)\n  3: tvm::relay::backend::RelayBuildModule::OptimizeImpl(tvm::IRModule)\n  2: tvm::relay::backend::BindParamsInModule(tvm::IRModule, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::NDArray, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, tvm::runtime::NDArray> > > const&)\n  1: tvm::IRModuleNode::Lookup(tvm::runtime::String const&) const\n  0: tvm::IRModuleNode::GetGlobalVar(tvm::runtime::String const&) const\n  File \"/home/jd/workspace/tvm-v0.9.0/src/ir/module.cc\", line 144\nValueError: Cannot find global var \"main\" in the Module\ncandidates are: [\"tvmgen_default_ccompiler_main_0\"]"
     ]
    }
   ],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "<class 'tvm.ir.expr.GlobalVar'> has no attribute params",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [96], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m z \u001b[39m=\u001b[39m mm\u001b[39m.\u001b[39mget_global_vars()[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m f \u001b[39m=\u001b[39m relay\u001b[39m.\u001b[39mFunction(z\u001b[39m.\u001b[39;49mparams, z\u001b[39m.\u001b[39mbody)\n\u001b[1;32m      3\u001b[0m mod \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mIRModule()\n\u001b[1;32m      4\u001b[0m mod[\u001b[39m'\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m f\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/runtime/object.py:67\u001b[0m, in \u001b[0;36mObject.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m _ffi_node_api\u001b[39m.\u001b[39mNodeGetAttr(\u001b[39mself\u001b[39m, name)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)), name)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: <class 'tvm.ir.expr.GlobalVar'> has no attribute params"
     ]
    }
   ],
   "source": [
    "z = mm.get_global_vars()[0]\n",
    "f = relay.Function(z.params, z.body)\n",
    "mod = tvm.IRModule()\n",
    "mod['main'] = f\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvmgen_default_ccompiler_main_4\n",
      "tvmgen_default_ccompiler_main_0\n",
      "tvmgen_default_ccompiler_main_2\n",
      "main\n"
     ]
    }
   ],
   "source": [
    "for func in mm.functions:\n",
    "    print(func.astext().split('\\n')[-1].split('@')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = relay.var(\"x\", shape=(8, 8))\n",
    "y = relay.var(\"y\", shape=(8, 8))\n",
    "z = mm['tvmgen_default_ccompiler_main_0'](x,y)\n",
    "f = relay.Function([x, y], z)\n",
    "mod1 = tvm.IRModule()\n",
    "mod1['main'] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.relay.function.Function"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  2: TVMFuncCall\n  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void>, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void>)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void>, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void>)#2}>(tvm::{lambda(tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void>, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void>)#2}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)\n  0: tvm::runtime::TVMMovableArgValueWithContext_::operator tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void><tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> >() const\n  3: TVMFuncCall\n  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void>, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void>)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void>, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void>)#2}>(tvm::{lambda(tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void>, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void>)#2}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)\n  1: tvm::runtime::TVMMovableArgValueWithContext_::operator tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void><tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> >() const\n  0: tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> tvm::runtime::TVMPODValue_::AsObjectRef<tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> >() const\n  File \"/home/jd/workspace/tvm-v0.9.0/include/tvm/runtime/packed_func.h\", line 777\nTVMError: In function ir.IRModule(0: Map<GlobalVar, BaseFunc>, 1: Map<GlobalTypeVar, relay.TypeData>) -> IRModule: error while converting argument 0: [21:29:35] /home/jd/workspace/tvm-v0.9.0/include/tvm/runtime/packed_func.h:1863: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (!checked_type.defined()) is false: Expected Map[GlobalVar, BaseFunc], but got relay.Function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [72], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m z\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mglobal_symbol \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m z \u001b[39m=\u001b[39m z\u001b[39m.\u001b[39mwith_attr(\u001b[39m\"\u001b[39m\u001b[39mglobal_symbol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m mod \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39;49mIRModule(z)\n\u001b[1;32m      5\u001b[0m \u001b[39m# mod['main'] = z\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m tvm\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mPassContext(opt_level\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/ir/module.py:62\u001b[0m, in \u001b[0;36mIRModule.__init__\u001b[0;34m(self, functions, type_definitions)\u001b[0m\n\u001b[1;32m     60\u001b[0m         mapped_type_defs[k] \u001b[39m=\u001b[39m v\n\u001b[1;32m     61\u001b[0m     type_definitions \u001b[39m=\u001b[39m mapped_type_defs\n\u001b[0;32m---> 62\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__init_handle_by_constructor__(_ffi_api\u001b[39m.\u001b[39;49mIRModule, functions, type_definitions)\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/object.py:136\u001b[0m, in \u001b[0;36mObjectBase.__init_handle_by_constructor__\u001b[0;34m(self, fconstructor, *args)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m# assign handle first to avoid error raising\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m handle \u001b[39m=\u001b[39m __init_by_constructor__(fconstructor, args)\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, ObjectHandle):\n\u001b[1;32m    138\u001b[0m     handle \u001b[39m=\u001b[39m ObjectHandle(handle)\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py:260\u001b[0m, in \u001b[0;36m__init_handle_by_constructor__\u001b[0;34m(fconstructor, args)\u001b[0m\n\u001b[1;32m    248\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    250\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    251\u001b[0m         fconstructor\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    259\u001b[0m ):\n\u001b[0;32m--> 260\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    261\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    262\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  2: TVMFuncCall\n  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void>, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void>)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void>, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void>)#2}>(tvm::{lambda(tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void>, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void>)#2}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)\n  0: tvm::runtime::TVMMovableArgValueWithContext_::operator tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void><tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> >() const\n  3: TVMFuncCall\n  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void>, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void>)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void>, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void>)#2}>(tvm::{lambda(tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void>, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void>)#2}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)\n  1: tvm::runtime::TVMMovableArgValueWithContext_::operator tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void><tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> >() const\n  0: tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> tvm::runtime::TVMPODValue_::AsObjectRef<tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> >() const\n  File \"/home/jd/workspace/tvm-v0.9.0/include/tvm/runtime/packed_func.h\", line 777\nTVMError: In function ir.IRModule(0: Map<GlobalVar, BaseFunc>, 1: Map<GlobalTypeVar, relay.TypeData>) -> IRModule: error while converting argument 0: [21:29:35] /home/jd/workspace/tvm-v0.9.0/include/tvm/runtime/packed_func.h:1863: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (!checked_type.defined()) is false: Expected Map[GlobalVar, BaseFunc], but got relay.Function\n"
     ]
    }
   ],
   "source": [
    "z = mm['tvmgen_default_ccompiler_main_0']\n",
    "z.attrs.global_symbol = 'main'\n",
    "z = z.with_attr(\"global_symbol\", \"main\")\n",
    "mod = tvm.IRModule(z)\n",
    "# mod['main'] = z\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target, params=params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func0 = func0.with_attr(\"global_symbol\", \"main\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#[version = \"0.0.5\"]\n",
       "def @main(%x: Tensor[(8, 8), float32], %y: Tensor[(8, 8), float32]) {\n",
       "  %0 = fn (%ccompiler_0_i0: Tensor[(8, 8), float32] /* ty=Tensor[(8, 8), float32] */, %ccompiler_0_i1: Tensor[(8, 8), float32] /* ty=Tensor[(8, 8), float32] */, Inline=1, Compiler=\"ccompiler\", global_symbol=\"tvmgen_default_ccompiler_main_0\", Primitive=1) -> Tensor[(8, 8), float32] {\n",
       "    add(%ccompiler_0_i0, %ccompiler_0_i1) /* ty=Tensor[(8, 8), float32] */\n",
       "  } /* ty=fn (Tensor[(8, 8), float32], Tensor[(8, 8), float32]) -> Tensor[(8, 8), float32] */;\n",
       "  %0(%x, %y)\n",
       "}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = mm['tvmgen_default_ccompiler_main_0']\n",
    "f = relay.Function(z.params, z.body)\n",
    "mod = tvm.IRModule()\n",
    "mod['main'] = f\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GraphExecutorFactoryModule' object has no attribute 'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lib\u001b[39m.\u001b[39;49mbody\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GraphExecutorFactoryModule' object has no attribute 'body'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fn (%ccompiler_0_i0: Tensor[(8, 8), float32] /* ty=Tensor[(8, 8), float32] */, %ccompiler_0_i1: Tensor[(8, 8), float32] /* ty=Tensor[(8, 8), float32] */) {\n",
       "  add(%ccompiler_0_i0, %ccompiler_0_i1) /* ty=Tensor[(8, 8), float32] */\n",
       "}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod2, target, params=params)\n",
    "    # lib = relay.build(mod2, target, params=params,mod_name='tvmgen_default_ccompiler_main_0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%ccompiler_0_i0: Tensor[(8, 8), float32] /* ty=Tensor[(8, 8), float32] */, %ccompiler_0_i1: Tensor[(8, 8), float32] /* ty=Tensor[(8, 8), float32] */, Inline=1, Compiler=\"ccompiler\", global_symbol=\"tvmgen_default_ccompiler_main_0\", Primitive=1) -> Tensor[(8, 8), float32] {\n",
      "  add(%ccompiler_0_i0, %ccompiler_0_i1) /* ty=Tensor[(8, 8), float32] */\n",
      "} /* ty=fn (Tensor[(8, 8), float32], Tensor[(8, 8), float32]) -> Tensor[(8, 8), float32] */\n"
     ]
    }
   ],
   "source": [
    "print(mm['tvmgen_default_ccompiler_main_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%x: Tensor[(8, 8), float32], %y: Tensor[(8, 8), float32]) {\n",
      "  %0 = fn (%ccompiler_0_i0: Tensor[(8, 8), float32] /* ty=Tensor[(8, 8), float32] */, %ccompiler_0_i1: Tensor[(8, 8), float32] /* ty=Tensor[(8, 8), float32] */, Inline=1, Compiler=\"ccompiler\", global_symbol=\"tvmgen_default_ccompiler_main_0\", Primitive=1) -> Tensor[(8, 8), float32] {\n",
      "    add(%ccompiler_0_i0, %ccompiler_0_i1) /* ty=Tensor[(8, 8), float32] */\n",
      "  } /* ty=fn (Tensor[(8, 8), float32], Tensor[(8, 8), float32]) -> Tensor[(8, 8), float32] */;\n",
      "  %0(%x, %y)\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = tvm.IRModule()\n",
    "mod[\"main\"] = f\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target, params=params)\n",
    "    # lib = relay.build(mod2, target, params=params,mod_name='tvmgen_default_ccompiler_main_0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tvm.relay.backend.executor_factory.GraphExecutorFactoryModule at 0x7fe15c70dd00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tvm.relay.function.Function'>\n"
     ]
    }
   ],
   "source": [
    "print(type(legion))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tvm.relay.function.Function'>\n"
     ]
    }
   ],
   "source": [
    "print(type(mm['main']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = type(mm['tvmgen_default_ccompiler_main_0'])\n",
    "# mod = tvm.IRModule()\n",
    "# mod[\"main\"] = mm['tvmgen_default_ccompiler_main_0']\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legion = mm['tvmgen_default_ccompiler_main_0']\n",
    "# mod2 = tvm.IRModule()\n",
    "# mod2[\"main\"] = legion\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    # lib = relay.build(mm['tvmgen_default_ccompiler_main_0'], target, params=params,mod_name='tvmgen_default_ccompiler_main_0')\n",
    "    lib = relay.build(mm, target, params=params,mod_name='tvmgen_default_ccompiler_main_0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  8: TVMFuncCall\n  7: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const\n  6: tvm::relay::backend::RelayBuildModule::Build(tvm::IRModule, tvm::runtime::Array<tvm::Target, void> const&, tvm::Target const&, tvm::relay::Executor const&, tvm::relay::Runtime const&, tvm::WorkspaceMemoryPools const&, tvm::ConstantMemoryPools const&, tvm::runtime::String)\n  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)\n  4: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::relay::backend::GraphExecutorCodegenModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  3: tvm::relay::backend::GraphExecutorCodegen::Codegen(tvm::IRModule, tvm::relay::Function, tvm::runtime::String)\n  2: tvm::relay::tec::UpdateMainWorkspaceSize(tvm::IRModule const&, tvm::CompilationConfig const&, tvm::runtime::Map<tvm::RelayExpr, tvm::relay::backend::StorageInfo, void, void>)\n  1: tvm::IRModuleNode::Lookup(tvm::runtime::String const&) const\n  0: tvm::IRModuleNode::GetGlobalVar(tvm::runtime::String const&) const\n  File \"/home/jd/workspace/tvm-v0.9.0/src/ir/module.cc\", line 144\nValueError: Cannot find global var \"main\" in the Module\ncandidates are: [\"tvmgen_default_ccompiler_main_0\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m mod2[\u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m legion\n\u001b[1;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m tvm\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mPassContext(opt_level\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[39m# lib = relay.build(mm['tvmgen_default_ccompiler_main_0'], target, params=params,mod_name='tvmgen_default_ccompiler_main_0')\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     lib \u001b[39m=\u001b[39m relay\u001b[39m.\u001b[39;49mbuild(mod2, target, params\u001b[39m=\u001b[39;49mparams,mod_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtvmgen_default_ccompiler_main_0\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/relay/build_module.py:438\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(ir_mod, target, target_host, executor, runtime, workspace_memory_pools, constant_memory_pools, params, mod_name)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39mwith\u001b[39;00m tophub_context:\n\u001b[1;32m    437\u001b[0m     bld_mod \u001b[39m=\u001b[39m BuildModule()\n\u001b[0;32m--> 438\u001b[0m     graph_json, runtime_mod, params \u001b[39m=\u001b[39m bld_mod\u001b[39m.\u001b[39;49mbuild(\n\u001b[1;32m    439\u001b[0m         mod\u001b[39m=\u001b[39;49mir_mod,\n\u001b[1;32m    440\u001b[0m         target\u001b[39m=\u001b[39;49mraw_targets,\n\u001b[1;32m    441\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    442\u001b[0m         executor\u001b[39m=\u001b[39;49mexecutor,\n\u001b[1;32m    443\u001b[0m         runtime\u001b[39m=\u001b[39;49mruntime,\n\u001b[1;32m    444\u001b[0m         workspace_memory_pools\u001b[39m=\u001b[39;49mworkspace_memory_pools,\n\u001b[1;32m    445\u001b[0m         constant_memory_pools\u001b[39m=\u001b[39;49mconstant_memory_pools,\n\u001b[1;32m    446\u001b[0m         mod_name\u001b[39m=\u001b[39;49mmod_name,\n\u001b[1;32m    447\u001b[0m     )\n\u001b[1;32m    448\u001b[0m     func_metadata \u001b[39m=\u001b[39m bld_mod\u001b[39m.\u001b[39mget_function_metadata()\n\u001b[1;32m    449\u001b[0m     devices \u001b[39m=\u001b[39m bld_mod\u001b[39m.\u001b[39mget_devices()\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/relay/build_module.py:161\u001b[0m, in \u001b[0;36mBuildModule.build\u001b[0;34m(self, mod, target, target_host, executor, runtime, workspace_memory_pools, constant_memory_pools, params, mod_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m autotvm\u001b[39m.\u001b[39mGLOBAL_SCOPE\u001b[39m.\u001b[39msilent \u001b[39m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m     is_auto_scheduler_enabled() \u001b[39mor\u001b[39;00m is_meta_schedule_enabled() \u001b[39mor\u001b[39;00m old_autotvm_silent\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m mod_name \u001b[39m=\u001b[39m mangle_module_name(mod_name)\n\u001b[0;32m--> 161\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build(\n\u001b[1;32m    162\u001b[0m     mod,\n\u001b[1;32m    163\u001b[0m     target,\n\u001b[1;32m    164\u001b[0m     target_host,\n\u001b[1;32m    165\u001b[0m     executor,\n\u001b[1;32m    166\u001b[0m     runtime,\n\u001b[1;32m    167\u001b[0m     workspace_memory_pools,\n\u001b[1;32m    168\u001b[0m     constant_memory_pools,\n\u001b[1;32m    169\u001b[0m     mod_name,\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    171\u001b[0m autotvm\u001b[39m.\u001b[39mGLOBAL_SCOPE\u001b[39m.\u001b[39msilent \u001b[39m=\u001b[39m old_autotvm_silent\n\u001b[1;32m    173\u001b[0m \u001b[39m# Get artifacts\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    225\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    227\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    228\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    236\u001b[0m ):\n\u001b[0;32m--> 237\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    238\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    239\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  8: TVMFuncCall\n  7: tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const\n  6: tvm::relay::backend::RelayBuildModule::Build(tvm::IRModule, tvm::runtime::Array<tvm::Target, void> const&, tvm::Target const&, tvm::relay::Executor const&, tvm::relay::Runtime const&, tvm::WorkspaceMemoryPools const&, tvm::ConstantMemoryPools const&, tvm::runtime::String)\n  5: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)\n  4: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::relay::backend::GraphExecutorCodegenModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  3: tvm::relay::backend::GraphExecutorCodegen::Codegen(tvm::IRModule, tvm::relay::Function, tvm::runtime::String)\n  2: tvm::relay::tec::UpdateMainWorkspaceSize(tvm::IRModule const&, tvm::CompilationConfig const&, tvm::runtime::Map<tvm::RelayExpr, tvm::relay::backend::StorageInfo, void, void>)\n  1: tvm::IRModuleNode::Lookup(tvm::runtime::String const&) const\n  0: tvm::IRModuleNode::GetGlobalVar(tvm::runtime::String const&) const\n  File \"/home/jd/workspace/tvm-v0.9.0/src/ir/module.cc\", line 144\nValueError: Cannot find global var \"main\" in the Module\ncandidates are: [\"tvmgen_default_ccompiler_main_0\"]"
     ]
    }
   ],
   "source": [
    "# legion = mm['tvmgen_default_ccompiler_main_0']\n",
    "# mod2 = tvm.IRModule()\n",
    "# mod2[\"main\"] = legion\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    # lib = relay.build(mm['tvmgen_default_ccompiler_main_0'], target, params=params,mod_name='tvmgen_default_ccompiler_main_0')\n",
    "    lib = relay.build(mm, target, params=params,mod_name='tvmgen_default_ccompiler_main_0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mm, target, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jd/workspace/tvm-v0.9.0/python/tvm/driver/build_module.py:267: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mm, target, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tvm.relay.backend.executor_factory.GraphExecutorFactoryModule at 0x7fe15b392190>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tvm.runtime.packed_func.PackedFunc at 0x7fdc8dacd240>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib['get_graph_json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_json_raw = lib['get_graph_json']()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygraphviz as pgv\n",
    "import json\n",
    "def show_graph(json_data, file_name=None):\n",
    "    if type(json_data) == str:\n",
    "        json_data = json.loads(json_data)\n",
    "    A = pgv.AGraph(directed=True)\n",
    "    for node_idx, node in enumerate(json_data['nodes']):\n",
    "        for src in node['inputs']:\n",
    "            # if args.show_size == 1:\n",
    "            if 1 == 1:\n",
    "                src_size = 1\n",
    "                for i in json_data['attrs']['shape'][1][src[0]]:\n",
    "                    src_size = src_size * i\n",
    "                \n",
    "                dst_size = 1\n",
    "                for i in json_data['attrs']['shape'][1][node_idx]:\n",
    "                    dst_size = dst_size * i\n",
    "                # print(src[0], json_data['nodes'][src[0]]['name'], src_size)\n",
    "\n",
    "                A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]) + \"[{}]\".format(src_size), node['name'] + '[{}]'.format(node_idx) + '{}'.format(json_data['attrs']['dltype'][1][node_idx]) + \"[{}]\".format(dst_size))\n",
    "            else:\n",
    "                A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]), node['name'] + '[{}]'.format(node_idx) + '{}'.format(json_data['attrs']['dltype'][1][node_idx]))\n",
    "    if file_name:\n",
    "        A.draw(file_name + '.png', format='png', prog='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(graph_json_raw, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
